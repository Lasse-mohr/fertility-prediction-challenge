{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c50e51",
   "metadata": {},
   "source": [
    "# ExcelFormer with the RNN\n",
    "The notebook contains the pipeline to train and validate the model (+ training the final version of the model on the full data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9914262-7a3e-4e3a-9e07-043d40efd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data packages\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, average_precision_score, f1_score, recall_score, precision_score, matthews_corrcoef\n",
    "\n",
    "from model.rnn import GRUDecoder\n",
    "from model.encoders import CustomExcelFormer\n",
    "from data_processing.pipeline import encoding_pipeline, get_generic_name\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from model.utils import get_device\n",
    "from model.dataset import PretrainingDataset\n",
    "from model.dataset import FinetuningDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f2486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(preds, targs, metric, n_bootstraps: int):\n",
    "    \"\"\"\n",
    "    Returns mean and 95% Confidentence intervals for a metric\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        idx = np.random.choice(preds.shape[0], size=preds.shape[0], replace=True)\n",
    "        results.append(metric(targs[idx], preds[idx]))\n",
    "\n",
    "    return {\"mean\": np.mean(results),\n",
    "            \"CIs\": np.quantile(results,q=np.array([0.025, 0.975]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18aee1dd-7884-40e7-be05-db070f1a452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders) device\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b651aa-752a-4c71-990a-332ff4099791",
   "metadata": {},
   "source": [
    "# Read the data\n",
    "\n",
    "Right now the notebook is set to work with fake data. This can be changed once the pipeline works.\n",
    "\n",
    "The data is stored as a Dict[person_id, Sequences] where Sequences is a Dict[year, survery_wave_response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b872e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataClass:\n",
    "    def __init__(self,\n",
    "                 data_path: str = \"data/training_data/PreFer_train_data.csv\",\n",
    "                 targets_path: str = 'data/training_data/PreFer_train_outcome.csv',\n",
    "                 codebook_path: str = 'data/codebooks/PreFer_codebook.csv',\n",
    "                 importance_path: str = 'features_importance_all.csv') -> None:\n",
    "        self.data = pd.read_csv(data_path, low_memory=False)\n",
    "        self.targets = pd.read_csv(targets_path)\n",
    "        self.codebook = pd.read_csv(codebook_path)\n",
    "        self.col_importance = pd.read_csv(importance_path)\n",
    "    def make_sequences(self, n_cols: int, use_codebook: bool = True):\n",
    "        custom_pairs = self.col_importance.feature.map(lambda x: get_generic_name(x)).unique()[:n_cols]\n",
    "        self.sequences = encoding_pipeline(self.data, self.codebook, \n",
    "                                           custom_pairs=custom_pairs, \n",
    "                                           importance=self.col_importance, \n",
    "                                           use_codebook=use_codebook)\n",
    "    def make_pretraining(self):\n",
    "        self.pretrain_dataset = PretrainingDataset(self.sequences)\n",
    "        self.seq_len = self.pretrain_dataset.get_seq_len()\n",
    "        self.vocab_size = self.pretrain_dataset.get_vocab_size()\n",
    "    def make_full_finetuning(self, batch_size): \n",
    "        \"\"\"Create dataloader for the whole finetuning dataset\"\"\"\n",
    "        targets = self.targets[self.targets.new_child.notna()]\n",
    "        full_person_ids =  targets['nomem_encr'].values\n",
    "        rnn_data = {person_id: (\n",
    "                torch.tensor([year-2007 for year, _ in wave_responses.items()]).to(device),\n",
    "                torch.tensor([ wave_response for _, wave_response in wave_responses.items()]).to(device)\n",
    "                )\n",
    "                for person_id, wave_responses in self.sequences.items()\n",
    "                }\n",
    "\n",
    "        # split data based on the splits made for the target\n",
    "        full_data = {person_id: rnn_data[person_id] for person_id in full_person_ids}\n",
    "        self.full_dataset = FinetuningDataset(full_data, targets = targets)\n",
    "        self.full_dataloader = DataLoader(self.full_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    def make_finetuning(self, batch_size, test_size: float = 0.2, val_size: float = 0.2):\n",
    "        \"\"\"\n",
    "        Create dataloaders for the train/val/test splits.\n",
    "        \"\"\"\n",
    "        targets = self.targets[self.targets.new_child.notna()]\n",
    "        train_person_ids, test_person_ids = train_test_split(targets['nomem_encr'], test_size=test_size, random_state=42)\n",
    "        train_person_ids, val_person_ids = train_test_split(train_person_ids, test_size=val_size, random_state=42)\n",
    "        rnn_data = {person_id: (\n",
    "                torch.tensor([year-2007 for year, _ in wave_responses.items()]).to(device),\n",
    "                torch.tensor([ wave_response for _, wave_response in wave_responses.items()]).to(device)\n",
    "                )\n",
    "                for person_id, wave_responses in self.sequences.items()\n",
    "                }\n",
    "\n",
    "        # split data based on the splits made for the target\n",
    "        train_data = {person_id: rnn_data[person_id] for person_id in train_person_ids}\n",
    "        val_data = {person_id: rnn_data[person_id] for person_id in val_person_ids}\n",
    "        test_data = {person_id: rnn_data[person_id] for person_id in test_person_ids}\n",
    "\n",
    "        self.train_dataset = FinetuningDataset(train_data, targets = targets)\n",
    "        self.val_dataset = FinetuningDataset(val_data, targets = targets)\n",
    "        self.test_dataset = FinetuningDataset(test_data, targets = targets)\n",
    "        self.train_dataloader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.val_dataloader = DataLoader(self.val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.test_dataloader  = DataLoader(self.test_dataset,  batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "279b9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE DATA\n",
    "data = DataClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "393fa042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "BATCH_SIZE = 16\n",
    "HIDDEN_SIZE =  48\n",
    "ENCODING_SIZE = 48\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 5\n",
    "NUM_EPOCHS = 12\n",
    "DETECT_ANOMALY = False\n",
    "LR = 1e-2\n",
    "\n",
    "assert HIDDEN_SIZE % NUM_HEADS == 0, \"Check that the hidden size is divisible\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f7e4345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomarx/Documents/GitHub/fertility-prediction-challenge/data_processing/pipeline.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  codebook[\"pairs\"] = codebook['var_name'].apply(get_generic_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data.make_sequences(n_cols=150)\n",
    "data.make_pretraining()\n",
    "data.make_finetuning(batch_size=BATCH_SIZE)\n",
    "data.make_full_finetuning(batch_size=BATCH_SIZE)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d77d87a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = data.seq_len\n",
    "VOCAB_SIZE = data.vocab_size\n",
    "print(SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddb66e-cba5-4bb9-854d-811d49599b93",
   "metadata": {},
   "source": [
    "# Model and Training Setup\n",
    "Here we setup the model and training procedures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54a2330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreFerPredictor(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = CustomExcelFormer(vocab_size=VOCAB_SIZE, \n",
    "                            hidden_size=HIDDEN_SIZE, \n",
    "                            out_size=ENCODING_SIZE,\n",
    "                            n_years=14,\n",
    "                            num_heads=NUM_HEADS,\n",
    "                            num_layers=NUM_LAYERS, \n",
    "                            sequence_len=SEQ_LEN, \n",
    "                            aium_dropout=0.3,\n",
    "                            diam_dropout=0.1,\n",
    "                            residual_dropout=0.1,\n",
    "                            embedding_dropout=0.25).to(device)\n",
    "        self.decoder = GRUDecoder(\n",
    "            input_size=ENCODING_SIZE,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_layers=2,\n",
    "            max_seq_len=14,\n",
    "            dropout=0.25,\n",
    "  \n",
    "            bidirectional=True,\n",
    "            with_attention = True\n",
    "        ).to(device)\n",
    "\n",
    "        self.enc_dropout = nn.Dropout(0.1)\n",
    "        self.enc_dropout1d = nn.Dropout1d(0.1)\n",
    "    def forward(self, input_year, input_seq, labels):\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings, _ = self.encoder(input_year, input_seq)#, y=labels.unsqueeze(-1).expand(-1, 14).reshape(-1), mixup_encoded=True)\n",
    "        encodings = encodings.view(bs,ss, -1)\n",
    "        encodings = self.enc_dropout(encodings)\n",
    "        encodings = self.enc_dropout1d(encodings)\n",
    "        mask = ~((input_seq == 101).sum(-1) == SEQ_LEN).view(bs,ss).detach()\n",
    "        # Forward pass\n",
    "        out = self.decoder(encodings, mask=mask).flatten()\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caf49964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "model = PreFerPredictor().to(device)\n",
    "# Define the loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([1/0.2]).to(device))\n",
    "\n",
    "# Define the optimization strategy\n",
    "optimizer = torch.optim.RAdam(model.parameters() , lr=LR, weight_decay=1e-2, decoupled_weight_decay=True)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = NUM_EPOCHS * len(data.train_dataloader), eta_min = 8e-4, last_epoch = -1)\n",
    "## EMA Weight Averaging\n",
    "avg_fn = optim.swa_utils.get_ema_avg_fn(0.99)\n",
    "avg_model = optim.swa_utils.AveragedModel(model, avg_fn=avg_fn, use_buffers=False)\n",
    "avg_start = 3\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "avg_model.train()\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "649bc948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(epoch):\n",
    "    \"\"\"\n",
    "    Run the validation loop\n",
    "    \"\"\"\n",
    "    val_loss = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    model.eval()\n",
    "    avg_model.eval()\n",
    "    for batch in data.val_dataloader:\n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "        input_year, input_seq = inputs\n",
    "        if epoch <= avg_start:\n",
    "            output = model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "        else:\n",
    "            output = avg_model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "\n",
    "        probs = F.sigmoid(output).flatten()\n",
    "        loss = loss_fn(output, labels)  \n",
    "        val_loss.append(loss.detach().cpu().numpy())\n",
    "        preds.extend(probs.detach().cpu().numpy().tolist())\n",
    "        targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    # Concatenate all the batches\n",
    "    yhat = torch.tensor(preds).flatten().detach().cpu().numpy()\n",
    "    ytrue = torch.tensor(targets).flatten().cpu().numpy()\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(ytrue, yhat > 0.5, average='binary')\n",
    "    mcc = matthews_corrcoef(ytrue, yhat > 0.5)\n",
    "    map_roc = average_precision_score(ytrue, yhat)\n",
    "    print(f\"-- mAP Score: {map_roc:.4f} -- f1-score: {f1:.3f} -- mcc: {mcc:.3f}\")\n",
    "    model.train()\n",
    "    avg_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82cf1777-49e7-462d-ae51-549ea6c8305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 40it [00:11,  3.59it/s, mean loss: 3.005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12, Loss: 3.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomarx/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- mAP Score: 0.3946 -- f1-score: 0.000 -- mcc: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 1: 40it [00:11,  3.61it/s, mean loss: 2.521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12, Loss: 2.5211\n",
      "-- mAP Score: 0.5830 -- f1-score: 0.488 -- mcc: 0.298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 2: 40it [00:11,  3.63it/s, mean loss: 2.034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12, Loss: 2.0345\n",
      "-- mAP Score: 0.7151 -- f1-score: 0.609 -- mcc: 0.519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 3: 40it [00:10,  3.64it/s, mean loss: 1.260]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12, Loss: 1.2595\n",
      "-- mAP Score: 0.7589 -- f1-score: 0.567 -- mcc: 0.536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 4: 40it [00:12,  3.15it/s, mean loss: 1.065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/12, Loss: 1.0653\n",
      "-- mAP Score: 0.7691 -- f1-score: 0.590 -- mcc: 0.556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 5: 40it [00:12,  3.16it/s, mean loss: 0.950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/12, Loss: 0.9497\n",
      "-- mAP Score: 0.7768 -- f1-score: 0.625 -- mcc: 0.574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 6: 40it [00:12,  3.20it/s, mean loss: 0.750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/12, Loss: 0.7499\n",
      "-- mAP Score: 0.7814 -- f1-score: 0.676 -- mcc: 0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 7: 40it [00:12,  3.22it/s, mean loss: 0.590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/12, Loss: 0.5899\n",
      "-- mAP Score: 0.7697 -- f1-score: 0.686 -- mcc: 0.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 8: 40it [00:12,  3.22it/s, mean loss: 0.419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/12, Loss: 0.4186\n",
      "-- mAP Score: 0.7549 -- f1-score: 0.722 -- mcc: 0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 9: 40it [00:12,  3.22it/s, mean loss: 0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/12, Loss: 0.2587\n",
      "-- mAP Score: 0.7379 -- f1-score: 0.722 -- mcc: 0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 10: 40it [00:12,  3.21it/s, mean loss: 0.268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12, Loss: 0.2684\n",
      "-- mAP Score: 0.7299 -- f1-score: 0.732 -- mcc: 0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 11: 40it [00:12,  3.22it/s, mean loss: 0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/12, Loss: 0.2421\n",
      "-- mAP Score: 0.7239 -- f1-score: 0.732 -- mcc: 0.671\n"
     ]
    }
   ],
   "source": [
    "loss_per_epoch = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss_per_step = []\n",
    "    loop_object  = tqdm(enumerate(data.train_dataloader), desc=f\"Epochs {epoch}\")\n",
    "    for i, batch in loop_object :        \n",
    "        optimizer.zero_grad() \n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "        input_year, input_seq = inputs\n",
    "        ### Model\n",
    "        output = model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "        probs = F.sigmoid(output).flatten()\n",
    "        ### Loss\n",
    "        loss = loss_fn(output, labels) #+ loss_focal(output, labels)\n",
    "        loss_per_step.append(loss.detach().cpu().numpy())\n",
    "        loop_object.set_postfix_str(\"mean loss: %.3f\"%np.mean(loss_per_step[-100:]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # AVERAGING starts only after 3 epochs\n",
    "        if epoch > avg_start:\n",
    "            avg_model.update_parameters(model)\n",
    "    # On epoch end\n",
    "    loss_per_epoch.append(np.mean(loss_per_step))\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss_per_epoch[-1]:.4f}\")\n",
    "    run_validation(epoch=epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0d2fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7660\n",
      "Recall: 0.7200\n",
      "F1 Score: 0.7423\n",
      "-- mAP Score: 0.8419 --\n"
     ]
    }
   ],
   "source": [
    "test_loss = []\n",
    "preds = []\n",
    "targets = []\n",
    "\n",
    "## Set both models into the eval mode.=\n",
    "avg_model.eval()\n",
    "for batch in data.test_dataloader:\n",
    "    inputs, labels = batch\n",
    "    labels = labels.to(torch.float).to(device)\n",
    "    input_year, input_seq = inputs\n",
    "    ### Model\n",
    "    output = avg_model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "    probs = F.sigmoid(output).flatten()\n",
    "\n",
    "    loss = loss_fn(output, labels)  \n",
    "    test_loss.append(loss.detach().cpu().numpy())\n",
    "    preds.extend(probs.detach().cpu().numpy().tolist())\n",
    "    targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "# Concatenate all the batches\n",
    "probs = torch.tensor(preds).flatten()\n",
    "actuals = torch.tensor(targets).flatten()\n",
    "# Concatenate all the batches\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(actuals.cpu().numpy(), probs.cpu().numpy() > 0.5, average='binary')\n",
    "map_roc = average_precision_score(actuals.numpy(), probs.numpy())\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"-- mAP Score: {map_roc:.4f} --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3891379b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([98., 24., 15.,  9.,  5.,  5.,  6.,  5., 15., 16.]),\n",
       " array([0.00353061, 0.10084035, 0.1981501 , 0.29545984, 0.3927696 ,\n",
       "        0.49007934, 0.58738911, 0.68469882, 0.78200859, 0.8793183 ,\n",
       "        0.97662807]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2a219b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Performance of the Model:\n",
      "\tF1 Score: {'mean': 0.739795146914729, 'CIs': array([0.63529412, 0.83168317])}\n",
      "\tMCC Score: {'mean': 0.6585547581908553, 'CIs': array([0.5325456 , 0.77700526])}\n",
      "\tPrecision: {'mean': 0.7652470989493324, 'CIs': array([0.63829787, 0.88372093])}\n",
      "\tRecall: {'mean': 0.719310777388724, 'CIs': array([0.59090909, 0.83673469])}\n",
      "\tmAP Score: {'mean': 0.8422834497648027, 'CIs': array([0.75197675, 0.91293439])}\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimated Performance of the Model:\")\n",
    "print(\"\\tF1 Score:\", bootstrap( probs.cpu().numpy() > 0.5, actuals.cpu().numpy(), metric= lambda x,y: f1_score(x,y), n_bootstraps=10000))\n",
    "print(\"\\tMCC Score:\", bootstrap( probs.cpu().numpy() > 0.5, actuals.cpu().numpy(), metric= lambda x,y: matthews_corrcoef(x,y), n_bootstraps=10000))\n",
    "print(\"\\tPrecision:\", bootstrap( probs.cpu().numpy() > 0.5, actuals.cpu().numpy(), metric= lambda x,y: precision_score(x,y), n_bootstraps=10000))\n",
    "print(\"\\tRecall:\", bootstrap( probs.cpu().numpy() > 0.5, actuals.cpu().numpy(), metric= lambda x,y: recall_score(x,y), n_bootstraps=10000))\n",
    "print(\"\\tmAP Score:\", bootstrap(preds = probs.cpu().numpy(), targs =  actuals.cpu().numpy(), metric= lambda x,y: average_precision_score(x,y), n_bootstraps=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c5ab9e",
   "metadata": {},
   "source": [
    "# Train Model on the Full Data\n",
    "(model for inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d6cf880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomarx/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/carlomarx/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "# Reinitialize the training \n",
    "model = PreFerPredictor().to(device)\n",
    "# Define the loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([1/0.2]).to(device))\n",
    "# Define the optimization strategy\n",
    "optimizer = torch.optim.RAdam(model.parameters() , lr=LR, weight_decay=1e-2, decoupled_weight_decay=True)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = NUM_EPOCHS * len(data.full_dataloader), eta_min = 5e-4, last_epoch = -1)\n",
    "## EMA Weight Averaging\n",
    "avg_fn = optim.swa_utils.get_ema_avg_fn(0.99)\n",
    "avg_model = optim.swa_utils.AveragedModel(model, avg_fn=avg_fn, use_buffers=False)\n",
    "avg_start = 3\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "avg_model.train()\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6db2254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 62it [00:18,  3.43it/s, mean loss: 1.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12, Loss: 1.2419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 1: 62it [00:17,  3.61it/s, mean loss: 0.846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12, Loss: 0.8462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 2: 62it [00:17,  3.59it/s, mean loss: 0.617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12, Loss: 0.6166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 3: 62it [00:17,  3.63it/s, mean loss: 0.517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12, Loss: 0.5165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 4: 62it [00:19,  3.17it/s, mean loss: 0.514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/12, Loss: 0.5141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 5: 62it [00:19,  3.14it/s, mean loss: 0.382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/12, Loss: 0.3821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 6: 62it [00:19,  3.17it/s, mean loss: 0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/12, Loss: 0.2580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 7: 62it [00:19,  3.16it/s, mean loss: 0.206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/12, Loss: 0.2062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 8: 62it [00:19,  3.14it/s, mean loss: 0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/12, Loss: 0.1280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 9: 62it [00:19,  3.14it/s, mean loss: 0.102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/12, Loss: 0.1019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 10: 62it [00:19,  3.12it/s, mean loss: 0.080]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12, Loss: 0.0796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 11: 62it [00:19,  3.19it/s, mean loss: 0.076]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/12, Loss: 0.0756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## TRAIN ON THE FULL DATA\n",
    "loss_per_epoch = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss_per_step = []\n",
    "    loop_object  = tqdm(enumerate(data.full_dataloader), desc=f\"Epochs {epoch}\")\n",
    "    for i, batch in loop_object :        \n",
    "        optimizer.zero_grad() \n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "        input_year, input_seq = inputs\n",
    "        ### Model\n",
    "        output = model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "        probs = F.sigmoid(output).flatten()\n",
    "        ### Loss\n",
    "        loss = loss_fn(output, labels)  \n",
    "        loss_per_step.append(loss.detach().cpu().numpy())\n",
    "        loop_object.set_postfix_str(\"mean loss: %.3f\"%np.mean(loss_per_step[-100:]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # AVERAGING starts only after 3 epochs\n",
    "        if epoch > avg_start:\n",
    "            avg_model.update_parameters(model)\n",
    "    # On epoch end\n",
    "    loss_per_epoch.append(np.mean(loss_per_step))\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss_per_epoch[-1]:.4f}\")\n",
    "    #run_validation(epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fad880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"weights/excel_rnn.pt\"\n",
    "torch.save(avg_model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce2862fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.full_model import PreFerPredictor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fc0683d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = optim.swa_utils.AveragedModel(PreFerPredictor())\n",
    "model.load_state_dict(torch.load(\"weights/excel_rnn.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6aef71",
   "metadata": {},
   "source": [
    "# Check if the loading pipeline works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee978aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders) device\n"
     ]
    }
   ],
   "source": [
    "from model.full_model import DataClass, PreFerPredictor\n",
    "import torch\n",
    "import pandas as pd\n",
    "from model.utils import get_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467f93b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataClass(to_predict_df=pd.read_csv(\"data/training_data/PreFer_train_data.csv\", low_memory=False).sample(frac=0.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b512871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomarx/Documents/GitHub/fertility-prediction-challenge/data_processing/pipeline.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  codebook[\"pairs\"] = codebook['var_name'].apply(get_generic_name)\n",
      "/Users/carlomarx/Documents/GitHub/fertility-prediction-challenge/data_processing/pipeline.py:62: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data = data.fillna(101)\n"
     ]
    }
   ],
   "source": [
    "data.make_sequences(n_cols=150)\n",
    "data.prepare_prediction(batch_size=16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d0cac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders) device\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ccf8500",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.optim.swa_utils.AveragedModel(PreFerPredictor())\n",
    "model.load_state_dict(torch.load(\"weights/excel_rnn.pt\", map_location=device))\n",
    "predictions = model.get_submodule(\"module\").predict(data.prediction_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "489ca064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63328d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([466.,  59.,  50.,  36.,  32.,  41.,  58.,  62., 125., 355.]),\n",
       " array([7.37222654e-05, 9.99787375e-02, 1.99883759e-01, 2.99788773e-01,\n",
       "        3.99693787e-01, 4.99598801e-01, 5.99503815e-01, 6.99408829e-01,\n",
       "        7.99313843e-01, 8.99218857e-01, 9.99123871e-01]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cfcc498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreFerPredictor(\n",
       "  (encoder): CustomExcelFormer(\n",
       "    (embedding): SurveyEmbeddings(\n",
       "      (answer_embedding): Embedding(860, 48, padding_idx=101)\n",
       "      (yearly_embedding): Embedding(14, 48)\n",
       "      (question_embedding): Embedding(149, 48)\n",
       "      (drop_year): Dropout1d(p=0.25, inplace=False)\n",
       "    )\n",
       "    (embedding_norm): InstanceNorm1d(149, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (excelformer_convs): ModuleList(\n",
       "      (0-4): 5 x ExcelFormerConv(\n",
       "        (norm_1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (DiaM): DiaM(\n",
       "          (lin_q): Linear(in_features=48, out_features=48, bias=True)\n",
       "          (lin_k): Linear(in_features=48, out_features=48, bias=True)\n",
       "          (lin_v): Linear(in_features=48, out_features=48, bias=True)\n",
       "          (lin_out): Linear(in_features=48, out_features=48, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm_2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (AiuM): AiuM(\n",
       "          (lin_1): Linear(in_features=48, out_features=48, bias=True)\n",
       "          (lin_2): Linear(in_features=48, out_features=48, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (excelformer_decoder): ExcelFormerDecoder(\n",
       "      (lin_f): Linear(in_features=149, out_features=48, bias=True)\n",
       "      (activation): Mish()\n",
       "      (lin_d): Linear(in_features=48, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): GRUDecoder(\n",
       "    (norm_in): InstanceNorm1d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (gru): GRU(48, 48, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "    (aggregation): AggAttention(\n",
       "      (act): Softmax(dim=1)\n",
       "    )\n",
       "    (norm_out): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=96, out_features=1, bias=False)\n",
       "  )\n",
       "  (enc_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (enc_dropout1d): Dropout1d(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_submodule('module')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6afebca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PreFer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
