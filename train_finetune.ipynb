{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9914262-7a3e-4e3a-9e07-043d40efd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data packages\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, average_precision_score\n",
    "\n",
    "from model.rnn import GRUDecoder\n",
    "from model.encoders import CustomExcelFormer\n",
    "from data_processing.pipeline import encoding_pipeline, get_generic_name\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from model.utils import get_device\n",
    "from model.dataset import PretrainingDataset\n",
    "from model.dataset import FinetuningDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18aee1dd-7884-40e7-be05-db070f1a452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders) device\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b651aa-752a-4c71-990a-332ff4099791",
   "metadata": {},
   "source": [
    "# Read the data\n",
    "\n",
    "Right now the notebook is set to work with fake data. This can be changed once the pipeline works.\n",
    "\n",
    "The data is stored as a Dict[person_id, Sequences] where Sequences is a Dict[year, survery_wave_response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b872e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataClass:\n",
    "    def __init__(self,\n",
    "                 data_path: str = \"data/training_data/PreFer_train_data.csv\",\n",
    "                 targets_path: str = 'data/training_data/PreFer_train_outcome.csv',\n",
    "                 codebook_path: str = 'data/codebooks/PreFer_codebook.csv',\n",
    "                 importance_path: str = 'features_importance_all.csv') -> None:\n",
    "        self.data = pd.read_csv(data_path, low_memory=False)\n",
    "        self.targets = pd.read_csv(targets_path)\n",
    "        self.codebook = pd.read_csv(codebook_path)\n",
    "        self.col_importance = pd.read_csv(importance_path)\n",
    "    def make_sequences(self, n_cols: int, use_codebook: bool = True):\n",
    "        custom_pairs = self.col_importance.feature.map(lambda x: get_generic_name(x)).unique()[:n_cols]\n",
    "        self.sequences = encoding_pipeline(self.data, self.codebook, \n",
    "                                           custom_pairs=custom_pairs, \n",
    "                                           importance=self.col_importance, \n",
    "                                           use_codebook=use_codebook)\n",
    "    def make_pretraining(self):\n",
    "        self.pretrain_dataset = PretrainingDataset(self.sequences)\n",
    "        self.seq_len = self.pretrain_dataset.get_seq_len()\n",
    "        self.vocab_size = self.pretrain_dataset.get_vocab_size()\n",
    "    def make_finetuning(self, batch_size, test_size: float = 0.2, val_size: float = 0.2):\n",
    "        targets = self.targets[self.targets.new_child.notna()]\n",
    "        train_person_ids, test_person_ids = train_test_split(targets['nomem_encr'], test_size=test_size, random_state=42)\n",
    "        train_person_ids, val_person_ids = train_test_split(train_person_ids, test_size=val_size, random_state=42)\n",
    "        rnn_data = {person_id: (\n",
    "                torch.tensor([year-2007 for year, _ in wave_responses.items()]).to(device),\n",
    "                torch.tensor([ wave_response for _, wave_response in wave_responses.items()]).to(device)\n",
    "                )\n",
    "                for person_id, wave_responses in self.sequences.items()\n",
    "                }\n",
    "\n",
    "        # split data based on the splits made for the target\n",
    "        train_data = {person_id: rnn_data[person_id] for person_id in train_person_ids}\n",
    "        val_data = {person_id: rnn_data[person_id] for person_id in val_person_ids}\n",
    "        test_data = {person_id: rnn_data[person_id] for person_id in test_person_ids}\n",
    "\n",
    "        self.train_dataset = FinetuningDataset(train_data, targets = targets)\n",
    "        self.val_dataset = FinetuningDataset(val_data, targets = targets)\n",
    "        self.test_dataset = FinetuningDataset(test_data, targets = targets)\n",
    "        self.train_dataloader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.val_dataloader = DataLoader(self.val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.test_dataloader  = DataLoader(self.test_dataset,  batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4e1c0",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "279b9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f7e4345",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m data\u001b[38;5;241m.\u001b[39mmake_pretraining()\n\u001b[1;32m      3\u001b[0m data\u001b[38;5;241m.\u001b[39mmake_finetuning(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 13\u001b[0m, in \u001b[0;36mDataClass.make_sequences\u001b[0;34m(self, n_cols, use_codebook)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_sequences\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_cols: \u001b[38;5;28mint\u001b[39m, use_codebook: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     12\u001b[0m     custom_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_importance\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: get_generic_name(x))\u001b[38;5;241m.\u001b[39munique()[:n_cols]\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequences \u001b[38;5;241m=\u001b[39m \u001b[43mencoding_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodebook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mcustom_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mimportance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol_importance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43muse_codebook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_codebook\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/fertility-prediction-challenge/data_processing/pipeline.py:14\u001b[0m, in \u001b[0;36mencoding_pipeline\u001b[0;34m(data, codebook, use_codebook, custom_pairs, save_inter_path, importance)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoding_pipeline\u001b[39m(data, codebook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, use_codebook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, custom_pairs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m                       save_inter_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_processing/codebook_false/encoding_pipeline/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m                       importance: pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     12\u001b[0m                       ):\n\u001b[0;32m---> 14\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     categorical_column_filepath \u001b[38;5;241m=\u001b[39m save_inter_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_columns.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m     quantile_column_filepath \u001b[38;5;241m=\u001b[39m save_inter_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantile_columns.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py:6811\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m   6662\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   6663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, deep: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   6664\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6665\u001b[0m \u001b[38;5;124;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[1;32m   6666\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6809\u001b[0m \u001b[38;5;124;03m    dtype: int64\u001b[39;00m\n\u001b[1;32m   6810\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6811\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6812\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[1;32m   6813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(data, axes\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   6814\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6815\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/managers.py:593\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m         new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m--> 593\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcopy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m res\u001b[38;5;241m.\u001b[39maxes \u001b[38;5;241m=\u001b[39m new_axes\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/blocks.py:796\u001b[0m, in \u001b[0;36mBlock.copy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    794\u001b[0m refs: BlockValuesRefs \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 796\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    797\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data.make_sequences(n_cols=150)\n",
    "data.make_pretraining()\n",
    "data.make_finetuning(batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d87a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "#ENCODING_SIZE = 64\n",
    "BATCH_SIZE = 8\n",
    "HIDDEN_SIZE = 64\n",
    "ENCODING_SIZE = 64\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 4\n",
    "NUM_EPOCHS = 13\n",
    "DETECT_ANOMALY = False\n",
    "SEQ_LEN = data.seq_len\n",
    "VOCAB_SIZE = data.vocab_size\n",
    "\n",
    "LR = 7e-3\n",
    "\n",
    "assert HIDDEN_SIZE % NUM_HEADS == 0, \"Check that the hidden size is divisible\"\n",
    "print(SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddb66e-cba5-4bb9-854d-811d49599b93",
   "metadata": {},
   "source": [
    "# Experimental Encoder (Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada8018b-7ebc-4db4-aadf-191b214b1470",
   "metadata": {},
   "source": [
    "# Train the RNN\n",
    "\n",
    "First we need to create Dataset class that can hold both the target (stored in a pd.DataFrame) and the sequences.\n",
    "\n",
    "The sequences will be of dimension 14 x encoding_dimension, because we have 14 years of surveys.\n",
    "\n",
    "I have created some code for getting the data into the right format, but it might not be useful.\n",
    "\n",
    "## Regarding masks\n",
    "Right now the masking is done already in the encoding. I haven't found exactly where Mikkel implemented this.\n",
    "So for now, assume that nothing is padded, and then we'll figure it out with Mikkel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a2330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreFerPredictor(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = CustomExcelFormer(vocab_size=VOCAB_SIZE, \n",
    "                            hidden_size=HIDDEN_SIZE, \n",
    "                            out_size=ENCODING_SIZE,\n",
    "                            n_years=14,\n",
    "                            num_heads=NUM_HEADS,\n",
    "                            num_layers=NUM_LAYERS, \n",
    "                            num_classes=2,\n",
    "                            sequence_len=SEQ_LEN, \n",
    "                            aium_dropout=0.3,\n",
    "                            diam_dropout=0.1,\n",
    "                            residual_dropout=0.05,\n",
    "                            embedding_dropout=0.2,\n",
    "                            mixup=None).to(device)\n",
    "        self.decoder = GRUDecoder(\n",
    "            input_size=ENCODING_SIZE,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_layers=3,\n",
    "            max_seq_len=14,\n",
    "            dropout=0.3,\n",
    "            bidirectional=True,\n",
    "            with_attention = True\n",
    "        ).to(device)\n",
    "\n",
    "        self.enc_dropout = nn.Dropout(p=0.2)\n",
    "    def forward(self, input_year, input_seq, labels):\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings, _ = self.encoder(input_year, input_seq)#, y=labels.unsqueeze(-1).expand(-1, 14).reshape(-1), mixup_encoded=True)\n",
    "        encodings = encodings.view(bs,ss, -1)\n",
    "        encodings = self.enc_dropout(encodings)\n",
    "        mask = ~((input_seq == 101).sum(-1) == SEQ_LEN).view(bs,ss).detach()\n",
    "\n",
    "        # Forward pass\n",
    "        out = self.decoder(encodings, mask=mask).flatten()\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf49964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Layer with the Dropout\n",
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = PreFerPredictor().to(device)\n",
    "# Define loss function and optimizer for RNN\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([1/0.2]).to(device))\n",
    "optimizer = torch.optim.RAdam(model.parameters() , lr=LR, weight_decay=1e-2, decoupled_weight_decay=True)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=LR, weight_decay=1e-2)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = NUM_EPOCHS, eta_min = 1e-4, last_epoch = -1)\n",
    "\n",
    "## Stochaistic Weight Averaging\n",
    "#avg_fn = optim.swa_utils.get_swa_avg_fn()\n",
    "avg_fn = optim.swa_utils.get_ema_avg_fn(0.9)\n",
    "avg_model = optim.swa_utils.AveragedModel(model, avg_fn=avg_fn, use_buffers=False)#multi_avg_fn=optim.swa_utils.get_swa_multi_avg_fn())\n",
    "avg_start = 3\n",
    "avg_scheduler = optim.swa_utils.SWALR(optimizer, swa_lr=1e-3) #typically has a high lr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "avg_model.train()\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649bc948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(epoch):\n",
    "    val_loss = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    model.eval()\n",
    "    avg_model.eval()\n",
    "    for batch in data.val_dataloader:\n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "        input_year, input_seq = inputs\n",
    "        if epoch <= avg_start:\n",
    "            output = model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "        else:\n",
    "            output = avg_model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "\n",
    "        probs = F.sigmoid(output).flatten()\n",
    "        loss = loss_fn(output, labels)  \n",
    "        val_loss.append(loss.detach().cpu().numpy())\n",
    "        preds.extend(probs.detach().cpu().numpy().tolist())\n",
    "        targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    # Concatenate all the batches\n",
    "    yhat = torch.tensor(preds).flatten().detach().cpu().numpy()\n",
    "    ytrue = torch.tensor(targets).flatten().cpu().numpy()\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(ytrue, yhat > 0.5, average='binary')\n",
    "    map_roc = average_precision_score(ytrue, yhat)\n",
    "    print(f\"-- mAP Score: {map_roc:.4f} -- f1-score: {f1:.3f}\")\n",
    "    model.train()\n",
    "    avg_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf1777-49e7-462d-ae51-549ea6c8305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 79it [00:14,  5.32it/s, mean loss: 1.598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 1.5980\n",
      "-- mAP Score: 0.4899 -- f1-score: 0.420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 1: 79it [00:14,  5.53it/s, mean loss: 1.198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Loss: 1.1982\n",
      "-- mAP Score: 0.6227 -- f1-score: 0.604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 2: 79it [00:14,  5.52it/s, mean loss: 0.774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Loss: 0.7742\n",
      "-- mAP Score: 0.6986 -- f1-score: 0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 3: 79it [00:14,  5.53it/s, mean loss: 0.565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Loss: 0.5650\n",
      "-- mAP Score: 0.6413 -- f1-score: 0.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 4: 79it [00:14,  5.43it/s, mean loss: 0.333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Loss: 0.3332\n",
      "-- mAP Score: 0.6973 -- f1-score: 0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 5: 79it [00:14,  5.46it/s, mean loss: 0.365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Loss: 0.3654\n",
      "-- mAP Score: 0.7158 -- f1-score: 0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 6: 79it [00:14,  5.28it/s, mean loss: 0.208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Loss: 0.2081\n",
      "-- mAP Score: 0.6895 -- f1-score: 0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 7: 79it [00:14,  5.45it/s, mean loss: 0.140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15, Loss: 0.1395\n",
      "-- mAP Score: 0.6832 -- f1-score: 0.744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 8: 79it [00:14,  5.38it/s, mean loss: 0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15, Loss: 0.0623\n",
      "-- mAP Score: 0.6800 -- f1-score: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 9: 79it [00:14,  5.45it/s, mean loss: 0.076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15, Loss: 0.0756\n",
      "-- mAP Score: 0.6778 -- f1-score: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 10: 79it [00:14,  5.55it/s, mean loss: 0.044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15, Loss: 0.0439\n",
      "-- mAP Score: 0.6766 -- f1-score: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 11: 79it [00:14,  5.53it/s, mean loss: 0.042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15, Loss: 0.0419\n",
      "-- mAP Score: 0.6802 -- f1-score: 0.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 12: 79it [00:14,  5.32it/s, mean loss: 0.041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15, Loss: 0.0414\n",
      "-- mAP Score: 0.6824 -- f1-score: 0.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 13: 79it [00:14,  5.51it/s, mean loss: 0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Loss: 0.3123\n",
      "-- mAP Score: 0.6883 -- f1-score: 0.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 14: 20it [00:03,  5.34it/s, mean loss: 0.502]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     loss_per_step\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     17\u001b[0m     loop_object\u001b[38;5;241m.\u001b[39mset_postfix_str(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean loss: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(loss_per_step[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:]))\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# AVERAGING starts only after 2 epochs\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_per_epoch = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # print(epoch)\n",
    "    loss_per_step = []\n",
    "    loop_object  = tqdm(enumerate(data.train_dataloader), desc=f\"Epochs {epoch}\")\n",
    "    for i, batch in loop_object :        \n",
    "        optimizer.zero_grad() \n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "        input_year, input_seq = inputs\n",
    "        ### Model\n",
    "        output = model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "        probs = F.sigmoid(output).flatten()\n",
    "        ### Loss\n",
    "        loss = loss_fn(output, labels)  \n",
    "        loss_per_step.append(loss.detach().cpu().numpy())\n",
    "        loop_object.set_postfix_str(\"mean loss: %.3f\"%np.mean(loss_per_step[-100:]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # AVERAGING starts only after 2 epochs\n",
    "    if epoch > avg_start:\n",
    "        avg_model.update_parameters(model)\n",
    "        avg_scheduler.step()\n",
    "    else:\n",
    "        scheduler.step()\n",
    "    # On epoch end\n",
    "    loss_per_epoch.append(np.mean(loss_per_step))\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss_per_epoch[-1]:.4f}\")\n",
    "    run_validation(epoch=epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7200\n",
      "Recall: 0.7200\n",
      "F1 Score: 0.7200\n",
      "-- mAP Score: 0.8111 --\n"
     ]
    }
   ],
   "source": [
    "test_loss = []\n",
    "preds = []\n",
    "targets = []\n",
    "\n",
    "## Set both models into the eval mode.=\n",
    "avg_model.eval()\n",
    "for batch in data.test_dataloader:\n",
    "    inputs, labels = batch\n",
    "    labels = labels.to(torch.float).to(device)\n",
    "    input_year, input_seq = inputs\n",
    "    ### Model\n",
    "    output = avg_model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "    probs = F.sigmoid(output).flatten()\n",
    "\n",
    "    loss = loss_fn(output, labels)  \n",
    "    test_loss.append(loss.detach().cpu().numpy())\n",
    "    preds.extend(probs.detach().cpu().numpy().tolist())\n",
    "    targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "# Concatenate all the batches\n",
    "probs = torch.tensor(preds).flatten()\n",
    "actuals = torch.tensor(targets).flatten()\n",
    "# Concatenate all the batches\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(actuals.cpu().numpy(), probs.cpu().numpy() > 0.5, average='binary')\n",
    "map_roc = average_precision_score(actuals.numpy(), probs.numpy())\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"-- mAP Score: {map_roc:.4f} --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e2b8e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import umap\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "w = avg_model.get_submodule(\"module\").encoder.embedding.question_embedding.weight.detach().cpu().numpy()\n",
    "projector = umap.UMAP(n_components=2)\n",
    "wp = projector.fit_transform(w)\n",
    "plt.scatter(wp[:,0], wp[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e75554",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dataset import FinetuningDataset\n",
    "from model.dataset import PretrainingDataset\n",
    "from sklearn.model_selection import KFold\n",
    "# read in data and prepare transformations\n",
    "data = pd.read_csv(\"data/training_data/PreFer_train_data.csv\", low_memory=False)\n",
    "targets = pd.read_csv('data/training_data/PreFer_train_outcome.csv')\n",
    "targets = targets[targets.new_child.notna()].reset_index(drop=True)\n",
    "codebook = pd.read_csv('data/codebooks/PreFer_codebook.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11969897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lmmi/fertility-prediction-challenge/data_processing/pipeline.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  codebook[\"pairs\"] = codebook['var_name'].apply(get_generic_name)\n"
     ]
    }
   ],
   "source": [
    "n_features = 100\n",
    "\n",
    "importance = pd.read_csv('features_importance_1000.csv')\n",
    "custom_pairs = importance.iloc[:n_features].feature.map(lambda x: get_generic_name(x))\n",
    "sequences = encoding_pipeline(data, codebook, custom_pairs=custom_pairs)\n",
    "\n",
    "rnn_data = {person_id: (\n",
    "        torch.tensor([year-2007 for year, _ in wave_responses.items()]).to(device),\n",
    "        torch.tensor([ wave_response for _, wave_response in wave_responses.items()]).to(device)\n",
    "        )\n",
    "        for person_id, wave_responses in sequences.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(HIDDEN_SIZE=64,\n",
    "               ENCODING_SIZE=64,\n",
    "               NUM_COLS=44,\n",
    "               num_epochs_ft=5,\n",
    "               learning_rate_ft=1e-3,\n",
    "               sequences = []\n",
    "               ):\n",
    "\n",
    "    pretrain_dataset = PretrainingDataset(sequences)\n",
    "    SEQ_LEN = pretrain_dataset.get_seq_len()\n",
    "    VOCAB_SIZE = pretrain_dataset.get_vocab_size()\n",
    "\n",
    "    encoder = TabularEncoder(vocab_size=VOCAB_SIZE, \n",
    "                             embedding_size=HIDDEN_SIZE, \n",
    "                             output_size=ENCODING_SIZE, \n",
    "                             num_layers=2, \n",
    "                             sequence_len=SEQ_LEN, \n",
    "                             layer_type = \"excel\",\n",
    "                             num_cols=NUM_COLS,\n",
    "                             dropout=0.1\n",
    "                             ).to(device).to(device=device)\n",
    "\n",
    "    decoder = GRUDecoder(\n",
    "        input_size=ENCODING_SIZE,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=2,\n",
    "        max_seq_len=14,\n",
    "        dropout=0.2,\n",
    "        bidirectional=False,\n",
    "        with_attention = True\n",
    "    ).to(device)\n",
    "\n",
    "    # Define loss function and optimizer for RNN\n",
    "    ft_loss = nn.BCELoss()\n",
    "    ft_optimizer = torch.optim.NAdam(list(decoder.parameters()) + list(encoder.parameters()) , lr=learning_rate_ft, weight_decay=1e-2, decoupled_weight_decay=True)\n",
    "    ft_scheduler = optim.lr_scheduler.CosineAnnealingLR(ft_optimizer, T_max = num_epochs_ft, eta_min = 1e-6, last_epoch = -1)\n",
    "\n",
    "    # Training loop\n",
    "    decoder.train()\n",
    "    encoder.train()\n",
    "\n",
    "    return encoder, decoder, ft_optimizer, ft_loss, ft_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c190fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_and_step(loop_object, encoder, decoder, ft_scheduler, ft_loss, ft_optimizer):\n",
    "    for i, batch in loop_object :        \n",
    "\n",
    "        ft_optimizer.zero_grad() \n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "\n",
    "        input_year, input_seq = inputs\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings = encoder(input_year, input_seq).view(bs,ss, -1)\n",
    "        mask = ~((input_seq == 101).sum(-1) == NUM_COLS).view(bs,ss).detach()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = nn.functional.sigmoid(decoder(encodings, mask=mask))\n",
    "\n",
    "        loss = ft_loss(torch.flatten(outputs), labels)  \n",
    "\n",
    "        loss.backward()\n",
    "        ft_optimizer.step()\n",
    "\n",
    "    # On epoch end\n",
    "    ft_scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d56baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_dataloader, encoder, decoder):\n",
    "    val_loss = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    ## Set both models into the eval mode.=\n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "    for batch in test_dataloader:\n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "\n",
    "        input_year, input_seq = inputs\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings = encoder(input_year, input_seq).view(bs,ss, -1)\n",
    "        mask = ~((input_seq == 101).sum(-1) == NUM_COLS).view(bs,ss).detach()\n",
    "\n",
    "        # Forward pass\n",
    "        xx = decoder(encodings, mask)\n",
    "        outputs = torch.nn.functional.sigmoid(xx).flatten()\n",
    "        loss = ft_loss(outputs, labels)  \n",
    "        val_loss.append(loss.detach().cpu().numpy())\n",
    "        preds.extend(outputs.detach().cpu().numpy().tolist())\n",
    "        targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "\n",
    "    # Concatenate all the batches\n",
    "    predictions = (torch.tensor(preds) > 0.5).float()\n",
    "    probs = F.sigmoid(predictions)\n",
    "    actuals = torch.tensor(targets).flatten()\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(actuals.cpu().numpy(), predictions.cpu().numpy(), average='binary')\n",
    "    map_roc = average_precision_score(actuals.numpy(), probs.numpy())\n",
    "    \n",
    "    return precision, recall, f1, map_roc\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b22c2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 74it [00:14,  5.09it/s]\n",
      "Epochs 1: 74it [00:09,  8.13it/s]\n",
      "Epochs 2: 74it [00:09,  7.96it/s]\n",
      "Epochs 3: 74it [00:09,  7.82it/s]\n",
      "Epochs 4: 74it [00:09,  8.01it/s]\n",
      "Epochs 5: 74it [00:09,  7.93it/s]\n",
      "Epochs 6: 74it [00:09,  8.12it/s]\n",
      "Epochs 7: 74it [00:09,  8.19it/s]\n",
      "Epochs 8: 74it [00:08,  8.25it/s]\n",
      "Epochs 9: 74it [00:08,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 74it [00:08,  8.30it/s]\n",
      "Epochs 1: 74it [00:08,  8.26it/s]\n",
      "Epochs 2: 74it [00:08,  8.33it/s]\n",
      "Epochs 3: 74it [00:09,  8.14it/s]\n",
      "Epochs 4: 74it [00:09,  7.91it/s]\n",
      "Epochs 5: 74it [00:08,  8.26it/s]\n",
      "Epochs 6: 74it [00:08,  8.25it/s]\n",
      "Epochs 7: 74it [00:08,  8.37it/s]\n",
      "Epochs 8: 74it [00:08,  8.31it/s]\n",
      "Epochs 9: 74it [00:09,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 74it [00:09,  7.96it/s]\n",
      "Epochs 1: 74it [00:08,  8.23it/s]\n",
      "Epochs 2: 74it [00:09,  8.01it/s]\n",
      "Epochs 3: 74it [00:08,  8.27it/s]\n",
      "Epochs 4: 74it [00:08,  8.33it/s]\n",
      "Epochs 5: 74it [00:09,  8.06it/s]\n",
      "Epochs 6: 74it [00:09,  8.20it/s]\n",
      "Epochs 7: 74it [00:09,  7.96it/s]\n",
      "Epochs 8: 74it [00:09,  8.05it/s]\n",
      "Epochs 9: 74it [00:09,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 75it [00:09,  8.07it/s]\n",
      "Epochs 1: 75it [00:09,  8.03it/s]\n",
      "Epochs 2: 75it [00:09,  8.11it/s]\n",
      "Epochs 3: 75it [00:09,  8.24it/s]\n",
      "Epochs 4: 75it [00:09,  8.26it/s]\n",
      "Epochs 5: 75it [00:09,  8.13it/s]\n",
      "Epochs 6: 75it [00:09,  8.12it/s]\n",
      "Epochs 7: 75it [00:09,  8.16it/s]\n",
      "Epochs 8: 75it [00:09,  8.28it/s]\n",
      "Epochs 9: 75it [00:09,  8.27it/s]\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE=64\n",
    "ENCODING_SIZE=64\n",
    "NUM_COLS=44\n",
    "num_epochs_ft=10\n",
    "learning_rate_ft=1e-3\n",
    "rnn_batch_size = 10\n",
    "\n",
    "n_splits = 4\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare for cross-validation\n",
    "prec_per_fold = []\n",
    "rec_per_fold = []\n",
    "f1_per_fold = []\n",
    "map_roc_per_fold = []\n",
    "\n",
    "train_prec_per_fold = []\n",
    "train_rec_per_fold = []\n",
    "train_f1_per_fold = []\n",
    "train_map_roc_per_fold = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(targets['nomem_encr'])):\n",
    "    print(f'Fold: {fold}')\n",
    "    train_person_ids = targets.loc[train_index, 'nomem_encr']\n",
    "    test_person_ids = targets.loc[val_index, 'nomem_encr']\n",
    "    \n",
    "    encoder, decoder, ft_optimizer, ft_loss, ft_scheduler = initialize(\n",
    "        HIDDEN_SIZE=64,\n",
    "        ENCODING_SIZE=64,\n",
    "        NUM_COLS=44,\n",
    "        num_epochs_ft=1,\n",
    "        learning_rate_ft=1e-3,\n",
    "        sequences=sequences,\n",
    "        )\n",
    "\n",
    "    train_data = {person_id: rnn_data[person_id] for person_id in train_person_ids}\n",
    "    test_data = {person_id: rnn_data[person_id] for person_id in test_person_ids}\n",
    "    \n",
    "    train_dataset = FinetuningDataset(train_data, targets = targets)\n",
    "    test_dataset = FinetuningDataset(test_data, targets = targets)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=rnn_batch_size, shuffle=True)\n",
    "    test_dataloader  = DataLoader(test_dataset,  batch_size=rnn_batch_size)\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs_ft):\n",
    "        loop_object  = tqdm(enumerate(train_dataloader), desc=f\"Epochs {epoch}\")\n",
    "    \n",
    "        evaluate_and_step(loop_object, encoder, decoder, ft_scheduler, ft_loss, ft_optimizer)\n",
    "\n",
    "    precision, recall, f1, map_roc = evaluate(test_dataloader, encoder, decoder)\n",
    "    precision_train, recall_train, f1_train, map_roc_train = evaluate(train_dataloader, encoder, decoder)\n",
    "    \n",
    "    prec_per_fold.append(precision)\n",
    "    rec_per_fold.append(recall)\n",
    "    f1_per_fold.append(f1)\n",
    "    map_roc_per_fold.append(map_roc)\n",
    "\n",
    "    train_prec_per_fold.append(precision_train)\n",
    "    train_rec_per_fold.append(recall_train)\n",
    "    train_f1_per_fold.append(f1_train)\n",
    "    train_map_roc_per_fold.append(map_roc_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e95d939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on test set\n",
      "Prec: 0.783 0.714 0.844 0.552\n",
      "Recall: 0.310 0.490 0.731 0.941\n",
      "f1: 0.444 0.581 0.784 0.696\n",
      "map roc: 0.405 0.455 0.674 0.531\n"
     ]
    }
   ],
   "source": [
    "print(\"Results on test set\")\n",
    "print(\"Prec:\", ' '.join(f\"{x:.3f}\" for x in prec_per_fold))\n",
    "print(\"Recall:\", ' '.join(f\"{x:.3f}\" for x in rec_per_fold))\n",
    "print(\"f1:\", ' '.join(f\"{x:.3f}\" for x in f1_per_fold))\n",
    "print(\"map roc:\", ' '.join(f\"{x:.3f}\" for x in map_roc_per_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on training set\n",
      "Prec: 0.978 0.964 0.901 0.708\n",
      "Recall: 0.565 0.839 0.912 0.963\n",
      "f1: 0.716 0.897 0.907 0.816\n",
      "map roc: 0.643 0.844 0.841 0.689\n"
     ]
    }
   ],
   "source": [
    "print('Results on training set')\n",
    "print(\"Prec:\", ' '.join(f\"{x:.3f}\" for x in train_prec_per_fold))\n",
    "print(\"Recall:\", ' '.join(f\"{x:.3f}\" for x in train_rec_per_fold))\n",
    "print(\"f1:\", ' '.join(f\"{x:.3f}\" for x in train_f1_per_fold))\n",
    "print(\"map roc:\", ' '.join(f\"{x:.3f}\" for x in train_map_roc_per_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110e679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f28042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PreFer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
