{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9914262-7a3e-4e3a-9e07-043d40efd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data packages\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, average_precision_score\n",
    "\n",
    "from model.rnn import GRUDecoder\n",
    "from model.encoders import CustomExcelFormer\n",
    "from data_processing.pipeline import encoding_pipeline, get_generic_name\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from model.utils import get_device\n",
    "from model.dataset import PretrainingDataset\n",
    "from model.dataset import FinetuningDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18aee1dd-7884-40e7-be05-db070f1a452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders) device\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b651aa-752a-4c71-990a-332ff4099791",
   "metadata": {},
   "source": [
    "# Read the data\n",
    "\n",
    "Right now the notebook is set to work with fake data. This can be changed once the pipeline works.\n",
    "\n",
    "The data is stored as a Dict[person_id, Sequences] where Sequences is a Dict[year, survery_wave_response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b58426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(n_cols: int):\n",
    "    # read in data and prepare transformations\n",
    "    data = pd.read_csv(\"data/training_data/PreFer_train_data.csv\", low_memory=False)\n",
    "    targets_df = pd.read_csv('data/training_data/PreFer_train_outcome.csv')\n",
    "    codebook = pd.read_csv('data/codebooks/PreFer_codebook.csv')\n",
    "\n",
    "    importance = pd.read_csv('features_importance_all.csv')\n",
    "    custom_pairs = importance.feature.map(lambda x: get_generic_name(x)).unique()[:n_cols]\n",
    "    \"Unique columns count: %s\" %len(set(custom_pairs))\n",
    "\n",
    "    # check if sequences have been preprocessed (saves time)\n",
    "    sequences = encoding_pipeline(data, codebook, custom_pairs=custom_pairs, importance=importance, use_codebook=True)\n",
    "    pretrain_dataset = PretrainingDataset(sequences)\n",
    "    return sequences, pretrain_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b872e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataClass:\n",
    "    def __init__(self,\n",
    "                 data_path: str = \"data/training_data/PreFer_train_data.csv\",\n",
    "                 targets_path: str = 'data/training_data/PreFer_train_outcome.csv',\n",
    "                 codebook_path: str = 'data/codebooks/PreFer_codebook.csv',\n",
    "                 importance_path: str = 'features_importance_all.csv') -> None:\n",
    "        self.data = pd.read_csv(data_path, low_memory=False)\n",
    "        self.targets = pd.read_csv(targets_path)\n",
    "        self.codebook = pd.read_csv(codebook_path)\n",
    "        self.col_importance = pd.read_csv(importance_path)\n",
    "    def make_sequences(self, n_cols: int, use_codebook: bool = True):\n",
    "        custom_pairs = self.col_importance.feature.map(lambda x: get_generic_name(x)).unique()[:n_cols]\n",
    "        self.sequences = encoding_pipeline(self.data, self.codebook, \n",
    "                                           custom_pairs=custom_pairs, \n",
    "                                           importance=self.col_importance, \n",
    "                                           use_codebook=use_codebook)\n",
    "    def make_pretraining(self):\n",
    "        self.pretrain_dataset = PretrainingDataset(self.sequences)\n",
    "        self.seq_len = self.pretrain_dataset.get_seq_len()\n",
    "        self.vocab_size = self.pretrain_dataset.get_vocab_size()\n",
    "    def make_finetuning(self, batch_size, test_size: float = 0.2, val_size: float = 0.2):\n",
    "        targets = self.targets[self.targets.new_child.notna()]\n",
    "        train_person_ids, test_person_ids = train_test_split(targets['nomem_encr'], test_size=test_size, random_state=42)\n",
    "        train_person_ids, val_person_ids = train_test_split(train_person_ids, test_size=val_size, random_state=42)\n",
    "        rnn_data = {person_id: (\n",
    "                torch.tensor([year-2007 for year, _ in wave_responses.items()]).to(device),\n",
    "                torch.tensor([ wave_response for _, wave_response in wave_responses.items()]).to(device)\n",
    "                )\n",
    "                for person_id, wave_responses in self.sequences.items()\n",
    "                }\n",
    "\n",
    "        # split data based on the splits made for the target\n",
    "        train_data = {person_id: rnn_data[person_id] for person_id in train_person_ids}\n",
    "        val_data = {person_id: rnn_data[person_id] for person_id in val_person_ids}\n",
    "        test_data = {person_id: rnn_data[person_id] for person_id in test_person_ids}\n",
    "\n",
    "        self.train_dataset = FinetuningDataset(train_data, targets = targets)\n",
    "        self.val_dataset = FinetuningDataset(val_data, targets = targets)\n",
    "        self.test_dataset = FinetuningDataset(test_data, targets = targets)\n",
    "        self.train_dataloader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.val_dataloader = DataLoader(self.val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.test_dataloader  = DataLoader(self.test_dataset,  batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4e1c0",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "279b9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f7e4345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomarx/Documents/GitHub/fertility-prediction-challenge/data_processing/pipeline.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  codebook[\"pairs\"] = codebook['var_name'].apply(get_generic_name)\n"
     ]
    }
   ],
   "source": [
    "data.make_sequences(n_cols=50)\n",
    "data.make_pretraining()\n",
    "data.make_finetuning(batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d77d87a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "### Initialization of the Autoencoder \n",
    "#ENCODING_SIZE = 64\n",
    "BATCH_SIZE = 16\n",
    "HIDDEN_SIZE = 64\n",
    "ENCODING_SIZE = 64\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 3\n",
    "NUM_EPOCHS = 10\n",
    "DETECT_ANOMALY = False\n",
    "SEQ_LEN = data.seq_len\n",
    "VOCAB_SIZE = data.vocab_size\n",
    "\n",
    "LR = 1e-3\n",
    "\n",
    "assert HIDDEN_SIZE % NUM_HEADS == 0, \"Check that the hidden size is divisible\"\n",
    "print(SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddb66e-cba5-4bb9-854d-811d49599b93",
   "metadata": {},
   "source": [
    "# Experimental Encoder (Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada8018b-7ebc-4db4-aadf-191b214b1470",
   "metadata": {},
   "source": [
    "# Train the RNN\n",
    "\n",
    "First we need to create Dataset class that can hold both the target (stored in a pd.DataFrame) and the sequences.\n",
    "\n",
    "The sequences will be of dimension 14 x encoding_dimension, because we have 14 years of surveys.\n",
    "\n",
    "I have created some code for getting the data into the right format, but it might not be useful.\n",
    "\n",
    "## Regarding masks\n",
    "Right now the masking is done already in the encoding. I haven't found exactly where Mikkel implemented this.\n",
    "So for now, assume that nothing is padded, and then we'll figure it out with Mikkel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "307bd370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Layer with the Dropout\n"
     ]
    }
   ],
   "source": [
    "encoder = CustomExcelFormer(vocab_size=VOCAB_SIZE, \n",
    "                            hidden_size=HIDDEN_SIZE, \n",
    "                            out_size=ENCODING_SIZE,\n",
    "                            n_years=14,\n",
    "                            num_heads=NUM_HEADS,\n",
    "                            num_layers=NUM_LAYERS, \n",
    "                            num_classes=2,\n",
    "                            sequence_len=SEQ_LEN, \n",
    "                            aium_dropout=0.3,\n",
    "                            diam_dropout=0.2,\n",
    "                            residual_dropout=0.1,\n",
    "                            embedding_dropout=0.1,\n",
    "                            mixup=None,\n",
    "                            beta=0.2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caf49964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomarx/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/carlomarx/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "decoder = GRUDecoder(\n",
    "    input_size=ENCODING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=3,\n",
    "    max_seq_len=14,\n",
    "    dropout=0.15,\n",
    "    bidirectional=True,\n",
    "    with_attention = True\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Define loss function and optimizer for RNN\n",
    "ft_loss = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([1/0.2]).to(device))\n",
    "ft_optimizer = torch.optim.RAdam(list(decoder.parameters()) + list(encoder.parameters()) , lr=LR, weight_decay=1e-2, decoupled_weight_decay=True)\n",
    "ft_scheduler = optim.lr_scheduler.CosineAnnealingLR(ft_optimizer, T_max = NUM_EPOCHS, eta_min = 1e-4, last_epoch = -1)\n",
    "\n",
    "# Training loop\n",
    "decoder.train()\n",
    "encoder.train()\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "649bc948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation():\n",
    "    val_loss = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    ## Set both models into the eval mode.=\n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "    for batch in data.val_dataloader:\n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "\n",
    "        input_year, input_seq = inputs\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings, _ = encoder(input_year, input_seq)\n",
    "        encodings = encodings.view(bs,ss, -1)\n",
    "        mask = ~((input_seq == 101).sum(-1) == SEQ_LEN).view(bs,ss).detach()\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        xx = decoder(encodings, mask)\n",
    "        outputs = F.sigmoid(xx).flatten()\n",
    "        loss = ft_loss(xx.flatten(), labels)  \n",
    "        val_loss.append(loss.detach().cpu().numpy())\n",
    "        preds.extend(outputs.detach().cpu().numpy().tolist())\n",
    "        targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    # Concatenate all the batches\n",
    "    probs = torch.tensor(preds).flatten()\n",
    "    actuals = torch.tensor(targets).flatten()\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(actuals.cpu().numpy(), probs.cpu().numpy() > 0.5, average='binary')\n",
    "    map_roc = average_precision_score(actuals.numpy(), probs.numpy())\n",
    "    print(f\"-- mAP Score: {map_roc:.4f} -- f1-score: {f1:.3f}\")\n",
    "    decoder.train()\n",
    "    encoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e741d2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:00, 933.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1918)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop_object  = tqdm(enumerate(data.train_dataloader))\n",
    "\n",
    "length = 0\n",
    "pos = 0\n",
    "for i, batch in loop_object :        \n",
    "        inputs, labels = batch\n",
    "        length +=labels.shape[0]\n",
    "        pos += (labels == 1).sum()\n",
    "\n",
    "pos/length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82cf1777-49e7-462d-ae51-549ea6c8305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 40it [00:08,  4.66it/s, mean loss: 1.229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.2294\n",
      "-- mAP Score: 0.5312 -- f1-score: 0.305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 1: 40it [00:05,  6.71it/s, mean loss: 1.089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.0890\n",
      "-- mAP Score: 0.6094 -- f1-score: 0.592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 2: 40it [00:05,  6.80it/s, mean loss: 0.840]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.8400\n",
      "-- mAP Score: 0.6606 -- f1-score: 0.571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 3: 40it [00:05,  6.70it/s, mean loss: 0.740]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.7400\n",
      "-- mAP Score: 0.7212 -- f1-score: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 4: 40it [00:05,  6.77it/s, mean loss: 0.618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.6176\n",
      "-- mAP Score: 0.7617 -- f1-score: 0.759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 5: 40it [00:05,  6.83it/s, mean loss: 0.558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.5579\n",
      "-- mAP Score: 0.7660 -- f1-score: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 6: 40it [00:05,  6.79it/s, mean loss: 0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.5319\n",
      "-- mAP Score: 0.7712 -- f1-score: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 7: 40it [00:05,  6.67it/s, mean loss: 0.461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.4612\n",
      "-- mAP Score: 0.7849 -- f1-score: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 8: 40it [00:05,  6.75it/s, mean loss: 0.468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.4684\n",
      "-- mAP Score: 0.7741 -- f1-score: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 9: 40it [00:05,  6.77it/s, mean loss: 0.415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.4149\n",
      "-- mAP Score: 0.7902 -- f1-score: 0.738\n"
     ]
    }
   ],
   "source": [
    "loss_per_epoch = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # print(epoch)\n",
    "    loss_per_step = []\n",
    "    loop_object  = tqdm(enumerate(data.train_dataloader), desc=f\"Epochs {epoch}\")\n",
    "    for i, batch in loop_object :        \n",
    "        ft_optimizer.zero_grad() \n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "        input_year, input_seq = inputs\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings, _ = encoder(input_year, input_seq)#, y=labels.unsqueeze(-1).expand(-1, 14).reshape(-1), mixup_encoded=True)\n",
    "        encodings = encodings.view(bs,ss, -1)\n",
    "        mask = ~((input_seq == 101).sum(-1) == SEQ_LEN).view(bs,ss).detach()\n",
    "\n",
    "        # Forward pass\n",
    "        xx = decoder(encodings, mask=mask).flatten()\n",
    "        outputs = F.sigmoid(xx)\n",
    "\n",
    "        loss = ft_loss(xx, labels)  \n",
    "        loss_per_step.append(loss.detach().cpu().numpy())\n",
    "        loop_object.set_postfix_str(\"mean loss: %.3f\"%np.mean(loss_per_step[-100:]))\n",
    "\n",
    "        loss.backward()\n",
    "        ft_optimizer.step()\n",
    "    # On epoch end\n",
    "    loss_per_epoch.append(np.mean(loss_per_step))\n",
    "    ft_scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss_per_epoch[-1]:.4f}\")\n",
    "    run_validation()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0d2fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7500\n",
      "Recall: 0.7800\n",
      "F1 Score: 0.7647\n",
      "-- mAP Score: 0.8322 --\n"
     ]
    }
   ],
   "source": [
    "test_loss = []\n",
    "preds = []\n",
    "targets = []\n",
    "\n",
    "## Set both models into the eval mode.=\n",
    "decoder.eval()\n",
    "encoder.eval()\n",
    "for batch in data.test_dataloader:\n",
    "    inputs, labels = batch\n",
    "    labels = labels.to(torch.float).to(device)\n",
    "\n",
    "    input_year, input_seq = inputs\n",
    "    bs, ss = labels.size(0), 14\n",
    "    input_year = input_year.reshape(-1).to(device)\n",
    "    input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "    \n",
    "    encodings, _ = encoder(input_year, input_seq)\n",
    "    encodings = encodings.view(bs,ss, -1)\n",
    "    mask = ~((input_seq == 101).sum(-1) == SEQ_LEN).view(bs,ss).detach()\n",
    "\n",
    "\n",
    "    # Forward pass\n",
    "    xx = decoder(encodings, mask).flatten()\n",
    "    outputs = F.sigmoid(xx)\n",
    "    loss = ft_loss(xx, labels)  \n",
    "    test_loss.append(loss.detach().cpu().numpy())\n",
    "    preds.extend(outputs.detach().cpu().numpy().tolist())\n",
    "    targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "# Concatenate all the batches\n",
    "probs = torch.tensor(preds).flatten()\n",
    "actuals = torch.tensor(targets).flatten()\n",
    "# Concatenate all the batches\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(actuals.cpu().numpy(), probs.cpu().numpy() > 0.5, average='binary')\n",
    "map_roc = average_precision_score(actuals.numpy(), probs.numpy())\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"-- mAP Score: {map_roc:.4f} --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e2b8e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import umap\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "w = encoder.embedding.question_embedding.weight.detach().cpu().numpy()\n",
    "projector = umap.UMAP(n_components=2)\n",
    "wp = projector.fit_transform(w)\n",
    "plt.scatter(wp[:,0], wp[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e75554",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dataset import FinetuningDataset\n",
    "from model.dataset import PretrainingDataset\n",
    "from sklearn.model_selection import KFold\n",
    "# read in data and prepare transformations\n",
    "data = pd.read_csv(\"data/training_data/PreFer_train_data.csv\", low_memory=False)\n",
    "targets = pd.read_csv('data/training_data/PreFer_train_outcome.csv')\n",
    "targets = targets[targets.new_child.notna()].reset_index(drop=True)\n",
    "codebook = pd.read_csv('data/codebooks/PreFer_codebook.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11969897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lmmi/fertility-prediction-challenge/data_processing/pipeline.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  codebook[\"pairs\"] = codebook['var_name'].apply(get_generic_name)\n"
     ]
    }
   ],
   "source": [
    "n_features = 100\n",
    "\n",
    "importance = pd.read_csv('features_importance_1000.csv')\n",
    "custom_pairs = importance.iloc[:n_features].feature.map(lambda x: get_generic_name(x))\n",
    "sequences = encoding_pipeline(data, codebook, custom_pairs=custom_pairs)\n",
    "\n",
    "rnn_data = {person_id: (\n",
    "        torch.tensor([year-2007 for year, _ in wave_responses.items()]).to(device),\n",
    "        torch.tensor([ wave_response for _, wave_response in wave_responses.items()]).to(device)\n",
    "        )\n",
    "        for person_id, wave_responses in sequences.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(HIDDEN_SIZE=64,\n",
    "               ENCODING_SIZE=64,\n",
    "               NUM_COLS=44,\n",
    "               num_epochs_ft=5,\n",
    "               learning_rate_ft=1e-3,\n",
    "               sequences = []\n",
    "               ):\n",
    "\n",
    "    pretrain_dataset = PretrainingDataset(sequences)\n",
    "    SEQ_LEN = pretrain_dataset.get_seq_len()\n",
    "    VOCAB_SIZE = pretrain_dataset.get_vocab_size()\n",
    "\n",
    "    encoder = TabularEncoder(vocab_size=VOCAB_SIZE, \n",
    "                             embedding_size=HIDDEN_SIZE, \n",
    "                             output_size=ENCODING_SIZE, \n",
    "                             num_layers=2, \n",
    "                             sequence_len=SEQ_LEN, \n",
    "                             layer_type = \"excel\",\n",
    "                             num_cols=NUM_COLS,\n",
    "                             dropout=0.1\n",
    "                             ).to(device).to(device=device)\n",
    "\n",
    "    decoder = GRUDecoder(\n",
    "        input_size=ENCODING_SIZE,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=2,\n",
    "        max_seq_len=14,\n",
    "        dropout=0.2,\n",
    "        bidirectional=False,\n",
    "        with_attention = True\n",
    "    ).to(device)\n",
    "\n",
    "    # Define loss function and optimizer for RNN\n",
    "    ft_loss = nn.BCELoss()\n",
    "    ft_optimizer = torch.optim.NAdam(list(decoder.parameters()) + list(encoder.parameters()) , lr=learning_rate_ft, weight_decay=1e-2, decoupled_weight_decay=True)\n",
    "    ft_scheduler = optim.lr_scheduler.CosineAnnealingLR(ft_optimizer, T_max = num_epochs_ft, eta_min = 1e-6, last_epoch = -1)\n",
    "\n",
    "    # Training loop\n",
    "    decoder.train()\n",
    "    encoder.train()\n",
    "\n",
    "    return encoder, decoder, ft_optimizer, ft_loss, ft_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c190fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_and_step(loop_object, encoder, decoder, ft_scheduler, ft_loss, ft_optimizer):\n",
    "    for i, batch in loop_object :        \n",
    "\n",
    "        ft_optimizer.zero_grad() \n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "\n",
    "        input_year, input_seq = inputs\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings = encoder(input_year, input_seq).view(bs,ss, -1)\n",
    "        mask = ~((input_seq == 101).sum(-1) == NUM_COLS).view(bs,ss).detach()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = nn.functional.sigmoid(decoder(encodings, mask=mask))\n",
    "\n",
    "        loss = ft_loss(torch.flatten(outputs), labels)  \n",
    "\n",
    "        loss.backward()\n",
    "        ft_optimizer.step()\n",
    "\n",
    "    # On epoch end\n",
    "    ft_scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d56baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_dataloader, encoder, decoder):\n",
    "    val_loss = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    ## Set both models into the eval mode.=\n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "    for batch in test_dataloader:\n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "\n",
    "        input_year, input_seq = inputs\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings = encoder(input_year, input_seq).view(bs,ss, -1)\n",
    "        mask = ~((input_seq == 101).sum(-1) == NUM_COLS).view(bs,ss).detach()\n",
    "\n",
    "        # Forward pass\n",
    "        xx = decoder(encodings, mask)\n",
    "        outputs = torch.nn.functional.sigmoid(xx).flatten()\n",
    "        loss = ft_loss(outputs, labels)  \n",
    "        val_loss.append(loss.detach().cpu().numpy())\n",
    "        preds.extend(outputs.detach().cpu().numpy().tolist())\n",
    "        targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "\n",
    "    # Concatenate all the batches\n",
    "    predictions = (torch.tensor(preds) > 0.5).float()\n",
    "    probs = F.sigmoid(predictions)\n",
    "    actuals = torch.tensor(targets).flatten()\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(actuals.cpu().numpy(), predictions.cpu().numpy(), average='binary')\n",
    "    map_roc = average_precision_score(actuals.numpy(), probs.numpy())\n",
    "    \n",
    "    return precision, recall, f1, map_roc\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b22c2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 74it [00:14,  5.09it/s]\n",
      "Epochs 1: 74it [00:09,  8.13it/s]\n",
      "Epochs 2: 74it [00:09,  7.96it/s]\n",
      "Epochs 3: 74it [00:09,  7.82it/s]\n",
      "Epochs 4: 74it [00:09,  8.01it/s]\n",
      "Epochs 5: 74it [00:09,  7.93it/s]\n",
      "Epochs 6: 74it [00:09,  8.12it/s]\n",
      "Epochs 7: 74it [00:09,  8.19it/s]\n",
      "Epochs 8: 74it [00:08,  8.25it/s]\n",
      "Epochs 9: 74it [00:08,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 74it [00:08,  8.30it/s]\n",
      "Epochs 1: 74it [00:08,  8.26it/s]\n",
      "Epochs 2: 74it [00:08,  8.33it/s]\n",
      "Epochs 3: 74it [00:09,  8.14it/s]\n",
      "Epochs 4: 74it [00:09,  7.91it/s]\n",
      "Epochs 5: 74it [00:08,  8.26it/s]\n",
      "Epochs 6: 74it [00:08,  8.25it/s]\n",
      "Epochs 7: 74it [00:08,  8.37it/s]\n",
      "Epochs 8: 74it [00:08,  8.31it/s]\n",
      "Epochs 9: 74it [00:09,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 74it [00:09,  7.96it/s]\n",
      "Epochs 1: 74it [00:08,  8.23it/s]\n",
      "Epochs 2: 74it [00:09,  8.01it/s]\n",
      "Epochs 3: 74it [00:08,  8.27it/s]\n",
      "Epochs 4: 74it [00:08,  8.33it/s]\n",
      "Epochs 5: 74it [00:09,  8.06it/s]\n",
      "Epochs 6: 74it [00:09,  8.20it/s]\n",
      "Epochs 7: 74it [00:09,  7.96it/s]\n",
      "Epochs 8: 74it [00:09,  8.05it/s]\n",
      "Epochs 9: 74it [00:09,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 75it [00:09,  8.07it/s]\n",
      "Epochs 1: 75it [00:09,  8.03it/s]\n",
      "Epochs 2: 75it [00:09,  8.11it/s]\n",
      "Epochs 3: 75it [00:09,  8.24it/s]\n",
      "Epochs 4: 75it [00:09,  8.26it/s]\n",
      "Epochs 5: 75it [00:09,  8.13it/s]\n",
      "Epochs 6: 75it [00:09,  8.12it/s]\n",
      "Epochs 7: 75it [00:09,  8.16it/s]\n",
      "Epochs 8: 75it [00:09,  8.28it/s]\n",
      "Epochs 9: 75it [00:09,  8.27it/s]\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE=64\n",
    "ENCODING_SIZE=64\n",
    "NUM_COLS=44\n",
    "num_epochs_ft=10\n",
    "learning_rate_ft=1e-3\n",
    "rnn_batch_size = 10\n",
    "\n",
    "n_splits = 4\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare for cross-validation\n",
    "prec_per_fold = []\n",
    "rec_per_fold = []\n",
    "f1_per_fold = []\n",
    "map_roc_per_fold = []\n",
    "\n",
    "train_prec_per_fold = []\n",
    "train_rec_per_fold = []\n",
    "train_f1_per_fold = []\n",
    "train_map_roc_per_fold = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(targets['nomem_encr'])):\n",
    "    print(f'Fold: {fold}')\n",
    "    train_person_ids = targets.loc[train_index, 'nomem_encr']\n",
    "    test_person_ids = targets.loc[val_index, 'nomem_encr']\n",
    "    \n",
    "    encoder, decoder, ft_optimizer, ft_loss, ft_scheduler = initialize(\n",
    "        HIDDEN_SIZE=64,\n",
    "        ENCODING_SIZE=64,\n",
    "        NUM_COLS=44,\n",
    "        num_epochs_ft=1,\n",
    "        learning_rate_ft=1e-3,\n",
    "        sequences=sequences,\n",
    "        )\n",
    "\n",
    "    train_data = {person_id: rnn_data[person_id] for person_id in train_person_ids}\n",
    "    test_data = {person_id: rnn_data[person_id] for person_id in test_person_ids}\n",
    "    \n",
    "    train_dataset = FinetuningDataset(train_data, targets = targets)\n",
    "    test_dataset = FinetuningDataset(test_data, targets = targets)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=rnn_batch_size, shuffle=True)\n",
    "    test_dataloader  = DataLoader(test_dataset,  batch_size=rnn_batch_size)\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs_ft):\n",
    "        loop_object  = tqdm(enumerate(train_dataloader), desc=f\"Epochs {epoch}\")\n",
    "    \n",
    "        evaluate_and_step(loop_object, encoder, decoder, ft_scheduler, ft_loss, ft_optimizer)\n",
    "\n",
    "    precision, recall, f1, map_roc = evaluate(test_dataloader, encoder, decoder)\n",
    "    precision_train, recall_train, f1_train, map_roc_train = evaluate(train_dataloader, encoder, decoder)\n",
    "    \n",
    "    prec_per_fold.append(precision)\n",
    "    rec_per_fold.append(recall)\n",
    "    f1_per_fold.append(f1)\n",
    "    map_roc_per_fold.append(map_roc)\n",
    "\n",
    "    train_prec_per_fold.append(precision_train)\n",
    "    train_rec_per_fold.append(recall_train)\n",
    "    train_f1_per_fold.append(f1_train)\n",
    "    train_map_roc_per_fold.append(map_roc_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e95d939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on test set\n",
      "Prec: 0.783 0.714 0.844 0.552\n",
      "Recall: 0.310 0.490 0.731 0.941\n",
      "f1: 0.444 0.581 0.784 0.696\n",
      "map roc: 0.405 0.455 0.674 0.531\n"
     ]
    }
   ],
   "source": [
    "print(\"Results on test set\")\n",
    "print(\"Prec:\", ' '.join(f\"{x:.3f}\" for x in prec_per_fold))\n",
    "print(\"Recall:\", ' '.join(f\"{x:.3f}\" for x in rec_per_fold))\n",
    "print(\"f1:\", ' '.join(f\"{x:.3f}\" for x in f1_per_fold))\n",
    "print(\"map roc:\", ' '.join(f\"{x:.3f}\" for x in map_roc_per_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef64799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on training set\n",
      "Prec: 0.978 0.964 0.901 0.708\n",
      "Recall: 0.565 0.839 0.912 0.963\n",
      "f1: 0.716 0.897 0.907 0.816\n",
      "map roc: 0.643 0.844 0.841 0.689\n"
     ]
    }
   ],
   "source": [
    "print('Results on training set')\n",
    "print(\"Prec:\", ' '.join(f\"{x:.3f}\" for x in train_prec_per_fold))\n",
    "print(\"Recall:\", ' '.join(f\"{x:.3f}\" for x in train_rec_per_fold))\n",
    "print(\"f1:\", ' '.join(f\"{x:.3f}\" for x in train_f1_per_fold))\n",
    "print(\"map roc:\", ' '.join(f\"{x:.3f}\" for x in train_map_roc_per_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110e679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f28042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PreFer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
