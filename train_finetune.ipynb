{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c50e51",
   "metadata": {},
   "source": [
    "# ExcelFormer with the RNN\n",
    "The notebook contains the pipeline to train and validate the model (+ training the final version of the model on the full data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9914262-7a3e-4e3a-9e07-043d40efd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data packages\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, average_precision_score, f1_score, recall_score, precision_score, matthews_corrcoef\n",
    "\n",
    "from model.rnn import GRUDecoder\n",
    "from model.encoders import CustomExcelFormer\n",
    "from data_processing.pipeline import encoding_pipeline, get_generic_name\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from model.utils import get_device\n",
    "from model.dataset import PretrainingDataset\n",
    "from model.dataset import FinetuningDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2f2486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(preds, targs, metric, n_bootstraps: int):\n",
    "    \"\"\"\n",
    "    Returns mean and 95% Confidentence intervals for a metric\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        idx = np.random.choice(preds.shape[0], size=preds.shape[0], replace=True)\n",
    "        results.append(metric(targs[idx], preds[idx]))\n",
    "\n",
    "    return {\"mean\": np.mean(results),\n",
    "            \"CIs\": np.quantile(results,q=np.array([0.025, 0.975]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18aee1dd-7884-40e7-be05-db070f1a452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders) device\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b651aa-752a-4c71-990a-332ff4099791",
   "metadata": {},
   "source": [
    "# Read the data\n",
    "\n",
    "Right now the notebook is set to work with fake data. This can be changed once the pipeline works.\n",
    "\n",
    "The data is stored as a Dict[person_id, Sequences] where Sequences is a Dict[year, survery_wave_response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b872e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataClass:\n",
    "    def __init__(self,\n",
    "                 data_path: str = \"data/training_data/PreFer_train_data.csv\",\n",
    "                 targets_path: str = 'data/training_data/PreFer_train_outcome.csv',\n",
    "                 codebook_path: str = 'data/codebooks/PreFer_codebook.csv',\n",
    "                 importance_path: str = 'features_importance_all.csv') -> None:\n",
    "        self.data = pd.read_csv(data_path, low_memory=False)\n",
    "        self.targets = pd.read_csv(targets_path)\n",
    "        self.codebook = pd.read_csv(codebook_path)\n",
    "        self.col_importance = pd.read_csv(importance_path)\n",
    "    def make_sequences(self, n_cols: int, use_codebook: bool = True):\n",
    "        custom_pairs = self.col_importance.feature.map(lambda x: get_generic_name(x)).unique()[:n_cols]\n",
    "        self.sequences = encoding_pipeline(self.data, self.codebook, \n",
    "                                           custom_pairs=custom_pairs, \n",
    "                                           importance=self.col_importance, \n",
    "                                           use_codebook=use_codebook)\n",
    "    def make_pretraining(self):\n",
    "        self.pretrain_dataset = PretrainingDataset(self.sequences)\n",
    "        self.seq_len = self.pretrain_dataset.get_seq_len()\n",
    "        self.vocab_size = self.pretrain_dataset.get_vocab_size()\n",
    "    def make_full_finetuning(self, batch_size): \n",
    "        \"\"\"Create dataloader for the whole finetuning dataset\"\"\"\n",
    "        targets = self.targets[self.targets.new_child.notna()]\n",
    "        full_person_ids =  targets['nomem_encr'].values\n",
    "        rnn_data = {person_id: (\n",
    "                torch.tensor([year-2007 for year, _ in wave_responses.items()]).to(device),\n",
    "                torch.tensor([ wave_response for _, wave_response in wave_responses.items()]).to(device)\n",
    "                )\n",
    "                for person_id, wave_responses in self.sequences.items()\n",
    "                }\n",
    "\n",
    "        # split data based on the splits made for the target\n",
    "        full_data = {person_id: rnn_data[person_id] for person_id in full_person_ids}\n",
    "        self.full_dataset = FinetuningDataset(full_data, targets = targets)\n",
    "        self.full_dataloader = DataLoader(self.full_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    def make_finetuning(self, batch_size, test_size: float = 0.2, val_size: float = 0.2):\n",
    "        \"\"\"\n",
    "        Create dataloaders for the train/val/test splits.\n",
    "        \"\"\"\n",
    "        targets = self.targets[self.targets.new_child.notna()]\n",
    "        train_person_ids, test_person_ids = train_test_split(targets['nomem_encr'], test_size=test_size, random_state=42)\n",
    "        train_person_ids, val_person_ids = train_test_split(train_person_ids, test_size=val_size, random_state=42)\n",
    "        rnn_data = {person_id: (\n",
    "                torch.tensor([year-2007 for year, _ in wave_responses.items()]).to(device),\n",
    "                torch.tensor([ wave_response for _, wave_response in wave_responses.items()]).to(device)\n",
    "                )\n",
    "                for person_id, wave_responses in self.sequences.items()\n",
    "                }\n",
    "\n",
    "        # split data based on the splits made for the target\n",
    "        train_data = {person_id: rnn_data[person_id] for person_id in train_person_ids}\n",
    "        val_data = {person_id: rnn_data[person_id] for person_id in val_person_ids}\n",
    "        test_data = {person_id: rnn_data[person_id] for person_id in test_person_ids}\n",
    "\n",
    "        self.train_dataset = FinetuningDataset(train_data, targets = targets)\n",
    "        self.val_dataset = FinetuningDataset(val_data, targets = targets)\n",
    "        self.test_dataset = FinetuningDataset(test_data, targets = targets)\n",
    "        self.train_dataloader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.val_dataloader = DataLoader(self.val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.test_dataloader  = DataLoader(self.test_dataset,  batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "279b9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE DATA\n",
    "data = DataClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "393fa042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "BATCH_SIZE = 16\n",
    "HIDDEN_SIZE =  48\n",
    "ENCODING_SIZE = 48\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 5\n",
    "NUM_EPOCHS = 12\n",
    "DETECT_ANOMALY = False\n",
    "LR = 1e-2\n",
    "\n",
    "assert HIDDEN_SIZE % NUM_HEADS == 0, \"Check that the hidden size is divisible\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f7e4345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomarx/Documents/GitHub/fertility-prediction-challenge/data_processing/pipeline.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  codebook[\"pairs\"] = codebook['var_name'].apply(get_generic_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data.make_sequences(n_cols=150)\n",
    "data.make_pretraining()\n",
    "data.make_finetuning(batch_size=BATCH_SIZE)\n",
    "data.make_full_finetuning(batch_size=BATCH_SIZE)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d77d87a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = data.seq_len\n",
    "VOCAB_SIZE = data.vocab_size\n",
    "print(SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddb66e-cba5-4bb9-854d-811d49599b93",
   "metadata": {},
   "source": [
    "# Model and Training Setup\n",
    "Here we setup the model and training procedures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54a2330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreFerPredictor(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = CustomExcelFormer(vocab_size=VOCAB_SIZE, \n",
    "                            hidden_size=HIDDEN_SIZE, \n",
    "                            out_size=ENCODING_SIZE,\n",
    "                            n_years=14,\n",
    "                            num_heads=NUM_HEADS,\n",
    "                            num_layers=NUM_LAYERS, \n",
    "                            sequence_len=SEQ_LEN, \n",
    "                            aium_dropout=0.3,\n",
    "                            diam_dropout=0.1,\n",
    "                            residual_dropout=0.1,\n",
    "                            embedding_dropout=0.25).to(device)\n",
    "        self.decoder = GRUDecoder(\n",
    "            input_size=ENCODING_SIZE,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_layers=2,\n",
    "            max_seq_len=14,\n",
    "            dropout=0.25,\n",
    "  \n",
    "            bidirectional=True,\n",
    "            with_attention = True\n",
    "        ).to(device)\n",
    "\n",
    "        self.enc_dropout = nn.Dropout(0.1)\n",
    "        self.enc_dropout1d = nn.Dropout1d(0.1)\n",
    "    def forward(self, input_year, input_seq, labels):\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings, _ = self.encoder(input_year, input_seq)#, y=labels.unsqueeze(-1).expand(-1, 14).reshape(-1), mixup_encoded=True)\n",
    "        encodings = encodings.view(bs,ss, -1)\n",
    "        encodings = self.enc_dropout(encodings)\n",
    "        encodings = self.enc_dropout1d(encodings)\n",
    "        mask = ~((input_seq == 101).sum(-1) == SEQ_LEN).view(bs,ss).detach()\n",
    "        # Forward pass\n",
    "        out = self.decoder(encodings, mask=mask).flatten()\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "caf49964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "model = PreFerPredictor().to(device)\n",
    "# Define the loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([1/0.2]).to(device))\n",
    "# Define the optimization strategy\n",
    "optimizer = torch.optim.RAdam(model.parameters() , lr=LR, weight_decay=1e-2, decoupled_weight_decay=True)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = NUM_EPOCHS * len(data.train_dataloader), eta_min = 8e-4, last_epoch = -1)\n",
    "## EMA Weight Averaging\n",
    "avg_fn = optim.swa_utils.get_ema_avg_fn(0.99)\n",
    "avg_model = optim.swa_utils.AveragedModel(model, avg_fn=avg_fn, use_buffers=False)\n",
    "avg_start = 3\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "avg_model.train()\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "649bc948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(epoch):\n",
    "    \"\"\"\n",
    "    Run the validation loop\n",
    "    \"\"\"\n",
    "    val_loss = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    model.eval()\n",
    "    avg_model.eval()\n",
    "    for batch in data.val_dataloader:\n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "        input_year, input_seq = inputs\n",
    "        if epoch <= avg_start:\n",
    "            output = model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "        else:\n",
    "            output = avg_model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "\n",
    "        probs = F.sigmoid(output).flatten()\n",
    "        loss = loss_fn(output, labels)  \n",
    "        val_loss.append(loss.detach().cpu().numpy())\n",
    "        preds.extend(probs.detach().cpu().numpy().tolist())\n",
    "        targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    # Concatenate all the batches\n",
    "    yhat = torch.tensor(preds).flatten().detach().cpu().numpy()\n",
    "    ytrue = torch.tensor(targets).flatten().cpu().numpy()\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(ytrue, yhat > 0.5, average='binary')\n",
    "    mcc = matthews_corrcoef(ytrue, yhat > 0.5)\n",
    "    map_roc = average_precision_score(ytrue, yhat)\n",
    "    print(f\"-- mAP Score: {map_roc:.4f} -- f1-score: {f1:.3f} -- mcc: {mcc:.3f}\")\n",
    "    model.train()\n",
    "    avg_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82cf1777-49e7-462d-ae51-549ea6c8305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 40it [00:11,  3.62it/s, mean loss: 1.126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12, Loss: 1.1261\n",
      "-- mAP Score: 0.4719 -- f1-score: 0.515 -- mcc: 0.305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 1: 40it [00:10,  3.70it/s, mean loss: 0.857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12, Loss: 0.8573\n",
      "-- mAP Score: 0.5581 -- f1-score: 0.630 -- mcc: 0.487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 2: 40it [00:10,  3.69it/s, mean loss: 0.634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12, Loss: 0.6340\n",
      "-- mAP Score: 0.7402 -- f1-score: 0.650 -- mcc: 0.532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 3: 40it [00:10,  3.69it/s, mean loss: 0.494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12, Loss: 0.4937\n",
      "-- mAP Score: 0.7239 -- f1-score: 0.640 -- mcc: 0.533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 4: 40it [00:12,  3.15it/s, mean loss: 0.469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/12, Loss: 0.4694\n",
      "-- mAP Score: 0.7471 -- f1-score: 0.709 -- mcc: 0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 5: 40it [00:12,  3.15it/s, mean loss: 0.289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/12, Loss: 0.2888\n",
      "-- mAP Score: 0.7685 -- f1-score: 0.716 -- mcc: 0.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 6: 40it [00:12,  3.16it/s, mean loss: 0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/12, Loss: 0.2406\n",
      "-- mAP Score: 0.7594 -- f1-score: 0.750 -- mcc: 0.666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 7: 40it [00:12,  3.17it/s, mean loss: 0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/12, Loss: 0.1780\n",
      "-- mAP Score: 0.7686 -- f1-score: 0.765 -- mcc: 0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 8: 40it [00:12,  3.17it/s, mean loss: 0.151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/12, Loss: 0.1510\n",
      "-- mAP Score: 0.7667 -- f1-score: 0.756 -- mcc: 0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 9: 40it [00:12,  3.15it/s, mean loss: 0.090]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/12, Loss: 0.0896\n",
      "-- mAP Score: 0.7655 -- f1-score: 0.756 -- mcc: 0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 10: 40it [00:12,  3.14it/s, mean loss: 0.067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12, Loss: 0.0675\n",
      "-- mAP Score: 0.7660 -- f1-score: 0.741 -- mcc: 0.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 11: 40it [00:12,  3.16it/s, mean loss: 0.116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/12, Loss: 0.1164\n",
      "-- mAP Score: 0.7632 -- f1-score: 0.741 -- mcc: 0.651\n"
     ]
    }
   ],
   "source": [
    "loss_per_epoch = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss_per_step = []\n",
    "    loop_object  = tqdm(enumerate(data.train_dataloader), desc=f\"Epochs {epoch}\")\n",
    "    for i, batch in loop_object :        \n",
    "        optimizer.zero_grad() \n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "        input_year, input_seq = inputs\n",
    "        ### Model\n",
    "        output = model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "        probs = F.sigmoid(output).flatten()\n",
    "        ### Loss\n",
    "        loss = loss_fn(output, labels)  \n",
    "        loss_per_step.append(loss.detach().cpu().numpy())\n",
    "        loop_object.set_postfix_str(\"mean loss: %.3f\"%np.mean(loss_per_step[-100:]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # AVERAGING starts only after 3 epochs\n",
    "        if epoch > avg_start:\n",
    "            avg_model.update_parameters(model)\n",
    "    # On epoch end\n",
    "    loss_per_epoch.append(np.mean(loss_per_step))\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss_per_epoch[-1]:.4f}\")\n",
    "    run_validation(epoch=epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0d2fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8947\n",
      "Recall: 0.6800\n",
      "F1 Score: 0.7727\n",
      "-- mAP Score: 0.8713 --\n"
     ]
    }
   ],
   "source": [
    "test_loss = []\n",
    "preds = []\n",
    "targets = []\n",
    "\n",
    "## Set both models into the eval mode.=\n",
    "avg_model.eval()\n",
    "for batch in data.test_dataloader:\n",
    "    inputs, labels = batch\n",
    "    labels = labels.to(torch.float).to(device)\n",
    "    input_year, input_seq = inputs\n",
    "    ### Model\n",
    "    output = avg_model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "    probs = F.sigmoid(output).flatten()\n",
    "\n",
    "    loss = loss_fn(output, labels)  \n",
    "    test_loss.append(loss.detach().cpu().numpy())\n",
    "    preds.extend(probs.detach().cpu().numpy().tolist())\n",
    "    targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "# Concatenate all the batches\n",
    "probs = torch.tensor(preds).flatten()\n",
    "actuals = torch.tensor(targets).flatten()\n",
    "# Concatenate all the batches\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(actuals.cpu().numpy(), probs.cpu().numpy() > 0.5, average='binary')\n",
    "map_roc = average_precision_score(actuals.numpy(), probs.numpy())\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"-- mAP Score: {map_roc:.4f} --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2a219b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Performance of the Model:\n",
      "\tF1 Score: {'mean': 0.7708339831055661, 'CIs': array([0.66666667, 0.86075949])}\n",
      "\tMCC Score: {'mean': 0.7200048252164511, 'CIs': array([0.59957542, 0.82768243])}\n",
      "\tPrecision: {'mean': 0.8948846709541379, 'CIs': array([0.78571429, 0.97619048])}\n",
      "\tRecall: {'mean': 0.6790740822522332, 'CIs': array([0.54545455, 0.80769231])}\n",
      "\tmAP Score: {'mean': 0.8722505476260705, 'CIs': array([0.78861422, 0.93726309])}\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimated Performance of the Model:\")\n",
    "print(\"\\tF1 Score:\", bootstrap( probs.cpu().numpy() > 0.5, actuals.cpu().numpy(), metric= lambda x,y: f1_score(x,y), n_bootstraps=10000))\n",
    "print(\"\\tMCC Score:\", bootstrap( probs.cpu().numpy() > 0.5, actuals.cpu().numpy(), metric= lambda x,y: matthews_corrcoef(x,y), n_bootstraps=10000))\n",
    "print(\"\\tPrecision:\", bootstrap( probs.cpu().numpy() > 0.5, actuals.cpu().numpy(), metric= lambda x,y: precision_score(x,y), n_bootstraps=10000))\n",
    "print(\"\\tRecall:\", bootstrap( probs.cpu().numpy() > 0.5, actuals.cpu().numpy(), metric= lambda x,y: recall_score(x,y), n_bootstraps=10000))\n",
    "print(\"\\tmAP Score:\", bootstrap(preds = probs.cpu().numpy(), targs =  actuals.cpu().numpy(), metric= lambda x,y: average_precision_score(x,y), n_bootstraps=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c5ab9e",
   "metadata": {},
   "source": [
    "# Train Model on the Full Data\n",
    "(model for inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d6cf880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "# Reinitialize the training \n",
    "model = PreFerPredictor().to(device)\n",
    "# Define the loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([1/0.2]).to(device))\n",
    "# Define the optimization strategy\n",
    "optimizer = torch.optim.RAdam(model.parameters() , lr=LR, weight_decay=1e-2, decoupled_weight_decay=True)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = NUM_EPOCHS * len(data.full_dataloader), eta_min = 5e-4, last_epoch = -1)\n",
    "## EMA Weight Averaging\n",
    "avg_fn = optim.swa_utils.get_ema_avg_fn(0.99)\n",
    "avg_model = optim.swa_utils.AveragedModel(model, avg_fn=avg_fn, use_buffers=False)\n",
    "avg_start = 3\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "avg_model.train()\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6db2254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 62it [00:16,  3.71it/s, mean loss: 1.282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12, Loss: 1.2818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 1: 62it [00:16,  3.75it/s, mean loss: 0.957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12, Loss: 0.9569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 2: 62it [00:16,  3.76it/s, mean loss: 0.699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12, Loss: 0.6994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 3: 62it [00:16,  3.75it/s, mean loss: 0.571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12, Loss: 0.5711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 4: 62it [00:19,  3.21it/s, mean loss: 0.490]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/12, Loss: 0.4896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 5: 62it [00:19,  3.22it/s, mean loss: 0.414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/12, Loss: 0.4139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 6: 62it [00:19,  3.21it/s, mean loss: 0.280]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/12, Loss: 0.2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 7: 62it [00:19,  3.20it/s, mean loss: 0.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/12, Loss: 0.2265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 8: 62it [00:19,  3.20it/s, mean loss: 0.202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/12, Loss: 0.2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 9: 62it [00:19,  3.23it/s, mean loss: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/12, Loss: 0.1247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 10: 62it [00:19,  3.21it/s, mean loss: 0.089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12, Loss: 0.0887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 11: 62it [00:19,  3.21it/s, mean loss: 0.094]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/12, Loss: 0.0942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## TRAIN ON THE FULL DATA\n",
    "loss_per_epoch = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss_per_step = []\n",
    "    loop_object  = tqdm(enumerate(data.full_dataloader), desc=f\"Epochs {epoch}\")\n",
    "    for i, batch in loop_object :        \n",
    "        optimizer.zero_grad() \n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "        input_year, input_seq = inputs\n",
    "        ### Model\n",
    "        output = model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "        probs = F.sigmoid(output).flatten()\n",
    "        ### Loss\n",
    "        loss = loss_fn(output, labels)  \n",
    "        loss_per_step.append(loss.detach().cpu().numpy())\n",
    "        loop_object.set_postfix_str(\"mean loss: %.3f\"%np.mean(loss_per_step[-100:]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # AVERAGING starts only after 3 epochs\n",
    "        if epoch > avg_start:\n",
    "            avg_model.update_parameters(model)\n",
    "    # On epoch end\n",
    "    loss_per_epoch.append(np.mean(loss_per_step))\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss_per_epoch[-1]:.4f}\")\n",
    "    #run_validation(epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2fad880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"weights/excel_rnn.pt\"\n",
    "torch.save(model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce2862fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.full_model import PreFerPredictor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc0683d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PreFerPredictor()\n",
    "model.load_state_dict(torch.load(\"weights/excel_rnn.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e63c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PreFer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
