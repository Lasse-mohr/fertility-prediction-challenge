{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9914262-7a3e-4e3a-9e07-043d40efd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data packages\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, average_precision_score\n",
    "\n",
    "from model.rnn import GRUDecoder\n",
    "from model.encoders import CustomExcelFormer\n",
    "from data_processing.pipeline import encoding_pipeline, get_generic_name\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from model.utils import get_device\n",
    "from model.dataset import PretrainingDataset\n",
    "from model.dataset import FinetuningDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18aee1dd-7884-40e7-be05-db070f1a452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders) device\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b651aa-752a-4c71-990a-332ff4099791",
   "metadata": {},
   "source": [
    "# Read the data\n",
    "\n",
    "Right now the notebook is set to work with fake data. This can be changed once the pipeline works.\n",
    "\n",
    "The data is stored as a Dict[person_id, Sequences] where Sequences is a Dict[year, survery_wave_response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b872e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataClass:\n",
    "    def __init__(self,\n",
    "                 data_path: str = \"data/training_data/PreFer_train_data.csv\",\n",
    "                 targets_path: str = 'data/training_data/PreFer_train_outcome.csv',\n",
    "                 codebook_path: str = 'data/codebooks/PreFer_codebook.csv',\n",
    "                 importance_path: str = 'features_importance_all.csv') -> None:\n",
    "        self.data = pd.read_csv(data_path, low_memory=False)\n",
    "        self.targets = pd.read_csv(targets_path)\n",
    "        self.codebook = pd.read_csv(codebook_path)\n",
    "        self.col_importance = pd.read_csv(importance_path)\n",
    "    def make_sequences(self, n_cols: int, use_codebook: bool = True):\n",
    "        custom_pairs = self.col_importance.feature.map(lambda x: get_generic_name(x)).unique()[:n_cols]\n",
    "        self.sequences = encoding_pipeline(self.data, self.codebook, \n",
    "                                           custom_pairs=custom_pairs, \n",
    "                                           importance=self.col_importance, \n",
    "                                           use_codebook=use_codebook)\n",
    "    def make_pretraining(self):\n",
    "        self.pretrain_dataset = PretrainingDataset(self.sequences)\n",
    "        self.seq_len = self.pretrain_dataset.get_seq_len()\n",
    "        self.vocab_size = self.pretrain_dataset.get_vocab_size()\n",
    "    def make_finetuning(self, batch_size, test_size: float = 0.2, val_size: float = 0.2):\n",
    "        targets = self.targets[self.targets.new_child.notna()]\n",
    "        train_person_ids, test_person_ids = train_test_split(targets['nomem_encr'], test_size=test_size, random_state=42)\n",
    "        train_person_ids, val_person_ids = train_test_split(train_person_ids, test_size=val_size, random_state=42)\n",
    "        rnn_data = {person_id: (\n",
    "                torch.tensor([year-2007 for year, _ in wave_responses.items()]).to(device),\n",
    "                torch.tensor([ wave_response for _, wave_response in wave_responses.items()]).to(device)\n",
    "                )\n",
    "                for person_id, wave_responses in self.sequences.items()\n",
    "                }\n",
    "\n",
    "        # split data based on the splits made for the target\n",
    "        train_data = {person_id: rnn_data[person_id] for person_id in train_person_ids}\n",
    "        val_data = {person_id: rnn_data[person_id] for person_id in val_person_ids}\n",
    "        test_data = {person_id: rnn_data[person_id] for person_id in test_person_ids}\n",
    "\n",
    "        self.train_dataset = FinetuningDataset(train_data, targets = targets)\n",
    "        self.val_dataset = FinetuningDataset(val_data, targets = targets)\n",
    "        self.test_dataset = FinetuningDataset(test_data, targets = targets)\n",
    "        self.train_dataloader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.val_dataloader = DataLoader(self.val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.test_dataloader  = DataLoader(self.test_dataset,  batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4e1c0",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "279b9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7e4345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomarx/Documents/GitHub/fertility-prediction-challenge/data_processing/pipeline.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  codebook[\"pairs\"] = codebook['var_name'].apply(get_generic_name)\n"
     ]
    }
   ],
   "source": [
    "data.make_sequences(n_cols=50)\n",
    "data.make_pretraining()\n",
    "data.make_finetuning(batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d77d87a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "#ENCODING_SIZE = 64\n",
    "BATCH_SIZE = 8\n",
    "HIDDEN_SIZE = 96\n",
    "ENCODING_SIZE = 96\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4\n",
    "NUM_EPOCHS = 20\n",
    "DETECT_ANOMALY = False\n",
    "SEQ_LEN = data.seq_len\n",
    "VOCAB_SIZE = data.vocab_size\n",
    "\n",
    "LR = 1e-3\n",
    "\n",
    "assert HIDDEN_SIZE % NUM_HEADS == 0, \"Check that the hidden size is divisible\"\n",
    "print(SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddb66e-cba5-4bb9-854d-811d49599b93",
   "metadata": {},
   "source": [
    "# Experimental Encoder (Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada8018b-7ebc-4db4-aadf-191b214b1470",
   "metadata": {},
   "source": [
    "# Train the RNN\n",
    "\n",
    "First we need to create Dataset class that can hold both the target (stored in a pd.DataFrame) and the sequences.\n",
    "\n",
    "The sequences will be of dimension 14 x encoding_dimension, because we have 14 years of surveys.\n",
    "\n",
    "I have created some code for getting the data into the right format, but it might not be useful.\n",
    "\n",
    "## Regarding masks\n",
    "Right now the masking is done already in the encoding. I haven't found exactly where Mikkel implemented this.\n",
    "So for now, assume that nothing is padded, and then we'll figure it out with Mikkel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54a2330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreFerPredictor(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = CustomExcelFormer(vocab_size=VOCAB_SIZE, \n",
    "                            hidden_size=HIDDEN_SIZE, \n",
    "                            out_size=ENCODING_SIZE,\n",
    "                            n_years=14,\n",
    "                            num_heads=NUM_HEADS,\n",
    "                            num_layers=NUM_LAYERS, \n",
    "                            num_classes=2,\n",
    "                            sequence_len=SEQ_LEN, \n",
    "                            aium_dropout=0.3,\n",
    "                            diam_dropout=0.2,\n",
    "                            residual_dropout=0.2,\n",
    "                            embedding_dropout=0.3,\n",
    "                            mixup=None,\n",
    "                            beta=0.2).to(device)\n",
    "        self.decoder = GRUDecoder(\n",
    "            input_size=ENCODING_SIZE,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_layers=3,\n",
    "            max_seq_len=14,\n",
    "            dropout=0.3,\n",
    "            bidirectional=True,\n",
    "            with_attention = True\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, input_year, input_seq, labels):\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings, _ = self.encoder(input_year, input_seq)#, y=labels.unsqueeze(-1).expand(-1, 14).reshape(-1), mixup_encoded=True)\n",
    "        encodings = encodings.view(bs,ss, -1)\n",
    "        mask = ~((input_seq == 101).sum(-1) == SEQ_LEN).view(bs,ss).detach()\n",
    "\n",
    "        # Forward pass\n",
    "        out = self.decoder(encodings, mask=mask).flatten()\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caf49964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Layer with the Dropout\n",
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = PreFerPredictor().to(device)\n",
    "# Define loss function and optimizer for RNN\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([1/0.2]).to(device))\n",
    "optimizer = torch.optim.RAdam(model.parameters() , lr=LR, weight_decay=1e-2, decoupled_weight_decay=True)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = NUM_EPOCHS, eta_min = 1e-4, last_epoch = -1)\n",
    "\n",
    "## Stochaistic Weight Averaging\n",
    "swa_model = optim.swa_utils.AveragedModel(model)\n",
    "swa_start = 5\n",
    "swa_scheduler = optim.swa_utils.SWALR(optimizer, swa_lr=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "649bc948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(epoch):\n",
    "    val_loss = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    model.eval()\n",
    "    swa_model.eval()\n",
    "    for batch in data.val_dataloader:\n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "        input_year, input_seq = inputs\n",
    "        output = model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "        probs = F.sigmoid(output).flatten()\n",
    "        loss = loss_fn(output, labels)  \n",
    "        val_loss.append(loss.detach().cpu().numpy())\n",
    "        preds.extend(probs.detach().cpu().numpy().tolist())\n",
    "        targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    # Concatenate all the batches\n",
    "    yhat = torch.tensor(preds).flatten().detach().cpu().numpy()\n",
    "    ytrue = torch.tensor(targets).flatten().cpu().numpy()\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(ytrue, yhat > 0.5, average='binary')\n",
    "    map_roc = average_precision_score(ytrue, yhat)\n",
    "    print(f\"-- mAP Score: {map_roc:.4f} -- f1-score: {f1:.3f}\")\n",
    "    model.train()\n",
    "    swa_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82cf1777-49e7-462d-ae51-549ea6c8305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 79it [00:13,  6.04it/s, mean loss: 1.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.3610\n",
      "-- mAP Score: 0.4825 -- f1-score: 0.521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 1: 79it [00:12,  6.13it/s, mean loss: 0.986]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss: 0.9864\n",
      "-- mAP Score: 0.6110 -- f1-score: 0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 2: 79it [00:13,  6.03it/s, mean loss: 0.898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss: 0.8978\n",
      "-- mAP Score: 0.6927 -- f1-score: 0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 3: 79it [00:13,  6.02it/s, mean loss: 0.832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Loss: 0.8319\n",
      "-- mAP Score: 0.7336 -- f1-score: 0.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 4: 79it [00:12,  6.14it/s, mean loss: 0.628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 0.6278\n",
      "-- mAP Score: 0.7999 -- f1-score: 0.769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 5: 79it [00:12,  6.23it/s, mean loss: 0.612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Loss: 0.6122\n",
      "-- mAP Score: 0.8187 -- f1-score: 0.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 6: 79it [00:12,  6.14it/s, mean loss: 0.473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Loss: 0.4733\n",
      "-- mAP Score: 0.8525 -- f1-score: 0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 7: 79it [00:13,  5.93it/s, mean loss: 0.441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Loss: 0.4405\n",
      "-- mAP Score: 0.8385 -- f1-score: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 8: 79it [00:13,  6.00it/s, mean loss: 0.374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Loss: 0.3741\n",
      "-- mAP Score: 0.8532 -- f1-score: 0.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 9: 79it [00:13,  6.06it/s, mean loss: 0.347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Loss: 0.3472\n",
      "-- mAP Score: 0.8500 -- f1-score: 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 10: 79it [00:13,  6.03it/s, mean loss: 0.320]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Loss: 0.3201\n",
      "-- mAP Score: 0.8588 -- f1-score: 0.759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 11: 79it [00:13,  6.06it/s, mean loss: 0.270]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Loss: 0.2701\n",
      "-- mAP Score: 0.8565 -- f1-score: 0.753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 12: 79it [00:13,  5.94it/s, mean loss: 0.279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Loss: 0.2789\n",
      "-- mAP Score: 0.8515 -- f1-score: 0.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 13: 79it [00:13,  5.78it/s, mean loss: 0.213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Loss: 0.2133\n",
      "-- mAP Score: 0.8486 -- f1-score: 0.756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 14: 79it [00:13,  5.83it/s, mean loss: 0.198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Loss: 0.1977\n",
      "-- mAP Score: 0.8438 -- f1-score: 0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 15: 79it [00:13,  5.89it/s, mean loss: 0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Loss: 0.2711\n",
      "-- mAP Score: 0.8184 -- f1-score: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 16: 79it [00:13,  5.96it/s, mean loss: 0.858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Loss: 0.8582\n",
      "-- mAP Score: 0.6716 -- f1-score: 0.320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 17: 68it [00:11,  5.75it/s, mean loss: 0.737]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     loss_per_step\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     17\u001b[0m     loop_object\u001b[38;5;241m.\u001b[39mset_postfix_str(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean loss: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(loss_per_step[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:]))\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m swa_start:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_per_epoch = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # print(epoch)\n",
    "    loss_per_step = []\n",
    "    loop_object  = tqdm(enumerate(data.train_dataloader), desc=f\"Epochs {epoch}\")\n",
    "    for i, batch in loop_object :        \n",
    "        optimizer.zero_grad() \n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "        input_year, input_seq = inputs\n",
    "        ### Model\n",
    "        output = model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "        probs = F.sigmoid(output).flatten()\n",
    "        ### Loss\n",
    "        loss = loss_fn(output, labels)  \n",
    "        loss_per_step.append(loss.detach().cpu().numpy())\n",
    "        loop_object.set_postfix_str(\"mean loss: %.3f\"%np.mean(loss_per_step[-100:]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch > swa_start:\n",
    "        swa_model.update_parameters(model)\n",
    "        swa_scheduler.step()\n",
    "    else:\n",
    "        scheduler.step()\n",
    "    # On epoch end\n",
    "    loss_per_epoch.append(np.mean(loss_per_step))\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss_per_epoch[-1]:.4f}\")\n",
    "    run_validation()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5d99a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AveragedModel(\n",
       "  (module): PreFerPredictor(\n",
       "    (encoder): CustomExcelFormer(\n",
       "      (embedding): SurveyEmbeddings(\n",
       "        (answer_embedding): Embedding(216, 96, padding_idx=101)\n",
       "        (yearly_embedding): Embedding(14, 96)\n",
       "        (question_embedding): Embedding(49, 96)\n",
       "        (drop_year): Dropout(p=0.3, inplace=False)\n",
       "        (drop_answer): Dropout(p=0.3, inplace=False)\n",
       "        (drop_question): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (embedding_norm): InstanceNorm1d(49, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (excelformer_convs): ModuleList(\n",
       "        (0-3): 4 x ExcelFormerConv(\n",
       "          (norm_1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (DiaM): DiaM(\n",
       "            (lin_q): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (lin_k): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (lin_v): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (lin_out): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (norm_2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (AiuM): AiuM(\n",
       "            (lin_1): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (lin_2): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (excelformer_decoder): ExcelFormerDecoder(\n",
       "        (lin_f): Linear(in_features=49, out_features=96, bias=True)\n",
       "        (activation): Mish()\n",
       "        (lin_d): Linear(in_features=96, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder): GRUDecoder(\n",
       "      (norm_in): InstanceNorm1d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (gru): GRU(96, 96, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "      (aggregation): AggAttention(\n",
       "        (act): Softmax(dim=1)\n",
       "      )\n",
       "      (norm_out): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (decoder): Linear(in_features=192, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swa_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0d2fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7358\n",
      "Recall: 0.7800\n",
      "F1 Score: 0.7573\n",
      "-- mAP Score: 0.8686 --\n"
     ]
    }
   ],
   "source": [
    "test_loss = []\n",
    "preds = []\n",
    "targets = []\n",
    "\n",
    "## Set both models into the eval mode.=\n",
    "swa_model.eval()\n",
    "for batch in data.test_dataloader:\n",
    "    inputs, labels = batch\n",
    "    labels = labels.to(torch.float).to(device)\n",
    "    input_year, input_seq = inputs\n",
    "    ### Model\n",
    "    output = swa_model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "    probs = F.sigmoid(output).flatten()\n",
    "\n",
    "    loss = loss_fn(output, labels)  \n",
    "    test_loss.append(loss.detach().cpu().numpy())\n",
    "    preds.extend(probs.detach().cpu().numpy().tolist())\n",
    "    targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "# Concatenate all the batches\n",
    "probs = torch.tensor(preds).flatten()\n",
    "actuals = torch.tensor(targets).flatten()\n",
    "# Concatenate all the batches\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(actuals.cpu().numpy(), probs.cpu().numpy() > 0.5, average='binary')\n",
    "map_roc = average_precision_score(actuals.numpy(), probs.numpy())\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"-- mAP Score: {map_roc:.4f} --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e2b8e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAGdCAYAAADJ6dNTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAwxUlEQVR4nO3db3BU13n48WcRIBFXuzU4QhIWWJZxsMAxJi6ObLVxHCi4mCFv/IcxKQnjNiGeGto0Ac/UodR2iDJp66TjITVtwVP5z6ST2oS6FomN7Qy2HIwVpSi4GGP9bIol02K8K9tBbnfP74WygpV2tXt3z733nHO/nxm9kLhC98/ee557znOeE1NKKQEAADDEpLB3AAAA4FwEJwAAwCgEJwAAwCgEJwAAwCgEJwAAwCgEJwAAwCgEJwAAwCgEJwAAwCiTw96BsTKZjLz99ttSW1srsVgs7N0BAAAlUErJ0NCQNDY2yqRJlfV9GBecvP3229LU1BT2bgAAgDIcP35cLrzwwor+D+OCk9raWhEZObh4PB7y3gAAgFKkUilpamoabccrYVxwkh3KicfjBCcAAFhGR0oGCbEAAMAoBCcAAMAoBCcAAMAoBCcAAMAoBCcAAMAoBCcAAMAoBCcAAMAoBCcAAMAoxhVhA2C/dEbJgf535eTQGamrrZHFzdOlahJrZQEoDcEJAK26+gZk657DMpA8M/qzhkSNbFnZKssXNIS4ZwBswbAOAG26+gZkfWdPTmAiIjKYPCPrO3ukq28gpD0DYBOCEwBapDNKtu45LCrPv2V/tnXPYUln8m0BAGcRnADQ4kD/u+N6TM6lRGQgeUYO9L8b3E4BsBLBCQAtTg4VDkzK2Q5AdBGcANCirrZG63YAoovgBIAWi5unS0OiRgpNGI7JyKydxc3Tg9wtABbyHJz87Gc/k5UrV0pjY6PEYjF54okncv5dKSXf/OY3paGhQaZNmyZLliyRo0eP6tpfAIaqmhSTLStbRUTGBSjZ77esbKXeCYCiPAcnH3zwgVxxxRXywAMP5P3373znO/L9739ffvCDH8jPf/5zOe+882TZsmVy5gzjzIDrli9okO1rFkl9Infopj5RI9vXLKLOCYCSxJRSZc/ri8Vi8vjjj8vnP/95ERnpNWlsbJSvfe1r8ud//uciIpJMJmXmzJmya9cuufXWW4v+n6lUShKJhCSTSYnH4+XuGoAQUSEWiB6d7bfWCrH9/f0yODgoS5YsGf1ZIpGQq6++Wrq7u/MGJ8PDwzI8PDz6fSqV0rlLAEJQNSkmbS0zwt4NAJbSmhA7ODgoIiIzZ87M+fnMmTNH/22sbdu2SSKRGP1qamrSuUsAAMAyoc/WueuuuySZTI5+HT9+POxdAgAAIdIanNTX14uIyDvvvJPz83feeWf038aqrq6WeDye8wUAAKJLa3DS3Nws9fX18swzz4z+LJVKyc9//nNpa2vT+acAAICjPCfEvv/++/L666+Pft/f3y+9vb0yffp0mT17tmzcuFHuvfdemTt3rjQ3N8vdd98tjY2NozN6AAAAJuI5ODl48KB89rOfHf3+z/7sz0REZO3atbJr1y75xje+IR988IH88R//sbz33nvS3t4uXV1dUlNDyWoACBvTvGGDiuqc+IE6JwDgj66+Adm653DO6tENiRrZsrKVAnmomM72O/TZOgAA/3X1Dcj6zp6cwEREZDB5RtZ39khX30BIewaMR3AChCydUdJ97JTs7j0h3cdOSTpjVGcmHJDOKNm657Dk+2Rlf7Z1z2E+ezCG1gqxALyhmx1BOND/7rgek3MpERlInpED/e9S2RdGoOcECAnd7Payrbfr5FBpC6+Wuh3gN3pOgBAU62aPyUg3+9LWemZSGMbG3q662tJmS5a6HeA3ek6AEHjpZoc5bO3tWtw8XRoSNVIozI3JSIC1uHl6kLsFFERwAoSAbnb72JxUWjUpJltWtoqIjAtQst9vWdlKLx2MQXAChIBudvvY3tu1fEGDbF+zSOoTuZ+p+kSNbF+zyNghKUQTOSdACLLd7IPJM3nfxGMy0mjQzW4OF3q7li9okKWt9VSIhfEIToAQZLvZ13f2SEwkJ0Chm91MrvR2VU2KMV0YxmNYBwgJ3ex2IakUCA49J0CIotDN7spCc/R2AcFh4T8AvrGxJkgxLh4ToIPO9pvgBMjDlbf9MGVrgox9wGTPos1DV3w+gPF0tt8M6wBj8GZcOdcr4JJUCviLhFjgHLZWADWN7TVBAISL4AT4DZsrgJrGhZogAMJDcAL8Bm/7+rhSEwQTs211ZtiDnBPgN3jb14cKuO4jNwt+oucE+A3e9ktX7I2ZhebcRm4W/EbPCfAbvO2XptQ35mwF3LHb1gf4ds2UX/1cn4kFMxCcAL9BBdDiCtUuyb4xj61dEmYFXIYd/OElN4vp1igXwzrAOVjvprByZzNla4KsWjhL2lpmBBaYMOzgD3KzEAR6ToAxorDeTTlseWNm2MFf5GYhCAQnQB5UAB3PljdmW4IoW5GbhSAwrAOgJLa8MdsSRNmKmVgIAsEJgJJk35gLNTkxGUk4DfuN2ZYgymbkZsFvDOsAKIkts5kYdggGuVnwEz0nAEpmwxszww7BCWMmFqIhppQyajGEVColiURCksmkxOPxsHcHQB42FDejzgkQLJ3tN8EJAGfZEEQBrtDZfpNzEnE8vOEypoQDdiI4iTCd3d4EOYB33DfB4Dzbh+AkoryukVLs/2JsH/CG+yYYnGc7MVsngspdIyUf1jABvOO+CQbn2V4EJxHkpbz3RHQGOUBUcN8Eg/NsN4KTCNJV3ltXkANECfdNMDjPdiM4iSBd5b1ZwwTwjvsmGJxnuxGcRJCuNVJYwwTwjvsmGJxnuxGcRJCu8t62LASHcKUzSrqPnZLdvSek+9ipyI/xc98Eg/NsN4KTiNKxRorra5jQqFauq29A2jv2yeodL8mGx3pl9Y6XpL1jX6RnSbh+35iC82w3ytdHnI7iRC7WEXDxmIJWqJZO9tNlykKBYeEzFgzOc3BYWwfGcakCI41q5dIZJe0d+wrOlojJSC/d/k3XW/s50cGl+8ZknOdgsLYOjOPKGibFaiPEZKQ2wtLWeh5uE/AyjdOFz025XLlvTMd5tg85J8A5qI2gB9M4AVSC4AQ4B42qHkzjBFAJghPgHDSqejCNE0AlCE6Ac9Co6sE0TgCVIDgBzkGjqo+OWjoAoompxEAe1EbQh2mcQDRQ5wQIAI0qAJSOOidAAKiNAADhIOcEAAAYheAEAAAYhWEdAJFFXhFgJoITAJHEjCzAXAzrAIic7MrTY9dRGkyekfWdPdLVNxDSngEQITgBnJfOKOk+dkp2956Q7mOnJJ0xqnpA4IqtPC0ysvJ01M8TECaGdQzC+Dd0Y+hiPC8rTzOV3A08W+1DcGIIGhHolh26GPv+nx26iGoJeVNXnqYB9QfPVjsRnBiARgS6FRu6iMnI0MXS1vrINYAmrjxNA+oPnq32IuckZIx/ww9ehi6ixrSVp0nO9QfPVrsRnISMRgR+MHXowgQmrTxNA+ofnq12IzgJGY0I/GDi0EXQJpqltHxBg2xfs0jqE7nHX5+oCbSrnwbUPzxb7UbOSchoROCH7NDFYPJM3rfymIw0xEENXQStlByO5QsaZGlrfahJqDSg/uHZajd6TkJm2vg33GDS0EXQvORwZFeeXrVwlrS1zAj8fNCA+odnq918CU6GhoZk48aNMmfOHJk2bZpcc8018vLLL/vxp6wX5UYE/jJl6CJItuVw0ID6h2er3XwZ1rn99tulr69P/vmf/1kaGxuls7NTlixZIocPH5ZZs2b58Setlm1ExnZD1zOVEBUyYegiSLYVWMs2oOs7eyQmkhNU0YBWjmervWJKKa2vEL/+9a+ltrZWdu/eLStWrBj9+ac+9Sm54YYb5N57753w91OplCQSCUkmkxKPx3XumvEowgRUZnfvCdnwWG/R7b5360JZtdCcFyXqnPiLZ2swdLbf2ntO/u///k/S6bTU1OR2JU+bNk32798/bvvh4WEZHh4e/T6VSuneJWtkx78BlMfWHI6o9XAFjWerfbQHJ7W1tdLW1ib33HOPXHbZZTJz5kx59NFHpbu7Wy655JJx22/btk22bt2qezcAa/GWVz6bZynRgAJnaR/WERE5duyYrFu3Tn72s59JVVWVLFq0SC699FJ55ZVX5NVXX83ZNl/PSVNTUySHdQC69yuXna0jkj+Hw9VkYCBsOod1fJmt09LSIs8//7y8//77cvz4cTlw4ID87//+r1x88cXjtq2urpZ4PJ7zBUQRZcz1iOIsJcA1vhZhO++88+S8886T06dPy969e+U73/mOn38OsBYL9ekV1RwOhgThCl+Ck71794pSSj7xiU/I66+/Ll//+tdl3rx58qUvfcmPPwdYz7YpsDaIWg4HQ4JwiS/DOslkUu644w6ZN2+e/OEf/qG0t7fL3r17ZcqUKX78uZJMtM4GEDbKmKMSDAnCNb70nNx8881y8803+/Ffl4U3CpjO1imwCB9DgnCR82vr8EYBG1DGHOViZWO4yOngxLZ1NhBdrAOCcjEkCBc5HZzwRgGbMAUW5WBIEC7ydSpx2HijgG2iOgUW5bO5Ki5QiNPBCW8UsFHUpsCiMqxsDBc5PaxDkiGAKIjqkCAlItzldM8JbxQAoiJqQ4KUiHCbLwv/VULnwkFZfIgBwB3ZEhFjGy8WdwyXzvbb6Z6TrKi9UQCAqyg6Fw2RCE5ESDIEABewDlU0RCY4sRErjAJALkpERAPBiaHIkwGA8aJSIiLqL6cEJwYqlOyVXQ+IZC8AURWFonO8nDpe58RGrAcEAIW5vg4Vi9WOIDgxDOsBAcDEXC06x8vpWQzrGIZkLwAozsUSEcxEOovgxDBRSfYCwhb1hEO/BHleXSsRwcvpWQQnholCshcQNhIO/cF5rQwvp2eRc2IY15O94B8WQSsNCYf+4LxWjsVqzyI4MZCryV6mcakx7+obkPaOfbJ6x0uy4bFeWb3jJWnv2EeDMAYJh/7gvOrBy+lZDOuUIYgxVReTvUziUvczdXFKR8KhPziv+mRfTsc+n+otfT6Vi+DEoyAbNdeSvUzhUmPOImjekHDoD86rXrycMqzjCWOq9nOt+5m6ON6QcOgPzqt+2ZfTVQtnSVvLjEgFJiIEJyVzrVELmin5HUE35n4ft2lvrKZc50JIOPQH5xW6MaxTIsZUy2dSfkeQjXkQx23SG6tJ17mQbMLh+s4eiYnkvGxELeFQJ84rdKPnpESmvaHawrShsKAa86CO25Q3VtOu80SYDecPzit0ouekRCa9odrCxGTNIIrcBXncJryxmnidiyHh0B+cV+hCz0mJTHlDtYmJyZpB1BEI+rjDfmM18TqXIuoJh37hvEIHek5KZMIbqm1MHQrzu45AGMcd5hurqdcZgL0ITjygOI43/+9/PixpuzCGwvxszMMaAgyrLg5DngB0IzjxiDHV0qQzSh498FbR7erj1aENhfnVmEdt8caoHS8A/5FzUgbGVIs70P+uDKaKd+OvXjzbufMXtfUxona8QCVMrwVkCnpO4ItS8wsuuuA8n/ckHFEbAoza8eoQxBpdQfwNlM6GWkCmIDiBL8hDiN4QYNSOtxJBNFI0hGZxaU2vIMSUUkb1KaVSKUkkEpJMJiUej4e9OyhTOqOkvWNf0TyE/Zuup/FCpBRqpLJ3gY5GKoi/gdJln4eFpty78jzU2X6TcwJfkIcAjBfEGl2sA2YeW2sBhYngBL4JuzgYYJogGikaQvNQC8g7ck7gK/IQgLOCaKRoCM1DDp53BCfwXVjFwQDTBNFI0RCah1pA3jGsAwABCWKNLtYBMw85eN4RnABAQIJopGxsCKNQmIwcPG+YSgwAAaPOyVm27KcuLhfG09l+E5wAQAioEEs9FtfobL9JiAUQKtMbUL8EkShucjJ6sXosMRmpx7K0tT4SnwfkIjjBqKg2EghP1Lr0cZaXeiymBljwD8EJRIRGAsFjrZFoox4LJsJsHYw2EmPfYrKNRFffQEh7BldRYh3UY8FECE4ijkYCYbCxxHoUprsGiXosmAjDOhHHuC/CYFuXPsOe+mXrsazv7JGYSM4Lkqn1WBAcek4izrZGAm6wqUufYU//UJgMhdBzEnE2NRJwhy1rjTDd1X8sDop86DmJOMZ9EQZbSqzbmBtjo2w9llULZ0lby4zQrzvCR3AScbY0EnCPDV36DHsC4WBYB6ONxNiEv3oS/uAz07v0GfYEwkFwAhExv5GAu0wusW5LbgzgGoITjDK5kQDCwHRXIBzknADABGzIjQFcQ88JoBkLKLqHYU8gWAQngEZUEnUXw55AcBjWATShkigA6EHPSRF00aMUlVYS5XMGAGcRnEyALnqUqpIFFPmcAUAuhnUKoIseXpRbSZTPGQpJZ5R0Hzslu3tPSPexU5LO5OuXA9xEz0keLPYFr8qpJMrnDIXQm4aoo+ckDxb7glflLKDI5wz50JsGEJzkxWJf8KqcBRRN+JwxdGCWYr1pIiO9aVwnuE57cJJOp+Xuu++W5uZmmTZtmrS0tMg999wjStlzM7HYF8rhtZJo2J+zrr4Bae/YJ6t3vCQbHuuV1TtekvaOfbyZh4jeNGCE9pyTjo4O2b59uzz00EMyf/58OXjwoHzpS1+SRCIhd955p+4/5wsW+0K5vFQSDfNzlh06GPt3s0MHrpVlt2Wqtgm9aYAJtAcnL774oqxatUpWrFghIiIXXXSRPProo3LgwAHdf8o3LPaFSpRaSTSsz1nUEnFtSi4NuzcNMIX2YZ1rrrlGnnnmGXnttddEROSXv/yl7N+/X2644Qbdf8pXLPaFIITxOYvS0IFtyaXlJFYDLtLec7J582ZJpVIyb948qaqqknQ6Lffdd5/cdtttebcfHh6W4eHh0e9TqZTuXSobi30hCEF/zqIydGBjD5GNvba2DJnBLtqDkx/+8Ify8MMPyyOPPCLz58+X3t5e2bhxozQ2NsratWvHbb9t2zbZunWr7t3QplgXPTcmdAhyUbmoDB1UUrU3TNnetLFDUfUGDkXZNGQGu8SU5mk0TU1NsnnzZrnjjjtGf3bvvfdKZ2en/Od//ue47fP1nDQ1NUkymZR4PK5z17TjxoSN0hkl7R37iibi7t90vdWB9u7eE7Lhsd6i233v1oWyauEs/3fII9NffAolVWf3kOHv6EmlUpJIJLS039pzTj788EOZNCn3v62qqpJMJpN3++rqaonH4zlfNrBtLLtU1L1wXzk1WWxkew9Rtjdt1cJZ0tYyw6jrQT0W+E37sM7KlSvlvvvuk9mzZ8v8+fPlF7/4hfzN3/yNrFu3TvefCo2NY9mloCcoOmwaOigXJQH8Y+uQGeyhPTj5u7/7O7n77rvlq1/9qpw8eVIaGxvly1/+snzzm9/U/adC4+KNGbW6F3A/4dvG5FJbRCWpGuHRHpzU1tbK/fffL/fff7/u/9oYrt2YrvYEobggE3HDEIUeojDYPmQG87EqcRlcuzFd7AkCslzvIQoDQ2bwGwv/lcG1Qkmu9QQBY5mcXGqjqCRVIzwEJ2Vw7cZ0rSeoEGYiAfpQRdtOtjwHGdYpk0tj2VHoomUmkl1Mr/GBEQyZ2cWm56D2ImyV0lnEJQiuPESzs3VE8s9qsPlNiGJRdrHpAQrYIojnoNFF2KLGlbFsV7toKRZlF1eLGwJhsvE5yLAORrnYRctMJHswpT06XOlxtoWNz0GCE+Rwre4FM5HsYeMDFN4FOWxHEDTCxucgwYmhuKn0iMpMJBfY+ACFN0FWorY5d0n389/G5yDBiYFsvqlME4WZSK6w8QGK0gU5bGfzchx+PP9tfA6SEGsYEgL1cqEmjS11CSrlWnFD5PIybFcJG5M/s/x6/tv4HCQ4MYjNN5XJbJ6J1NU3IO0d+2T1jpdkw2O9snrHS9Lesc/JINXGByhKF9SwXVBBkG5+P/9tew4yrGMQEgL9Y+NMJJu7pssVdHFDcruCU+pw3P8MDcvu3hNlXw9bc5eCeP7b9BwkODGIrTeVLWyaiRTlabVBPUDJ7QpWsbwHEZFJMZF7nnx19PtyroetuUtBPf9teQ4yrGMQW2+qoEQl90LE3q5pXfwubkhuV/AmGrbLGntLl3M9bM1d4vmfi+DEILbeVEGIUu6FCL1ofiK3KzyF8h4KxZ7lXA9bc5d4/uciODGIrTdVMZX2eETxLZe3KP9EvVcqbMsXNMj+TdfLo3/0afnerQvl7hWXjesxOVc518O25E8Rd5//5SLnxDAurXYsUvm4flRzL2ysS2ALeqXCd27ew+7eEyX9jtfrYVPyZ5Zrz/9KEJwYyMabKh8ds02iOoMp+xa1vrNHYpJ/pegovUXpRK+UWfy8HrYkf57Lled/pQhODGXjTXUuXT0eUX7L5S3KH/RKmYXrMZ7tz38dCE7gC109HlF/y+UtSj96pczC9UA+JMTCF7p6PMhg939arZ9Mnf5tasKkqefLb6ZeD4SHnhP4QlePB29V3phU8dT0Imem9UqZfr78Ztr1QLhiSimjQvNUKiWJREKSyaTE4/GwdwdlSmeUtHfsKzqOvH/T9SU9fKL+4C6FSeeoUDJ09krzNpyL8wUX6Gy/CU7gm+wDVyR/j4fXB65JvQKmMalxywamhXKOvAamruN8wRU6229yTuAb3ePINude+Mm0iqcUOfOG8wWMR84JfMU4sv9MqwUT5enf5eB8AeMRnMB3zNn3l2mNW9Snf3vF+QLGY1gHsJxpjRvTv73hfAHjEZwAljOtcWMBM284X8B4BCeA5Uxs3Ciq5Q3nC8jFVGLAESbVOcli+rc3nC/YjDonFuPhAz/x+QIQFp3tN7N1AmTimy3cwswoAC4g5yQg2QqeY+tRDCbPyPrOHunqGwhpzwAAMAvBSQBMq+AJAIDJCE4CQHlqAABKR85JAEyr4AkA5SDhGkEhOAmAaRU8AcArEvoRJIZ1AmBaBU8A8IKEfgSN4CQAJlbwBMKSzijpPnZKdveekO5jp0gENxwJ/fZw6d5iWCcg2fLUY7tF6+kWRYREcWjA9jwNLwn91NgJj2v3FsFJgJYvaJClrfVWP6iAcmWHBsa+y2WHBlxcQ8aFBoOEfvO5eG8xrBOwbAXPVQtnSVvLDAITREIUhwZcydMgod9srt5bBCcAfBe1Wj8uNRgk9JvN1XuL4ASA76I2NOBSg0FCv9lcvbcITgD4LmpDA641GNmE/vpE7vWpT9RYmc/gElfvLRJiYR3bZz9EUXZoYDB5Ju9QR0xGGjpXhgZcbDBI6DeTq/cWwQms4sLshyjKDg2s7+yRmEjOQ9TFoQFXG4xsQj/M4eq9xbAOrOHK7IeoitLQAHkaCJKL91ZMKWVUungqlZJEIiHJZFLi8XjYuwNDpDNK2jv2FUwyzL6J7t90PQ98w0VpWI6ePgQp7HtLZ/vNsA6sQJVKd0RpaIA8jegKI1Bw6d4iOIEVXJv9gOhwqcFAaegxqxw5J7CCi7MfALiH3Dg9CE5gBapUBsul1U2BoLhUGThsDOvACq5OlzMRXdIIQ9jJnDqQG6cPwQmskZ0uN7bhrC+z4XThYaibi6ubwnyuBMTkxulDcAKr6Jr94MrDUKdiXdIxGemSXtpaH/kgzjVhBuouBcTkxulDcALrVDr7waWHoU50SUdTmIG6awGxq5WBw0BCLCKFhLXC6JKOnrBnlri0erMIlYF1IjhBpLj2MNSJLuloMSFQdzEgdrGUfBgY1kGkuPgw1IUu6WgxYRjP1YCYysCVo+cEkeLqw1AHuqSjxYRA3eX6RdncuFULZ0lbywzuG48IThApLj8MdaBLOjpMCNQJiFEIwzqIFIq5FUeXdDSYMoynu34R3BBTShk1LUHnkstAIdQ5Ac7O1hHJH6gH2VtGUUT76Wy/CU4sxE2sB+cRIFCHPkYHJxdddJG8+eab437+1a9+VR544IGiv09wMjEeJECwohDERuEY4T+jg5P//u//lnQ6Pfp9X1+fLF26VJ599lm57rrriv4+wUlhhSqbhtEFC294+NuJlwHYLOjnjtHByVgbN26Uf/u3f5OjR49KLFb8pBCc5JfOKGnv2FewLkE2eW3/putp9AxDA2cnXgZgszCeOzrbb1+nEn/00UfS2dkp69atKxiYDA8PSyqVyvnCeFQ2tVPY5cFRHhOqpwLlcuG542tw8sQTT8h7770nX/ziFwtus23bNkkkEqNfTU1Nfu6StUwomARvaODsxcsAbOXKc8fX4OQf//Ef5YYbbpDGxsaC29x1112STCZHv44fP+7nLlnLhIJJ8IYGzl68DMBWrjx3fCvC9uabb8rTTz8t//qv/zrhdtXV1VJdXe3XblgpXxKTKQWTUDoauGD4kfTHywBs5cpzx7fgZOfOnVJXVycrVqzw6084aaIkJiqb2oUGzn9+Jf3xMgBbufLc8WVYJ5PJyM6dO2Xt2rUyeTIV8ktVLIlJRFj3xCJhrOOTzijpPnZKdveekO5jp4wfV66En0l/rPkCW7myfpgvU4l/8pOfyLJly+TIkSNy6aWXevrdqE4l9jJVWESomWGJIMuDR2nKclBT66N0TuGOsJYlsKrOiVdRDU66j52S1TteKrrdo3/0aWlrmRHAHkGXIBq4qNXkCPJ+oYAebGR7nRPGXAzhShITxvN7ld9iUwdjMjJ1cGlrfaCNqp+NepD3S9WkGC8EsI7tq4sTnBjClSQm5OdnA+dl6mBQjazfb23cL0BxNgfWvtY5QelcSWJC8EzrdQuiOiX3i/uilNyN8eg5MUR2dgBTheGVSb0IQQ0xcb+4jURk0HNikOULGpgqDM9M6kUIsjol94ubXFgXBpWj58QwticxIXgm9SIEPcTE/eIWU5O7ETyCEwPZnMSEcGR7EcZ2hdcH3BUexhAT94s7TEzuRjgITgBHmNCLQNl3VMK05G6Eh+AEcEjYvQgmDTHBPiYldyNcJMQC0IpEVZQrrORupi2bh54TANqZMMQE+4TR88a0ZTOxtg4AwChBBQxRW5PKb6ytAwBwVhA9b0xbNhvBCQDAOH4ndzNt2WwkxAIAIodpy2YjOAEARA7Tls1GcAIAiByT1qTCeAQnAIDIyU5bFpFxAQoFA8NHcAIAiCQKBpqL2ToAgMiiYKCZCE4iLp1R3JSwFp9f6BD2mlQYj+AkwijbDJu58PkluALyo3x9RFG2GTZz4fPrQnAFnEtn+01CbAQVK9ssMlK2mZU5YSIXPr/Z4GpshdLB5BlZ39kjXX0DIe0ZYAaCkwjyUrYZMI3tn18XgivAbwQnEUTZZtjM9s+v7cEVEASCkwiibDNsZvvn1/bgCggCs3UiKFu2eTB5Jm/XckxGihBRthnFhDHbxPbPr+3BFXIx48ofBCcRlC3bvL6zR2IiOQ94yjajVGHNNrH982t7cIWzmHHlH4Z1IoqyzahE2LNNbP78sqaLG8K+B1xHnZOIo0sSXqUzSto79hVM6sy++e/fdL3vnyWbP7+8ddvLpHvAJDrbb4Z1Io6yzfDKy2wTvz9bNn9+WdPFXibdA64iOAHgCbNN9LE5uIoy7gH/kXMCwBNmmyDquAf8R3ACwJPsbJNCgw8xGcmdYLYJXMU94D+CEwCeMNsEUcc94D+CEwCe2TyVF9CBe8BfTCUGDGXDNFkT99HEfYK7+LydxVRiwHG21MAwbbaJLecN7jDtHnAFwzqAYag8WR7OG+AOghPAIOmMkq17DuddcyX7s617Dks6Y9RobOg4b4BbCE4Ag3ipPImzOG+AWwhOAINQebI8nDfALQQngEGoPFkezhvgFoITwCBUniwP5w1wC8EJYBAqT5aH8wa4heAEMAyVJ8vDeQPcQYVYwFBUniwP5w0IBxVigQig8mR5OG+A/RjWAQAARiE4AQAARiE4AQAARiE4AQAARiEhFoBVmI0DuI/gBIA1uvoGZOuewzmL/DUkamTLylbqmAAOYVgHgBW6+gZkfWfPuNWHB5NnZH1nj3T1DYS0ZwB0IzgBYLx0RsnWPYclX8XI7M+27jks6YxRNSUBlIngBIDxDvS/O67H5FxKRAaSZ+RA/7vB7RQA3xCcADDeyaHCgUk52wEwG8EJAOPV1dYU38jDdgDMRnACwHiLm6dLQ6JGCk0YjsnIrJ3FzdOD3C0APiE4AWC8qkkx2bKyVURkXICS/X7LylbqnQCOIDgBYIXlCxpk+5pFUp/IHbqpT9TI9jWLqHMCOIQibACssXxBgyxtradCLOA4ghMAVqmaFJO2lhmB/C1K5QPhIDgBgDwolQ+Ex5eckxMnTsiaNWtkxowZMm3aNLn88svl4MGDfvwpANCOUvlAuLQHJ6dPn5Zrr71WpkyZIk899ZQcPnxY/vqv/1rOP/983X8KALSjVD4QPu3DOh0dHdLU1CQ7d+4c/Vlzc7PuPwMAvvBSKj+o3BcgarT3nPz4xz+Wq666Sm666Sapq6uTK6+8Unbs2FFw++HhYUmlUjlfABAWSuUD4dMenLzxxhuyfft2mTt3ruzdu1fWr18vd955pzz00EN5t9+2bZskEonRr6amJt27BAAls7FUfjqjpPvYKdnde0K6j51iyAnWiymltH6Kp06dKldddZW8+OKLoz+788475eWXX5bu7u5x2w8PD8vw8PDo96lUSpqamiSZTEo8Hte5awBQVDqjpL1jnwwmz+TNO4nJSOG3/ZuuN2JaMbOKYIpUKiWJREJL+62956ShoUFaW1tzfnbZZZfJW2+9lXf76upqicfjOV8AEBabSuUzqwiu0h6cXHvttXLkyJGcn7322msyZ84c3X8KAHxhQ6l8ZhXBZdpn6/zpn/6pXHPNNfKtb31Lbr75Zjlw4IA8+OCD8uCDD+r+UwDgG9NL5TOrCC7THpz8zu/8jjz++ONy1113yV/91V9Jc3Oz3H///XLbbbfp/lMA4KsgS+V7xawiuMyX8vU33nij3HjjjX781wAAsXNWEVAqX8rXAwD8tbh5ujQkasYl7WbFZGTWzuLm6UHuFqAFwQkAWMimWUWAVwQnAGApG2YVAeXwJecEABAM02cVAeUgOAEAy5k8qwgoB8M6AADAKAQnAADAKAQnAADAKOScAECZ0hlFIirgA4ITAChDV9+AbN1zOGd9m4ZEjWxZ2coUXqBCDOsAgEddfQOyvrNn3MJ7g8kzsr6zR7r6BkLaM8ANBCcA4EE6o2TrnsOi8vxb9mdb9xyWdCbfFgBKQXACAB4c6H93XI/JuZSIDCTPyIH+d4PbKcAxBCcA4MHJocKBSTnbARiP4AQAPKirrSm+kYftAIxHcAIAHixuni4NiZpxKwFnxWRk1s7i5ulB7hbgFIITAPCgalJMtqxsFREZF6Bkv9+yspV6J0AFCE4AwKPlCxpk+5pFUp/IHbqpT9TI9jWLqHMCVIgibABQhuULGmRpaz0VYgEfEJwAQJmqJsWkrWVG2LsBOIdhHQAAYBSCEwAAYBSCEwAAYBSCEwAAYBSCEwAAYBSCEwAAYBSCEwAAYBSCEwAAYBSCEwAAYBTjKsQqpUREJJVKhbwnAACgVNl2O9uOV8K44GRoaEhERJqamkLeEwAA4NXQ0JAkEomK/o+Y0hHiaJTJZOTtt9+W2tpaicXsXkArlUpJU1OTHD9+XOLxeNi7oxXHZi+Xj49js5fLxxeVY6utrZWhoSFpbGyUSZMqyxoxrudk0qRJcuGFF4a9G1rF43HnPpBZHJu9XD4+js1eLh9fFI6t0h6TLBJiAQCAUQhOAACAUQhOfFRdXS1btmyR6urqsHdFO47NXi4fH8dmL5ePj2PzzriEWAAAEG30nAAAAKMQnAAAAKMQnAAAAKMQnAAAAKMQnJRp+/bt8slPfnK08ExbW5s89dRTBbfftWuXxGKxnK+ampoA97h83/72tyUWi8nGjRsn3O5f/uVfZN68eVJTUyOXX365/Pu//3swO1iBUo7Npmv3l3/5l+P2dd68eRP+ji3Xzeux2XTdREROnDgha9askRkzZsi0adPk8ssvl4MHD074O88995wsWrRIqqur5ZJLLpFdu3YFs7Nl8Hp8zz333LjrF4vFZHBwMMC9Lu6iiy7Ku5933HFHwd+x5Z7zemw67znjKsTa4sILL5Rvf/vbMnfuXFFKyUMPPSSrVq2SX/ziFzJ//vy8vxOPx+XIkSOj39tQnv/ll1+Wv//7v5dPfvKTE2734osvyurVq2Xbtm1y4403yiOPPCKf//znpaenRxYsWBDQ3npT6rGJ2HXt5s+fL08//fTo95MnF77NbbtuXo5NxJ7rdvr0abn22mvls5/9rDz11FPy8Y9/XI4ePSrnn39+wd/p7++XFStWyFe+8hV5+OGH5ZlnnpHbb79dGhoaZNmyZQHufXHlHF/WkSNHcqqq1tXV+bmrnr388suSTqdHv+/r65OlS5fKTTfdlHd7m+45r8cmovGeU9Dm/PPPV//wD/+Q99927typEolEsDtUoaGhITV37lz105/+VH3mM59RGzZsKLjtzTffrFasWJHzs6uvvlp9+ctf9nkvy+Pl2Gy6dlu2bFFXXHFFydvbdN28HptN123Tpk2qvb3d0+984xvfUPPnz8/52S233KKWLVumc9e0KOf4nn32WSUi6vTp0/7slE82bNigWlpaVCaTyfvvNt1zYxU7Np33HMM6GqTTaXnsscfkgw8+kLa2toLbvf/++zJnzhxpamqSVatWya9+9asA99K7O+64Q1asWCFLliwpum13d/e47ZYtWybd3d1+7V5FvBybiF3X7ujRo9LY2CgXX3yx3HbbbfLWW28V3Na26+bl2ETsuW4//vGP5aqrrpKbbrpJ6urq5Morr5QdO3ZM+Ds2Xbtyji9r4cKF0tDQIEuXLpUXXnjB5z2tzEcffSSdnZ2ybt26gj0GNl23c5VybCL67jmCkwocOnRIfuu3fkuqq6vlK1/5ijz++OPS2tqad9tPfOIT8k//9E+ye/du6ezslEwmI9dcc43813/9V8B7XZrHHntMenp6ZNu2bSVtPzg4KDNnzsz52cyZM40bHxbxfmw2Xburr75adu3aJV1dXbJ9+3bp7++X3/3d35WhoaG829t03bwem03X7Y033pDt27fL3LlzZe/evbJ+/Xq588475aGHHir4O4WuXSqVkl//+td+77In5RxfQ0OD/OAHP5Af/ehH8qMf/Uiamprkuuuuk56engD33JsnnnhC3nvvPfniF79YcBub7rlzlXJsWu85Lf0vETU8PKyOHj2qDh48qDZv3qwuuOAC9atf/aqk3/3oo49US0uL+ou/+Auf99K7t956S9XV1alf/vKXoz8rNvQxZcoU9cgjj+T87IEHHlB1dXV+7WZZyjm2sUy+dmOdPn1axePxgsONtly3fIod21gmX7cpU6aotra2nJ/9yZ/8ifr0pz9d8Hfmzp2rvvWtb+X87Mknn1Qioj788ENf9rNc5RxfPr/3e7+n1qxZo3PXtPr93/99deONN064ja33XCnHNlYl9xw9JxWYOnWqXHLJJfKpT31Ktm3bJldccYV873vfK+l3p0yZIldeeaW8/vrrPu+ld6+88oqcPHlSFi1aJJMnT5bJkyfL888/L9///vdl8uTJOQlSWfX19fLOO+/k/Oydd96R+vr6oHa7JOUc21gmX7uxfvu3f1suvfTSgvtqy3XLp9ixjWXydWtoaBjX63rZZZdNOGxV6NrF43GZNm2aL/tZrnKOL5/Fixcbef1ERN588015+umn5fbbb59wOxvvuVKPbaxK7jmCE40ymYwMDw+XtG06nZZDhw5JQ0ODz3vl3ec+9zk5dOiQ9Pb2jn5dddVVctttt0lvb69UVVWN+522tjZ55plncn7205/+dMIcnDCUc2xjmXztxnr//ffl2LFjBffVluuWT7FjG8vk63bttdfmzHAQEXnttddkzpw5BX/HpmtXzvHl09vba+T1ExHZuXOn1NXVyYoVKybczqbrllXqsY1V0T3nua8FSimlNm/erJ5//nnV39+v/uM//kNt3rxZxWIx9ZOf/EQppdQXvvAFtXnz5tHtt27dqvbu3auOHTumXnnlFXXrrbeqmpqakoeBwjZ26GPs8b3wwgtq8uTJ6rvf/a569dVX1ZYtW9SUKVPUoUOHQthbb4odm03X7mtf+5p67rnnVH9/v3rhhRfUkiVL1AUXXKBOnjyplLL7unk9Npuu24EDB9TkyZPVfffdp44ePaoefvhh9bGPfUx1dnaObrN582b1hS98YfT7N954Q33sYx9TX//619Wrr76qHnjgAVVVVaW6urrCOIQJlXN8f/u3f6ueeOIJdfToUXXo0CG1YcMGNWnSJPX000+HcQgTSqfTavbs2WrTpk3j/s3me04pb8em854jOCnTunXr1Jw5c9TUqVPVxz/+cfW5z31uNDBRaqTBW7t27ej3GzduVLNnz1ZTp05VM2fOVH/wB3+genp6Qtjz8oxtwMcen1JK/fCHP1SXXnqpmjp1qpo/f7568skng93JMhU7Npuu3S233KIaGhrU1KlT1axZs9Qtt9yiXn/99dF/t/m6eT02m66bUkrt2bNHLViwQFVXV6t58+apBx98MOff165dqz7zmc/k/OzZZ59VCxcuVFOnTlUXX3yx2rlzZ3A77JHX4+vo6FAtLS2qpqZGTZ8+XV133XVq3759Ae91afbu3atERB05cmTcv9l8zynl7dh03nMxpZTy3t8CAADgD3JOAACAUQhOAACAUQhOAACAUQhOAACAUQhOAACAUQhOAACAUQhOAACAUQhOAACAUQhOAACAUQhOAACAUQhOAACAUQhOAACAUf4/it2xGNS35uUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import umap\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "w = model.encoder.embedding.question_embedding.weight.detach().cpu().numpy()\n",
    "projector = umap.UMAP(n_components=2)\n",
    "wp = projector.fit_transform(w)\n",
    "plt.scatter(wp[:,0], wp[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e75554",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dataset import FinetuningDataset\n",
    "from model.dataset import PretrainingDataset\n",
    "from sklearn.model_selection import KFold\n",
    "# read in data and prepare transformations\n",
    "data = pd.read_csv(\"data/training_data/PreFer_train_data.csv\", low_memory=False)\n",
    "targets = pd.read_csv('data/training_data/PreFer_train_outcome.csv')\n",
    "targets = targets[targets.new_child.notna()].reset_index(drop=True)\n",
    "codebook = pd.read_csv('data/codebooks/PreFer_codebook.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11969897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lmmi/fertility-prediction-challenge/data_processing/pipeline.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  codebook[\"pairs\"] = codebook['var_name'].apply(get_generic_name)\n"
     ]
    }
   ],
   "source": [
    "n_features = 100\n",
    "\n",
    "importance = pd.read_csv('features_importance_1000.csv')\n",
    "custom_pairs = importance.iloc[:n_features].feature.map(lambda x: get_generic_name(x))\n",
    "sequences = encoding_pipeline(data, codebook, custom_pairs=custom_pairs)\n",
    "\n",
    "rnn_data = {person_id: (\n",
    "        torch.tensor([year-2007 for year, _ in wave_responses.items()]).to(device),\n",
    "        torch.tensor([ wave_response for _, wave_response in wave_responses.items()]).to(device)\n",
    "        )\n",
    "        for person_id, wave_responses in sequences.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(HIDDEN_SIZE=64,\n",
    "               ENCODING_SIZE=64,\n",
    "               NUM_COLS=44,\n",
    "               num_epochs_ft=5,\n",
    "               learning_rate_ft=1e-3,\n",
    "               sequences = []\n",
    "               ):\n",
    "\n",
    "    pretrain_dataset = PretrainingDataset(sequences)\n",
    "    SEQ_LEN = pretrain_dataset.get_seq_len()\n",
    "    VOCAB_SIZE = pretrain_dataset.get_vocab_size()\n",
    "\n",
    "    encoder = TabularEncoder(vocab_size=VOCAB_SIZE, \n",
    "                             embedding_size=HIDDEN_SIZE, \n",
    "                             output_size=ENCODING_SIZE, \n",
    "                             num_layers=2, \n",
    "                             sequence_len=SEQ_LEN, \n",
    "                             layer_type = \"excel\",\n",
    "                             num_cols=NUM_COLS,\n",
    "                             dropout=0.1\n",
    "                             ).to(device).to(device=device)\n",
    "\n",
    "    decoder = GRUDecoder(\n",
    "        input_size=ENCODING_SIZE,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=2,\n",
    "        max_seq_len=14,\n",
    "        dropout=0.2,\n",
    "        bidirectional=False,\n",
    "        with_attention = True\n",
    "    ).to(device)\n",
    "\n",
    "    # Define loss function and optimizer for RNN\n",
    "    ft_loss = nn.BCELoss()\n",
    "    ft_optimizer = torch.optim.NAdam(list(decoder.parameters()) + list(encoder.parameters()) , lr=learning_rate_ft, weight_decay=1e-2, decoupled_weight_decay=True)\n",
    "    ft_scheduler = optim.lr_scheduler.CosineAnnealingLR(ft_optimizer, T_max = num_epochs_ft, eta_min = 1e-6, last_epoch = -1)\n",
    "\n",
    "    # Training loop\n",
    "    decoder.train()\n",
    "    encoder.train()\n",
    "\n",
    "    return encoder, decoder, ft_optimizer, ft_loss, ft_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c190fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_and_step(loop_object, encoder, decoder, ft_scheduler, ft_loss, ft_optimizer):\n",
    "    for i, batch in loop_object :        \n",
    "\n",
    "        ft_optimizer.zero_grad() \n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "\n",
    "        input_year, input_seq = inputs\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings = encoder(input_year, input_seq).view(bs,ss, -1)\n",
    "        mask = ~((input_seq == 101).sum(-1) == NUM_COLS).view(bs,ss).detach()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = nn.functional.sigmoid(decoder(encodings, mask=mask))\n",
    "\n",
    "        loss = ft_loss(torch.flatten(outputs), labels)  \n",
    "\n",
    "        loss.backward()\n",
    "        ft_optimizer.step()\n",
    "\n",
    "    # On epoch end\n",
    "    ft_scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d56baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_dataloader, encoder, decoder):\n",
    "    val_loss = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    ## Set both models into the eval mode.=\n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "    for batch in test_dataloader:\n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "\n",
    "        input_year, input_seq = inputs\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings = encoder(input_year, input_seq).view(bs,ss, -1)\n",
    "        mask = ~((input_seq == 101).sum(-1) == NUM_COLS).view(bs,ss).detach()\n",
    "\n",
    "        # Forward pass\n",
    "        xx = decoder(encodings, mask)\n",
    "        outputs = torch.nn.functional.sigmoid(xx).flatten()\n",
    "        loss = ft_loss(outputs, labels)  \n",
    "        val_loss.append(loss.detach().cpu().numpy())\n",
    "        preds.extend(outputs.detach().cpu().numpy().tolist())\n",
    "        targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "\n",
    "    # Concatenate all the batches\n",
    "    predictions = (torch.tensor(preds) > 0.5).float()\n",
    "    probs = F.sigmoid(predictions)\n",
    "    actuals = torch.tensor(targets).flatten()\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(actuals.cpu().numpy(), predictions.cpu().numpy(), average='binary')\n",
    "    map_roc = average_precision_score(actuals.numpy(), probs.numpy())\n",
    "    \n",
    "    return precision, recall, f1, map_roc\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b22c2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 74it [00:14,  5.09it/s]\n",
      "Epochs 1: 74it [00:09,  8.13it/s]\n",
      "Epochs 2: 74it [00:09,  7.96it/s]\n",
      "Epochs 3: 74it [00:09,  7.82it/s]\n",
      "Epochs 4: 74it [00:09,  8.01it/s]\n",
      "Epochs 5: 74it [00:09,  7.93it/s]\n",
      "Epochs 6: 74it [00:09,  8.12it/s]\n",
      "Epochs 7: 74it [00:09,  8.19it/s]\n",
      "Epochs 8: 74it [00:08,  8.25it/s]\n",
      "Epochs 9: 74it [00:08,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 74it [00:08,  8.30it/s]\n",
      "Epochs 1: 74it [00:08,  8.26it/s]\n",
      "Epochs 2: 74it [00:08,  8.33it/s]\n",
      "Epochs 3: 74it [00:09,  8.14it/s]\n",
      "Epochs 4: 74it [00:09,  7.91it/s]\n",
      "Epochs 5: 74it [00:08,  8.26it/s]\n",
      "Epochs 6: 74it [00:08,  8.25it/s]\n",
      "Epochs 7: 74it [00:08,  8.37it/s]\n",
      "Epochs 8: 74it [00:08,  8.31it/s]\n",
      "Epochs 9: 74it [00:09,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 74it [00:09,  7.96it/s]\n",
      "Epochs 1: 74it [00:08,  8.23it/s]\n",
      "Epochs 2: 74it [00:09,  8.01it/s]\n",
      "Epochs 3: 74it [00:08,  8.27it/s]\n",
      "Epochs 4: 74it [00:08,  8.33it/s]\n",
      "Epochs 5: 74it [00:09,  8.06it/s]\n",
      "Epochs 6: 74it [00:09,  8.20it/s]\n",
      "Epochs 7: 74it [00:09,  7.96it/s]\n",
      "Epochs 8: 74it [00:09,  8.05it/s]\n",
      "Epochs 9: 74it [00:09,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 75it [00:09,  8.07it/s]\n",
      "Epochs 1: 75it [00:09,  8.03it/s]\n",
      "Epochs 2: 75it [00:09,  8.11it/s]\n",
      "Epochs 3: 75it [00:09,  8.24it/s]\n",
      "Epochs 4: 75it [00:09,  8.26it/s]\n",
      "Epochs 5: 75it [00:09,  8.13it/s]\n",
      "Epochs 6: 75it [00:09,  8.12it/s]\n",
      "Epochs 7: 75it [00:09,  8.16it/s]\n",
      "Epochs 8: 75it [00:09,  8.28it/s]\n",
      "Epochs 9: 75it [00:09,  8.27it/s]\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE=64\n",
    "ENCODING_SIZE=64\n",
    "NUM_COLS=44\n",
    "num_epochs_ft=10\n",
    "learning_rate_ft=1e-3\n",
    "rnn_batch_size = 10\n",
    "\n",
    "n_splits = 4\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare for cross-validation\n",
    "prec_per_fold = []\n",
    "rec_per_fold = []\n",
    "f1_per_fold = []\n",
    "map_roc_per_fold = []\n",
    "\n",
    "train_prec_per_fold = []\n",
    "train_rec_per_fold = []\n",
    "train_f1_per_fold = []\n",
    "train_map_roc_per_fold = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(targets['nomem_encr'])):\n",
    "    print(f'Fold: {fold}')\n",
    "    train_person_ids = targets.loc[train_index, 'nomem_encr']\n",
    "    test_person_ids = targets.loc[val_index, 'nomem_encr']\n",
    "    \n",
    "    encoder, decoder, ft_optimizer, ft_loss, ft_scheduler = initialize(\n",
    "        HIDDEN_SIZE=64,\n",
    "        ENCODING_SIZE=64,\n",
    "        NUM_COLS=44,\n",
    "        num_epochs_ft=1,\n",
    "        learning_rate_ft=1e-3,\n",
    "        sequences=sequences,\n",
    "        )\n",
    "\n",
    "    train_data = {person_id: rnn_data[person_id] for person_id in train_person_ids}\n",
    "    test_data = {person_id: rnn_data[person_id] for person_id in test_person_ids}\n",
    "    \n",
    "    train_dataset = FinetuningDataset(train_data, targets = targets)\n",
    "    test_dataset = FinetuningDataset(test_data, targets = targets)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=rnn_batch_size, shuffle=True)\n",
    "    test_dataloader  = DataLoader(test_dataset,  batch_size=rnn_batch_size)\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs_ft):\n",
    "        loop_object  = tqdm(enumerate(train_dataloader), desc=f\"Epochs {epoch}\")\n",
    "    \n",
    "        evaluate_and_step(loop_object, encoder, decoder, ft_scheduler, ft_loss, ft_optimizer)\n",
    "\n",
    "    precision, recall, f1, map_roc = evaluate(test_dataloader, encoder, decoder)\n",
    "    precision_train, recall_train, f1_train, map_roc_train = evaluate(train_dataloader, encoder, decoder)\n",
    "    \n",
    "    prec_per_fold.append(precision)\n",
    "    rec_per_fold.append(recall)\n",
    "    f1_per_fold.append(f1)\n",
    "    map_roc_per_fold.append(map_roc)\n",
    "\n",
    "    train_prec_per_fold.append(precision_train)\n",
    "    train_rec_per_fold.append(recall_train)\n",
    "    train_f1_per_fold.append(f1_train)\n",
    "    train_map_roc_per_fold.append(map_roc_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e95d939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on test set\n",
      "Prec: 0.783 0.714 0.844 0.552\n",
      "Recall: 0.310 0.490 0.731 0.941\n",
      "f1: 0.444 0.581 0.784 0.696\n",
      "map roc: 0.405 0.455 0.674 0.531\n"
     ]
    }
   ],
   "source": [
    "print(\"Results on test set\")\n",
    "print(\"Prec:\", ' '.join(f\"{x:.3f}\" for x in prec_per_fold))\n",
    "print(\"Recall:\", ' '.join(f\"{x:.3f}\" for x in rec_per_fold))\n",
    "print(\"f1:\", ' '.join(f\"{x:.3f}\" for x in f1_per_fold))\n",
    "print(\"map roc:\", ' '.join(f\"{x:.3f}\" for x in map_roc_per_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef64799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on training set\n",
      "Prec: 0.978 0.964 0.901 0.708\n",
      "Recall: 0.565 0.839 0.912 0.963\n",
      "f1: 0.716 0.897 0.907 0.816\n",
      "map roc: 0.643 0.844 0.841 0.689\n"
     ]
    }
   ],
   "source": [
    "print('Results on training set')\n",
    "print(\"Prec:\", ' '.join(f\"{x:.3f}\" for x in train_prec_per_fold))\n",
    "print(\"Recall:\", ' '.join(f\"{x:.3f}\" for x in train_rec_per_fold))\n",
    "print(\"f1:\", ' '.join(f\"{x:.3f}\" for x in train_f1_per_fold))\n",
    "print(\"map roc:\", ' '.join(f\"{x:.3f}\" for x in train_map_roc_per_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110e679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f28042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PreFer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
