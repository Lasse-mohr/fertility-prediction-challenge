{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c9914262-7a3e-4e3a-9e07-043d40efd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data packages\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, average_precision_score\n",
    "\n",
    "from model.rnn import GRUDecoder\n",
    "from model.encoders import CustomExcelFormer\n",
    "from data_processing.pipeline import encoding_pipeline, get_generic_name\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from model.utils import get_device\n",
    "from model.dataset import PretrainingDataset\n",
    "from model.dataset import FinetuningDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18aee1dd-7884-40e7-be05-db070f1a452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders) device\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b651aa-752a-4c71-990a-332ff4099791",
   "metadata": {},
   "source": [
    "# Read the data\n",
    "\n",
    "Right now the notebook is set to work with fake data. This can be changed once the pipeline works.\n",
    "\n",
    "The data is stored as a Dict[person_id, Sequences] where Sequences is a Dict[year, survery_wave_response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b872e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataClass:\n",
    "    def __init__(self,\n",
    "                 data_path: str = \"data/training_data/PreFer_train_data.csv\",\n",
    "                 targets_path: str = 'data/training_data/PreFer_train_outcome.csv',\n",
    "                 codebook_path: str = 'data/codebooks/PreFer_codebook.csv',\n",
    "                 importance_path: str = 'features_importance_all.csv') -> None:\n",
    "        self.data = pd.read_csv(data_path, low_memory=False)\n",
    "        self.targets = pd.read_csv(targets_path)\n",
    "        self.codebook = pd.read_csv(codebook_path)\n",
    "        self.col_importance = pd.read_csv(importance_path)\n",
    "    def make_sequences(self, n_cols: int, use_codebook: bool = True):\n",
    "        custom_pairs = self.col_importance.feature.map(lambda x: get_generic_name(x)).unique()[:n_cols]\n",
    "        self.sequences = encoding_pipeline(self.data, self.codebook, \n",
    "                                           custom_pairs=custom_pairs, \n",
    "                                           importance=self.col_importance, \n",
    "                                           use_codebook=use_codebook)\n",
    "    def make_pretraining(self):\n",
    "        self.pretrain_dataset = PretrainingDataset(self.sequences)\n",
    "        self.seq_len = self.pretrain_dataset.get_seq_len()\n",
    "        self.vocab_size = self.pretrain_dataset.get_vocab_size()\n",
    "    def make_finetuning(self, batch_size, test_size: float = 0.2, val_size: float = 0.2):\n",
    "        targets = self.targets[self.targets.new_child.notna()]\n",
    "        train_person_ids, test_person_ids = train_test_split(targets['nomem_encr'], test_size=test_size, random_state=42)\n",
    "        train_person_ids, val_person_ids = train_test_split(train_person_ids, test_size=val_size, random_state=42)\n",
    "        rnn_data = {person_id: (\n",
    "                torch.tensor([year-2007 for year, _ in wave_responses.items()]).to(device),\n",
    "                torch.tensor([ wave_response for _, wave_response in wave_responses.items()]).to(device)\n",
    "                )\n",
    "                for person_id, wave_responses in self.sequences.items()\n",
    "                }\n",
    "\n",
    "        # split data based on the splits made for the target\n",
    "        train_data = {person_id: rnn_data[person_id] for person_id in train_person_ids}\n",
    "        val_data = {person_id: rnn_data[person_id] for person_id in val_person_ids}\n",
    "        test_data = {person_id: rnn_data[person_id] for person_id in test_person_ids}\n",
    "\n",
    "        self.train_dataset = FinetuningDataset(train_data, targets = targets)\n",
    "        self.val_dataset = FinetuningDataset(val_data, targets = targets)\n",
    "        self.test_dataset = FinetuningDataset(test_data, targets = targets)\n",
    "        self.train_dataloader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.val_dataloader = DataLoader(self.val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.test_dataloader  = DataLoader(self.test_dataset,  batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4e1c0",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "279b9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f7e4345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomarx/Documents/GitHub/fertility-prediction-challenge/data_processing/pipeline.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  codebook[\"pairs\"] = codebook['var_name'].apply(get_generic_name)\n"
     ]
    }
   ],
   "source": [
    "data.make_sequences(n_cols=150)\n",
    "data.make_pretraining()\n",
    "data.make_finetuning(batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d77d87a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "#ENCODING_SIZE = 64\n",
    "BATCH_SIZE = 16\n",
    "HIDDEN_SIZE = 128\n",
    "ENCODING_SIZE = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4\n",
    "NUM_EPOCHS = 12\n",
    "DETECT_ANOMALY = False\n",
    "SEQ_LEN = data.seq_len\n",
    "VOCAB_SIZE = data.vocab_size\n",
    "\n",
    "LR = 7e-3\n",
    "\n",
    "assert HIDDEN_SIZE % NUM_HEADS == 0, \"Check that the hidden size is divisible\"\n",
    "print(SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddb66e-cba5-4bb9-854d-811d49599b93",
   "metadata": {},
   "source": [
    "# Experimental Encoder (Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada8018b-7ebc-4db4-aadf-191b214b1470",
   "metadata": {},
   "source": [
    "# Train the RNN\n",
    "\n",
    "First we need to create Dataset class that can hold both the target (stored in a pd.DataFrame) and the sequences.\n",
    "\n",
    "The sequences will be of dimension 14 x encoding_dimension, because we have 14 years of surveys.\n",
    "\n",
    "I have created some code for getting the data into the right format, but it might not be useful.\n",
    "\n",
    "## Regarding masks\n",
    "Right now the masking is done already in the encoding. I haven't found exactly where Mikkel implemented this.\n",
    "So for now, assume that nothing is padded, and then we'll figure it out with Mikkel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "54a2330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreFerPredictor(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = CustomExcelFormer(vocab_size=VOCAB_SIZE, \n",
    "                            hidden_size=HIDDEN_SIZE, \n",
    "                            out_size=ENCODING_SIZE,\n",
    "                            n_years=14,\n",
    "                            num_heads=NUM_HEADS,\n",
    "                            num_layers=NUM_LAYERS, \n",
    "                            sequence_len=SEQ_LEN, \n",
    "                            aium_dropout=0.2,\n",
    "                            diam_dropout=0.15,\n",
    "                            residual_dropout=0.00,\n",
    "                            embedding_dropout=0.05).to(device)\n",
    "        self.decoder = GRUDecoder(\n",
    "            input_size=ENCODING_SIZE,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_layers=3,\n",
    "            max_seq_len=14,\n",
    "            dropout=0.2,\n",
    "  \n",
    "            bidirectional=True,\n",
    "            with_attention = True\n",
    "        ).to(device)\n",
    "\n",
    "        self.enc_dropout = nn.Dropout(0.1)\n",
    "        self.enc_dropout1d = nn.Dropout1d(0.05)\n",
    "    def forward(self, input_year, input_seq, labels):\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings, _ = self.encoder(input_year, input_seq)#, y=labels.unsqueeze(-1).expand(-1, 14).reshape(-1), mixup_encoded=True)\n",
    "        encodings = encodings.view(bs,ss, -1)\n",
    "        encodings = self.enc_dropout(encodings)\n",
    "        encodings = self.enc_dropout1d(encodings)\n",
    "        mask = ~((input_seq == 101).sum(-1) == SEQ_LEN).view(bs,ss).detach()\n",
    "        # Forward pass\n",
    "        out = self.decoder(encodings, mask=mask).flatten()\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d0de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "caf49964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "model = PreFerPredictor().to(device)\n",
    "# Define loss function and optimizer for RNN\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([1/0.2]).to(device))\n",
    "optimizer = torch.optim.RAdam(model.parameters() , lr=LR, weight_decay=1e-2, decoupled_weight_decay=True)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=LR, weight_decay=1e-2)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = NUM_EPOCHS * len(data.train_dataloader), eta_min = 5e-4, last_epoch = -1)\n",
    "#scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n",
    "#scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-2, epochs=NUM_EPOCHS, steps_per_epoch=len(data.train_dataloader))\n",
    "#scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \n",
    "#                                                           T_0 = 8 * len(data.train_dataloader), \n",
    "#                                                           eta_min = 1e-4, \n",
    "#                                                           last_epoch = -1)\n",
    "\n",
    "## Stochaistic Weight Averaging\n",
    "#avg_fn = optim.swa_utils.get_swa_avg_fn()\n",
    "avg_fn = optim.swa_utils.get_ema_avg_fn(0.95)\n",
    "avg_model = optim.swa_utils.AveragedModel(model, avg_fn=avg_fn, use_buffers=False)#multi_avg_fn=optim.swa_utils.get_swa_multi_avg_fn())\n",
    "avg_start = 3\n",
    "#avg_scheduler = optim.swa_utils.SWALR(optimizer, swa_lr=1e-3) #typically has a high lr\n",
    "\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "avg_model.train()\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "649bc948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(epoch):\n",
    "    val_loss = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    model.eval()\n",
    "    avg_model.eval()\n",
    "    for batch in data.val_dataloader:\n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "        input_year, input_seq = inputs\n",
    "        if epoch <= avg_start:\n",
    "            output = model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "        else:\n",
    "            output = avg_model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "\n",
    "        probs = F.sigmoid(output).flatten()\n",
    "        loss = loss_fn(output, labels)  \n",
    "        val_loss.append(loss.detach().cpu().numpy())\n",
    "        preds.extend(probs.detach().cpu().numpy().tolist())\n",
    "        targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    # Concatenate all the batches\n",
    "    yhat = torch.tensor(preds).flatten().detach().cpu().numpy()\n",
    "    ytrue = torch.tensor(targets).flatten().cpu().numpy()\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(ytrue, yhat > 0.5, average='binary')\n",
    "    map_roc = average_precision_score(ytrue, yhat)\n",
    "    print(f\"-- mAP Score: {map_roc:.4f} -- f1-score: {f1:.3f}\")\n",
    "    model.train()\n",
    "    avg_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "82cf1777-49e7-462d-ae51-549ea6c8305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 40it [00:18,  2.16it/s, mean loss: 1.539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12, Loss: 1.5391\n",
      "-- mAP Score: 0.4995 -- f1-score: 0.327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 1: 40it [00:18,  2.22it/s, mean loss: 1.160]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12, Loss: 1.1597\n",
      "-- mAP Score: 0.5983 -- f1-score: 0.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 2: 40it [00:18,  2.21it/s, mean loss: 0.719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12, Loss: 0.7190\n",
      "-- mAP Score: 0.7473 -- f1-score: 0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 3: 40it [00:18,  2.22it/s, mean loss: 0.477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12, Loss: 0.4772\n",
      "-- mAP Score: 0.7311 -- f1-score: 0.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 4: 40it [00:19,  2.06it/s, mean loss: 0.289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/12, Loss: 0.2894\n",
      "-- mAP Score: 0.7465 -- f1-score: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 5: 40it [00:19,  2.07it/s, mean loss: 0.257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/12, Loss: 0.2572\n",
      "-- mAP Score: 0.7232 -- f1-score: 0.744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 6: 40it [00:19,  2.07it/s, mean loss: 0.244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/12, Loss: 0.2444\n",
      "-- mAP Score: 0.7807 -- f1-score: 0.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 7: 40it [00:19,  2.09it/s, mean loss: 0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/12, Loss: 0.1720\n",
      "-- mAP Score: 0.7774 -- f1-score: 0.756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 8: 40it [00:19,  2.06it/s, mean loss: 0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/12, Loss: 0.1779\n",
      "-- mAP Score: 0.7905 -- f1-score: 0.771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 9: 8it [00:04,  1.80it/s, mean loss: 0.061]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     loop_object\u001b[38;5;241m.\u001b[39mset_postfix_str(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean loss: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(loss_per_step[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:]))\n\u001b[1;32m     18\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 19\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# AVERAGING starts only after 3 epochs\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/radam.py:131\u001b[0m, in \u001b[0;36mRAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    127\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    129\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, grads, exp_avgs, exp_avg_sqs, state_steps)\n\u001b[0;32m--> 131\u001b[0m     \u001b[43mradam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/radam.py:266\u001b[0m, in \u001b[0;36mradam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, state_steps, decoupled_weight_decay, foreach, differentiable, capturable, has_complex, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_radam\n\u001b[0;32m--> 266\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/radam.py:367\u001b[0m, in \u001b[0;36m_single_tensor_radam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, state_steps, beta1, beta2, lr, weight_decay, eps, differentiable, decoupled_weight_decay, capturable, has_complex)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rho_t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5.0\u001b[39m:\n\u001b[0;32m--> 367\u001b[0m         param\u001b[38;5;241m.\u001b[39madd_(bias_corrected_exp_avg \u001b[38;5;241m*\u001b[39m lr \u001b[38;5;241m*\u001b[39m \u001b[43m_compute_adaptive_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m _compute_rect(), alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m         param\u001b[38;5;241m.\u001b[39madd_(bias_corrected_exp_avg \u001b[38;5;241m*\u001b[39m lr, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/optim/radam.py:359\u001b[0m, in \u001b[0;36m_single_tensor_radam.<locals>._compute_adaptive_lr\u001b[0;34m()\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m exp_avg_sq_sqrt\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mbias_correction2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexp_avg_sq_sqrt\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__rdiv__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rdiv__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreciprocal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m other\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_per_epoch = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # print(epoch)\n",
    "    loss_per_step = []\n",
    "    loop_object  = tqdm(enumerate(data.train_dataloader), desc=f\"Epochs {epoch}\")\n",
    "    for i, batch in loop_object :        \n",
    "        optimizer.zero_grad() \n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "        input_year, input_seq = inputs\n",
    "        ### Model\n",
    "        output = model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "        probs = F.sigmoid(output).flatten()\n",
    "        ### Loss\n",
    "        loss = loss_fn(output, labels)  \n",
    "        loss_per_step.append(loss.detach().cpu().numpy())\n",
    "        loop_object.set_postfix_str(\"mean loss: %.3f\"%np.mean(loss_per_step[-100:]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # AVERAGING starts only after 3 epochs\n",
    "        if epoch > avg_start:\n",
    "            avg_model.update_parameters(model)\n",
    "    # On epoch end\n",
    "    # On epoch end\n",
    "    loss_per_epoch.append(np.mean(loss_per_step))\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss_per_epoch[-1]:.4f}\")\n",
    "    run_validation(epoch=epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d0d2fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7000\n",
      "Recall: 0.7000\n",
      "F1 Score: 0.7000\n",
      "-- mAP Score: 0.8382 --\n"
     ]
    }
   ],
   "source": [
    "test_loss = []\n",
    "preds = []\n",
    "targets = []\n",
    "\n",
    "## Set both models into the eval mode.=\n",
    "avg_model.eval()\n",
    "for batch in data.test_dataloader:\n",
    "    inputs, labels = batch\n",
    "    labels = labels.to(torch.float).to(device)\n",
    "    input_year, input_seq = inputs\n",
    "    ### Model\n",
    "    output = avg_model(input_year=input_year, input_seq=input_seq, labels=labels)\n",
    "    probs = F.sigmoid(output).flatten()\n",
    "\n",
    "    loss = loss_fn(output, labels)  \n",
    "    test_loss.append(loss.detach().cpu().numpy())\n",
    "    preds.extend(probs.detach().cpu().numpy().tolist())\n",
    "    targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "# Concatenate all the batches\n",
    "probs = torch.tensor(preds).flatten()\n",
    "actuals = torch.tensor(targets).flatten()\n",
    "# Concatenate all the batches\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(actuals.cpu().numpy(), probs.cpu().numpy() > 0.5, average='binary')\n",
    "map_roc = average_precision_score(actuals.numpy(), probs.numpy())\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"-- mAP Score: {map_roc:.4f} --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f2a219b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def bootstrap(preds, targs, metric, n_bootstraps: int):\n",
    "    results = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        idx = np.random.choice(preds.shape[0], size=preds.shape[0], replace=True)\n",
    "        results.append(metric(targs[idx], preds[idx]))\n",
    "\n",
    "    return {\"mean\": np.mean(results),\n",
    "            \"CIs\": np.quantile(results,q=np.array([0.025, 0.975]))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "40bd71b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.6981486802796952, 'CIs': array([0.58925945, 0.7962963 ])}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap( probs.cpu().numpy() > 0.5, actuals.cpu().numpy(), metric= lambda x,y: f1_score(x,y), n_bootstraps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b8e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import umap\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "w = avg_model.get_submodule(\"module\").encoder.embedding.question_embedding.weight.detach().cpu().numpy()\n",
    "projector = umap.UMAP(n_components=2)\n",
    "wp = projector.fit_transform(w)\n",
    "plt.scatter(wp[:,0], wp[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e75554",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dataset import FinetuningDataset\n",
    "from model.dataset import PretrainingDataset\n",
    "from sklearn.model_selection import KFold\n",
    "# read in data and prepare transformations\n",
    "data = pd.read_csv(\"data/training_data/PreFer_train_data.csv\", low_memory=False)\n",
    "targets = pd.read_csv('data/training_data/PreFer_train_outcome.csv')\n",
    "targets = targets[targets.new_child.notna()].reset_index(drop=True)\n",
    "codebook = pd.read_csv('data/codebooks/PreFer_codebook.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11969897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lmmi/fertility-prediction-challenge/data_processing/pipeline.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  codebook[\"pairs\"] = codebook['var_name'].apply(get_generic_name)\n"
     ]
    }
   ],
   "source": [
    "n_features = 100\n",
    "\n",
    "importance = pd.read_csv('features_importance_1000.csv')\n",
    "custom_pairs = importance.iloc[:n_features].feature.map(lambda x: get_generic_name(x))\n",
    "sequences = encoding_pipeline(data, codebook, custom_pairs=custom_pairs)\n",
    "\n",
    "rnn_data = {person_id: (\n",
    "        torch.tensor([year-2007 for year, _ in wave_responses.items()]).to(device),\n",
    "        torch.tensor([ wave_response for _, wave_response in wave_responses.items()]).to(device)\n",
    "        )\n",
    "        for person_id, wave_responses in sequences.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(HIDDEN_SIZE=64,\n",
    "               ENCODING_SIZE=64,\n",
    "               NUM_COLS=44,\n",
    "               num_epochs_ft=5,\n",
    "               learning_rate_ft=1e-3,\n",
    "               sequences = []\n",
    "               ):\n",
    "\n",
    "    pretrain_dataset = PretrainingDataset(sequences)\n",
    "    SEQ_LEN = pretrain_dataset.get_seq_len()\n",
    "    VOCAB_SIZE = pretrain_dataset.get_vocab_size()\n",
    "\n",
    "    encoder = TabularEncoder(vocab_size=VOCAB_SIZE, \n",
    "                             embedding_size=HIDDEN_SIZE, \n",
    "                             output_size=ENCODING_SIZE, \n",
    "                             num_layers=2, \n",
    "                             sequence_len=SEQ_LEN, \n",
    "                             layer_type = \"excel\",\n",
    "                             num_cols=NUM_COLS,\n",
    "                             dropout=0.1\n",
    "                             ).to(device).to(device=device)\n",
    "\n",
    "    decoder = GRUDecoder(\n",
    "        input_size=ENCODING_SIZE,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=2,\n",
    "        max_seq_len=14,\n",
    "        dropout=0.2,\n",
    "        bidirectional=False,\n",
    "        with_attention = True\n",
    "    ).to(device)\n",
    "\n",
    "    # Define loss function and optimizer for RNN\n",
    "    ft_loss = nn.BCELoss()\n",
    "    ft_optimizer = torch.optim.NAdam(list(decoder.parameters()) + list(encoder.parameters()) , lr=learning_rate_ft, weight_decay=1e-2, decoupled_weight_decay=True)\n",
    "    ft_scheduler = optim.lr_scheduler.CosineAnnealingLR(ft_optimizer, T_max = num_epochs_ft, eta_min = 1e-6, last_epoch = -1)\n",
    "\n",
    "    # Training loop\n",
    "    decoder.train()\n",
    "    encoder.train()\n",
    "\n",
    "    return encoder, decoder, ft_optimizer, ft_loss, ft_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c190fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_and_step(loop_object, encoder, decoder, ft_scheduler, ft_loss, ft_optimizer):\n",
    "    for i, batch in loop_object :        \n",
    "\n",
    "        ft_optimizer.zero_grad() \n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "\n",
    "        input_year, input_seq = inputs\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings = encoder(input_year, input_seq).view(bs,ss, -1)\n",
    "        mask = ~((input_seq == 101).sum(-1) == NUM_COLS).view(bs,ss).detach()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = nn.functional.sigmoid(decoder(encodings, mask=mask))\n",
    "\n",
    "        loss = ft_loss(torch.flatten(outputs), labels)  \n",
    "\n",
    "        loss.backward()\n",
    "        ft_optimizer.step()\n",
    "\n",
    "    # On epoch end\n",
    "    ft_scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d56baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_dataloader, encoder, decoder):\n",
    "    val_loss = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    ## Set both models into the eval mode.=\n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "    for batch in test_dataloader:\n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "\n",
    "        input_year, input_seq = inputs\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings = encoder(input_year, input_seq).view(bs,ss, -1)\n",
    "        mask = ~((input_seq == 101).sum(-1) == NUM_COLS).view(bs,ss).detach()\n",
    "\n",
    "        # Forward pass\n",
    "        xx = decoder(encodings, mask)\n",
    "        outputs = torch.nn.functional.sigmoid(xx).flatten()\n",
    "        loss = ft_loss(outputs, labels)  \n",
    "        val_loss.append(loss.detach().cpu().numpy())\n",
    "        preds.extend(outputs.detach().cpu().numpy().tolist())\n",
    "        targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "\n",
    "    # Concatenate all the batches\n",
    "    predictions = (torch.tensor(preds) > 0.5).float()\n",
    "    probs = F.sigmoid(predictions)\n",
    "    actuals = torch.tensor(targets).flatten()\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(actuals.cpu().numpy(), predictions.cpu().numpy(), average='binary')\n",
    "    map_roc = average_precision_score(actuals.numpy(), probs.numpy())\n",
    "    \n",
    "    return precision, recall, f1, map_roc\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b22c2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 74it [00:14,  5.09it/s]\n",
      "Epochs 1: 74it [00:09,  8.13it/s]\n",
      "Epochs 2: 74it [00:09,  7.96it/s]\n",
      "Epochs 3: 74it [00:09,  7.82it/s]\n",
      "Epochs 4: 74it [00:09,  8.01it/s]\n",
      "Epochs 5: 74it [00:09,  7.93it/s]\n",
      "Epochs 6: 74it [00:09,  8.12it/s]\n",
      "Epochs 7: 74it [00:09,  8.19it/s]\n",
      "Epochs 8: 74it [00:08,  8.25it/s]\n",
      "Epochs 9: 74it [00:08,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 74it [00:08,  8.30it/s]\n",
      "Epochs 1: 74it [00:08,  8.26it/s]\n",
      "Epochs 2: 74it [00:08,  8.33it/s]\n",
      "Epochs 3: 74it [00:09,  8.14it/s]\n",
      "Epochs 4: 74it [00:09,  7.91it/s]\n",
      "Epochs 5: 74it [00:08,  8.26it/s]\n",
      "Epochs 6: 74it [00:08,  8.25it/s]\n",
      "Epochs 7: 74it [00:08,  8.37it/s]\n",
      "Epochs 8: 74it [00:08,  8.31it/s]\n",
      "Epochs 9: 74it [00:09,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 74it [00:09,  7.96it/s]\n",
      "Epochs 1: 74it [00:08,  8.23it/s]\n",
      "Epochs 2: 74it [00:09,  8.01it/s]\n",
      "Epochs 3: 74it [00:08,  8.27it/s]\n",
      "Epochs 4: 74it [00:08,  8.33it/s]\n",
      "Epochs 5: 74it [00:09,  8.06it/s]\n",
      "Epochs 6: 74it [00:09,  8.20it/s]\n",
      "Epochs 7: 74it [00:09,  7.96it/s]\n",
      "Epochs 8: 74it [00:09,  8.05it/s]\n",
      "Epochs 9: 74it [00:09,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 75it [00:09,  8.07it/s]\n",
      "Epochs 1: 75it [00:09,  8.03it/s]\n",
      "Epochs 2: 75it [00:09,  8.11it/s]\n",
      "Epochs 3: 75it [00:09,  8.24it/s]\n",
      "Epochs 4: 75it [00:09,  8.26it/s]\n",
      "Epochs 5: 75it [00:09,  8.13it/s]\n",
      "Epochs 6: 75it [00:09,  8.12it/s]\n",
      "Epochs 7: 75it [00:09,  8.16it/s]\n",
      "Epochs 8: 75it [00:09,  8.28it/s]\n",
      "Epochs 9: 75it [00:09,  8.27it/s]\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE=64\n",
    "ENCODING_SIZE=64\n",
    "NUM_COLS=44\n",
    "num_epochs_ft=10\n",
    "learning_rate_ft=1e-3\n",
    "rnn_batch_size = 10\n",
    "\n",
    "n_splits = 4\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare for cross-validation\n",
    "prec_per_fold = []\n",
    "rec_per_fold = []\n",
    "f1_per_fold = []\n",
    "map_roc_per_fold = []\n",
    "\n",
    "train_prec_per_fold = []\n",
    "train_rec_per_fold = []\n",
    "train_f1_per_fold = []\n",
    "train_map_roc_per_fold = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(targets['nomem_encr'])):\n",
    "    print(f'Fold: {fold}')\n",
    "    train_person_ids = targets.loc[train_index, 'nomem_encr']\n",
    "    test_person_ids = targets.loc[val_index, 'nomem_encr']\n",
    "    \n",
    "    encoder, decoder, ft_optimizer, ft_loss, ft_scheduler = initialize(\n",
    "        HIDDEN_SIZE=64,\n",
    "        ENCODING_SIZE=64,\n",
    "        NUM_COLS=44,\n",
    "        num_epochs_ft=1,\n",
    "        learning_rate_ft=1e-3,\n",
    "        sequences=sequences,\n",
    "        )\n",
    "\n",
    "    train_data = {person_id: rnn_data[person_id] for person_id in train_person_ids}\n",
    "    test_data = {person_id: rnn_data[person_id] for person_id in test_person_ids}\n",
    "    \n",
    "    train_dataset = FinetuningDataset(train_data, targets = targets)\n",
    "    test_dataset = FinetuningDataset(test_data, targets = targets)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=rnn_batch_size, shuffle=True)\n",
    "    test_dataloader  = DataLoader(test_dataset,  batch_size=rnn_batch_size)\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs_ft):\n",
    "        loop_object  = tqdm(enumerate(train_dataloader), desc=f\"Epochs {epoch}\")\n",
    "    \n",
    "        evaluate_and_step(loop_object, encoder, decoder, ft_scheduler, ft_loss, ft_optimizer)\n",
    "\n",
    "    precision, recall, f1, map_roc = evaluate(test_dataloader, encoder, decoder)\n",
    "    precision_train, recall_train, f1_train, map_roc_train = evaluate(train_dataloader, encoder, decoder)\n",
    "    \n",
    "    prec_per_fold.append(precision)\n",
    "    rec_per_fold.append(recall)\n",
    "    f1_per_fold.append(f1)\n",
    "    map_roc_per_fold.append(map_roc)\n",
    "\n",
    "    train_prec_per_fold.append(precision_train)\n",
    "    train_rec_per_fold.append(recall_train)\n",
    "    train_f1_per_fold.append(f1_train)\n",
    "    train_map_roc_per_fold.append(map_roc_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e95d939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on test set\n",
      "Prec: 0.783 0.714 0.844 0.552\n",
      "Recall: 0.310 0.490 0.731 0.941\n",
      "f1: 0.444 0.581 0.784 0.696\n",
      "map roc: 0.405 0.455 0.674 0.531\n"
     ]
    }
   ],
   "source": [
    "print(\"Results on test set\")\n",
    "print(\"Prec:\", ' '.join(f\"{x:.3f}\" for x in prec_per_fold))\n",
    "print(\"Recall:\", ' '.join(f\"{x:.3f}\" for x in rec_per_fold))\n",
    "print(\"f1:\", ' '.join(f\"{x:.3f}\" for x in f1_per_fold))\n",
    "print(\"map roc:\", ' '.join(f\"{x:.3f}\" for x in map_roc_per_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on training set\n",
      "Prec: 0.978 0.964 0.901 0.708\n",
      "Recall: 0.565 0.839 0.912 0.963\n",
      "f1: 0.716 0.897 0.907 0.816\n",
      "map roc: 0.643 0.844 0.841 0.689\n"
     ]
    }
   ],
   "source": [
    "print('Results on training set')\n",
    "print(\"Prec:\", ' '.join(f\"{x:.3f}\" for x in train_prec_per_fold))\n",
    "print(\"Recall:\", ' '.join(f\"{x:.3f}\" for x in train_rec_per_fold))\n",
    "print(\"f1:\", ' '.join(f\"{x:.3f}\" for x in train_f1_per_fold))\n",
    "print(\"map roc:\", ' '.join(f\"{x:.3f}\" for x in train_map_roc_per_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110e679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f28042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PreFer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
