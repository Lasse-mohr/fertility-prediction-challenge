{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9914262-7a3e-4e3a-9e07-043d40efd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data packages\n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model.rnn import GRUDecoder, SimpleDecoder\n",
    "from model.autoencoder import AutoEncoder, SimpleAutoEncoder\n",
    "from data_processing.pipeline import encoding_pipeline, get_generic_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18aee1dd-7884-40e7-be05-db070f1a452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders) device\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    # Check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        # If CUDA is available, select the first CUDA device\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(\"Using CUDA device:\", torch.cuda.get_device_name(0))\n",
    "    # Check for MPS availability on supported macOS devices (requires PyTorch 1.12 or newer)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        # If MPS is available, use MPS device\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS (Metal Performance Shaders) device\")\n",
    "    else:\n",
    "        # Fallback to CPU if neither CUDA nor MPS is available\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b651aa-752a-4c71-990a-332ff4099791",
   "metadata": {},
   "source": [
    "# Read the data\n",
    "\n",
    "Right now the notebook is set to work with fake data. This can be changed once the pipeline works.\n",
    "\n",
    "The data is stored as a Dict[person_id, Sequences] where Sequences is a Dict[year, survery_wave_response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c3f871-21e8-418c-9aa7-31ad09283a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pk/3vzybg253k1d3n7qzxkts_2c0000gn/T/ipykernel_8976/1529815470.py:2: DtypeWarning: Columns (2583,2584,2585,2586,2587,2588,2589,4358,4359,4360,4361,4362,4363,4364,4365,4366,4367,4368,4369,4370,4371,4372,4373,4374,4375,4379,4380,4381,4382,4383,4384,4385,4386,4387,4388,4389,4390,4391,4392,4393,4394,4395,4396,4397,4398,4399,4400,4401,4405,4406,4407,4408,4409,5215,5216,5219,5220,5613,5614,5615,5616,5617,5618,5619,5620,5621,5622,5624,5625,5626,5627,5628,5629,5630,5631,5632,5633,5634,5635,5636,5638,5639,5640,5787,5788,5789,5790,5791,5792,5793,5794,5795,5796,6393,6394,6395,6396,6397,6398,6399,6400,6401,6402,6403,6619,6620,6621,6622,6623,6624,6625,6626,6627,6628,6629,6630,6631,6632,6633,6634,6635,6638,6640,6641,6642,6643,6644,6645,6646,6647,6648,6649,6650,6651,6652,6653,6654,6655,6656,6657,6658,6659,6660,6661,6664,6666,6667,6668,6669,6670,6965,6966,6967,6968,6969,6970,6971,6972,6973,6974,6975,7064,7065,7066,7067,7068,7069,7070,7071,7072,7073,7074,7163,7164,7165,7166,7167,7168,7169,7170,7171,7172,7408,7409,7410,7411,7412,7413,7414,7415,7416,7417,7418,7419,7420,8818,8819,8820,8821,8822,8823,8824,8825,8826,8827,8828,9989,9990,9991,9992,9993,9994,9995,9996,9997,9998,9999,10065,10066,10067,10068,10069,10070,10071,10072,10073,10074,10075,10076,10077,10078,10079,10080,10081,10082,10083,10085,10086,10087,10088,10089,10090,10091,10092,10093,10094,10095,10096,10097,10098,10099,10100,10101,10102,10103,10104,10105,10106,10107,10108,10109,10111,10112,10113,10114,10115,10116,10340,10341,10342,10343,10344,10736,10737,10738,10739,10740,10741,10742,10743,10744,10745,10746,10747,10748,11896,11897,11898,11899,11900,11901,11902,11903,11904,11905,11906,11907,12168,12169,12170,12171,12172,12173,12174,12175,12176,12177,12178,12179,12180,12181,12182,12183,12184,12185,12186,12187,12188,12189,12190,12191,12192,12193,13339,13340,13341,13342,13343,13344,13345,13346,13347,13348,13349,13488,13489,13490,13491,13492,13493,13494,13495,13496,13497,13498,13499,13500,13501,13502,13506,13507,13510,13511,13512,13513,13514,13515,13516,13517,13518,13519,13520,13521,13522,13523,13524,13525,13526,13530,13531,13534,13535,15787,15788,15789,15790,15791,15792,15793,15794,15795,15796,15797,15798,15799,15800,15801,15802,15805,15808,15809,15810,15811,15812,15813,15814,15815,15816,15817,15818,15819,15820,15821,15822,15823,15824,15825,15826,15829,15832,15833,15834,15951,15952,16600,16601,16602,16606,16607,16608,16612,16613,16754,16755,16756,16757,16758,16759,16760,16761,16762,16763,16764,16765,16848,17490,17491,17492,17493,17494,17495,17496,17497,17498,17499,17500,17501,17505,17506,17507,17508,17509,17510,17511,17512,17513,17514,17515,17516,17517,17521,17548,17549,17550,17551,17552,17553,17554,17555,17556,17559,17560,17563,17564,17605,17608,17628,17630,17644,17645,17646,17647,17648,17649,17650,17654,17655,17656,17657,17658,17659,17660,17797,17798,17799,17800,17801,17802,17807,17808,17809,17810,17811,17812,17813,17820,17821,17824,17914,17915,17916,17917,17980,17981,17982,17983,17984,17985,17986,17987,17988,17989,17990,17991,17992,17993,17994,17995,17996,17997,17998,17999,18000,18001,18002,18003,18004,18005,18006,18007,18008,18009,18010,18011,18012,18013,18014,18015,18055,18056,18057,18058,18059,18060,18061,18062,18063,18064,18065,18066,18067,18068,18069,18070,18071,18072,18073,18074,18075,18076,18077,18078,18079,18080,18081,18082,18083,18084,18085,18086,18087,18088,18089,18090,18091,18092,18093,18094,18095,18096,18097,18098,18099,18100,18101,18102,18103,18104,18105,18106,18107,18108,18109,18110,18111,18112,18197,18198,18199,18200,18201,18202,18204,18205,18206,18207,18208,18209,18210,18211,18212,18213,18215,18216,18217,18218,18219,18220,18221,18222,18223,18224,18225,18226,18227,18228,18229,18239,18241,18242,18243,18244,18245,18246,18247,18248,18249,18250,18251,18311,18317,18318,18319,18320,18322,18324,18329,18330,18331,18332,18333,18334,18335,18336,18337,18338,18339,18340,18341,18345,18347,18349,18351,18352,18353,18354,18355,18356,18357,18358,18359,18360,18361,18407,18409,18414,18416,18428,18429,18430,18431,18432,18433,18434,18435,18436,18437,18438,18441,18450,18451,18452,18453,18454,18455,18456,18457,18458,18459,18460,18744,18745,18746,18747,18748,18749,18750,18751,18752,18753,18754,18755,18756,18783,18784,18785,18786,18787,18788,18789,18790,18791,18792,18793,18794,18795,19062,19063,19064,19065,19066,19067,19068,19069,19070,19071,19072,19073,19074,19075,19076,19077,19078,19082,19083,19084,19085,19086,19087,19088,19089,19090,19091,19092,19093,19094,19095,19096,19097,19098,19099,19100,19101,19102,19103,19104,19108,19109,19110,19111,19112,19113,19141,19142,19143,19144,19145,19146,19147,19148,19149,19150,19160,19161,19162,19163,19164,19165,19166,19167,19168,19169,19170,19189,19190,19227,19228,20075,20076,20077,20078,20079,20080,20081,20082,20083,20084,20085,20086,20087,20165,20166,20167,20168,20169,20170,20171,20172,20173,20174,20175,20176,20177,20241,20242,20243,20244,20245,20246,20247,20248,20249,20250,20251,20252,20757,20758,20759,20760,20761,20762,20763,20764,20765,20766,20767,20768,20769,23130,23131,23132,23133,23134,23135,23136,23137,23138,23139,23140,23141,23142,23272,23273,23274,23275,23276,23277,23278,23279,23280,23281,23282,23283,23284,23414,23415,23416,23417,23418,23419,23420,23421,23422,23423,23424,23425,23426,23556,23557,23558,23559,23560,23561,23562,23563,23564,23565,23566,23567,23568,23698,23699,23700,23701,23702,23703,23704,23705,23706,23707,23708,23709,23710,23814,23815,23816,23817,23818,23819,23820,23821,23822,23823,23824,23825,23826,23827,23828,23829,23830,23835,23836,23837,23838,23839,23840,23841,23842,23843,23844,23845,23846,23847,23848,23849,23850,23851,23852,23853,23854,23855,23856,23861,23862,23863,23864,23865,24683,24684,24685,24686,24687,24688,24746,24747,24748,24749,24750,24751,24752,24974,24975,24976,24977,24978,24979,24980,24981,24982,24983,24984,24985,24986,24995,25003,25153,25154,25155,25156,25157,25158,25159,25160,25161,25162,25163,25190,25191,25192,25193,25194,25195,25196,25197,25198,25199,25200,25434,25435,25436,25437,25438,25439,25440,25441,25442,25443,25444,25445,25446,25530,25531,25532,25533,25534,25535,25536,25537,25538,25539,25540,25575,25576,25577,25578,25579,25580,25581,25582,25583,25584,25585,25658,25659,25660,25661,25662,25663,25664,25665,25666,25667,25668,25693,25694,25695,25696,25697,25698,25699,25700,25701,25702,25703,25728,25729,25730,25731,25732,25733,25734,25735,25736,25737,25738,25772,25773,25774,25775,25776,25777,25778,25779,25780,25781,25782,25849,25850,25851,25852,25853,25854,25855,25856,25857,25858,25859,25882,25883,25884,25886,25887,25888,25889,25890,25891,25892,25915,25916,25917,25918,25919,25920,25921,25922,25923,25924,25925,25959,25960,25961,25962,25963,25964,25965,25966,25967,25968,25969,26036,26037,26038,26039,26040,26041,26042,26044,26045,26046,26069,26070,26071,26072,26074,26075,26076,26077,26078,26079,26413,26414,26415,26416,26417,26418,26419,26420,26421,26422,26423,26424,26425,26865,26866,26867,26868,26869,26870,26871,26872,26873,26874,26875,26876,26877,26914,26915,26916,26917,26918,26919,26920,26921,26922,26923,26924,26925,26926,26963,26964,26965,26966,26967,26968,26969,26970,26971,26972,26973,26974,26975,27012,27015,27017,27018,27019,27020,27022,27023,27024,27061,27066,27068,27069,27070,27071,27073,27115,27118,27119,27120,27122,27164,27167,27168,27169,27171,27213,27216,27217,27218,27265,27266,27314,27810,27811,27812,27813,27814,27815,27816,27817,27818,27819,27820,27821,27822,27823,27824,27825,27826,27827,27828,27829,27830,27831,27832,27833,27834,27835,27839,27842,27844,27845,27846,27847,27848,27859,27861,27872,30979,30980,30981,30982,30983,30984,30985,30986,30987,30988,30989,30990,30991,30992,30993,30994,30995,30996,30999,31000,31001,31002,31003,31004,31005,31006,31007,31008,31009,31010,31011,31012,31013,31014,31015,31016,31017,31018,31019,31020,31021,31022,31025,31026,31027,31028,31029,31030) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"data/training_data/PreFer_train_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "# read in data and prepare transformations\n",
    "data = pd.read_csv(\"data/training_data/PreFer_train_data.csv\")\n",
    "targets = pd.read_csv('data/training_data/PreFer_train_outcome.csv')\n",
    "codebook = pd.read_csv('data/codebooks/PreFer_codebook.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0077c81",
   "metadata": {},
   "source": [
    "### Select the top 10 most important questions (there's overlap, so there's only gonna be 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d39d171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.read_csv('features_importance_1000.csv')\n",
    "custom_pairs = importance.iloc[:50].feature.map(lambda x: get_generic_name(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d165d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomarx/Documents/GitHub/fertility-prediction-challenge/data_processing/pipeline.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  codebook[\"pairs\"] = codebook['var_name'].apply(get_generic_name)\n"
     ]
    }
   ],
   "source": [
    "# check if sequences have been preprocessed (saves time)\n",
    "if False:# os.path.exists('data/processed_data/sequences.pt'):\n",
    "    sequences = torch.load('data/processed_data/sequences.pt')\n",
    "else:\n",
    "    sequences = encoding_pipeline(data, codebook, custom_pairs=custom_pairs)\n",
    "    #torch.save(sequences, 'data/processed_data/sequences.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddb66e-cba5-4bb9-854d-811d49599b93",
   "metadata": {},
   "source": [
    "# Train the SIMPLE Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d9275bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dataset import PretrainingDataset\n",
    "pretrain_dataset = PretrainingDataset(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8544475-54af-4874-9e83-7f39193eb651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomarx/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/carlomarx/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### Initialization of the Autoencoder \n",
    "HIDDEN_DIM = 128\n",
    "#ENCODING_SIZE = 64\n",
    "BATCH_SIZE = 96\n",
    "num_epochs_autoencoder = 2\n",
    "learning_rate_autoencoder = 5e-2\n",
    "\n",
    "SEQ_LEN = pretrain_dataset.get_seq_len()\n",
    "vocab_size = pretrain_dataset.get_vocab_size()\n",
    "\n",
    "train_dataloader = DataLoader(pretrain_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "autoencoder = SimpleAutoEncoder(vocab_size=vocab_size, embedding_size=HIDDEN_DIM, sequence_len=SEQ_LEN).to(device)\n",
    "\n",
    "#loss_f1 = nn.HuberLoss(delta=1.0)\n",
    "loss_cls = nn.CrossEntropyLoss()\n",
    "#loss_cos = nn.CosineEmbeddingLoss()\n",
    "optimizer = optim.RAdam( autoencoder.parameters(), lr = learning_rate_autoencoder, weight_decay=1e-2, decoupled_weight_decay=True)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = num_epochs_autoencoder, eta_min = 1e-5, last_epoch = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9094f791",
   "metadata": {},
   "source": [
    "### (or) Train the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slows down the training but allows use to detect nan\n",
    "DETECT_ANOMALY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be77146-e5b0-4a06-9d9e-d651c88d32a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" INPUT MODEL NAME BELOW (FOR CHECKPOINTS)\"\"\"\n",
    "model_name = \"foo\"\n",
    "\n",
    "autoencoder.train()\n",
    "autoencoder.to(device)\n",
    "loss_metric = []\n",
    "all_loss = []   # for plotting\n",
    "######## ANNOMALY DETECTION\n",
    "torch.autograd.set_detect_anomaly(DETECT_ANOMALY)\n",
    "\n",
    "for epoch in range(num_epochs_autoencoder):\n",
    "    loss_epoch_metric = []\n",
    "    loop_object  = tqdm(enumerate(train_dataloader), desc=f\"Epochs {epoch}\")\n",
    "    for i, (year, seq) in loop_object :\n",
    "        optimizer.zero_grad()\n",
    "        year = year.to(device)\n",
    "        seq = seq.to(device)\n",
    "\n",
    "        x= autoencoder(year, seq)\n",
    "        loss = loss_cls(x.permute(0,2,1), seq.long()) #+ #+ loss_cos(x1.reshape(x1.size(1) * x1.size(0), -1 ), \n",
    "                                                       #         autoencoder.embedding(year, seq).view(x1.size(1) * x1.size(0), -1), \n",
    "                                                        #        torch.ones(seq.size(0) * seq.size(1)).to(device))\n",
    "         #+ 0.7 * loss_f1(x1, autoencoder.embedding(year, seq)) +  \n",
    "        loss_epoch_metric.append(loss.detach().cpu().numpy())\n",
    "        all_loss.append(loss_epoch_metric[-1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loop_object.set_postfix_str(\"mean loss: %.3f\"%np.mean(loss_epoch_metric[-100:]))\n",
    "    ## After epoch end\n",
    "    scheduler.step()\n",
    "    loss_metric.append(np.mean(loss_epoch_metric))\n",
    "    print(f'epoch {epoch} \\t Loss: {loss_metric[-1]:.4g} and LR: {scheduler.get_last_lr()[0]:.5g}')\n",
    "    torch.save(autoencoder.state_dict(), f'weights/{model_name}_{epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f78caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(all_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3968c76",
   "metadata": {},
   "source": [
    "### Explroe the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0778109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Embedding has 0 NaNs\n",
      "Year Embedding has 0 NaNs\n"
     ]
    }
   ],
   "source": [
    "### FAST CHECK\n",
    "print(\"Answer Embedding has %s NaNs\" %torch.isnan(autoencoder.embedding.answer_embedding.weight).sum().cpu().numpy())\n",
    "print(\"Year Embedding has %s NaNs\" %torch.isnan(autoencoder.embedding.yearly_embedding.weight).sum().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8118b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "w = autoencoder.embedding.answer_embedding.weight.detach().cpu().numpy()\n",
    "projector = umap.UMAP(n_components=2)\n",
    "wp = projector.fit_transform(w)\n",
    "plt.scatter(wp[:,0], wp[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada8018b-7ebc-4db4-aadf-191b214b1470",
   "metadata": {},
   "source": [
    "# Train the RNN\n",
    "\n",
    "First we need to create Dataset class that can hold both the target (stored in a pd.DataFrame) and the sequences.\n",
    "\n",
    "The sequences will be of dimension 14 x encoding_dimension, because we have 14 years of surveys.\n",
    "\n",
    "I have created some code for getting the data into the right format, but it might not be useful.\n",
    "\n",
    "## Regarding masks\n",
    "Right now the masking is done already in the encoding. I haven't found exactly where Mikkel implemented this.\n",
    "So for now, assume that nothing is padded, and then we'll figure it out with Mikkel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af1786da-d2b5-4e1f-81f6-1db7bca5515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# its not everyone we have a target for, so we do restrict the data to \n",
    "# the ones with known outcomes\n",
    "targets = targets[targets.new_child.notna()]\n",
    "train_person_ids, test_person_ids = train_test_split(targets['nomem_encr'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "754d624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_data = {person_id: (\n",
    "        torch.tensor([year-2007 for year, _ in wave_responses.items()]).to(device),\n",
    "        torch.tensor([ wave_response for _, wave_response in wave_responses.items()]).to(device)\n",
    "        )\n",
    "        for person_id, wave_responses in sequences.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eaadc01-1e81-4727-b7f4-f14823463fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data based on the splits made for the target\n",
    "train_data = {person_id: rnn_data[person_id] for person_id in train_person_ids}\n",
    "test_data = {person_id: rnn_data[person_id] for person_id in test_person_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d85edc9-8457-439f-8fe9-fbd94eb9b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dataset import FinetuningDataset\n",
    "train_dataset = FinetuningDataset(train_data, targets = targets)\n",
    "test_dataset = FinetuningDataset(test_data, targets = targets)\n",
    "\n",
    "rnn_batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=rnn_batch_size, shuffle=True)\n",
    "test_dataloader  = DataLoader(test_dataset,  batch_size=rnn_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caf49964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is going to set all input MASK to None\n",
      "The model is going to set all input MASK to None\n",
      "Ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomarx/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "# ft - fine-tuning\n",
    "\n",
    "HIDDEN_SIZE = 64\n",
    "ENCODING_SIZE = 32\n",
    "\n",
    "num_epochs_ft = 40\n",
    "learning_rate_ft = 5e-3\n",
    "\n",
    "aggregator = nn.Sequential(\n",
    "    nn.LazyInstanceNorm1d(),\n",
    "    nn.LazyLinear(ENCODING_SIZE),\n",
    "    nn.LazyBatchNorm1d()).to(device)\n",
    "\n",
    "######## WHAT we used initialy\n",
    "decoder = GRUDecoder(\n",
    "    input_size=ENCODING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    max_seq_len=14\n",
    ").to(device)\n",
    "\n",
    "#### Simple RNN\n",
    "decoder = decoder = GRUDecoder(\n",
    "    input_size=ENCODING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    max_seq_len=14,\n",
    "    dropout=0.01,\n",
    "    bidirectional= False, \n",
    "    num_layers=1,\n",
    "    with_attention= True,\n",
    "    xavier_initialization=True,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "##### SIMPLE DECODER with AVERGING\n",
    "#decoder = SimpleDecoder(input_size=ENCODING_SIZE, output_size=1).to(device)\n",
    "\n",
    "# Define loss function and optimizer for RNN\n",
    "ft_loss = torch.nn.BCELoss()\n",
    "ft_optimizer = torch.optim.RAdam(list(decoder.parameters()) + list(autoencoder.parameters()) + list(aggregator.parameters()) , lr=learning_rate_ft, weight_decay=1e-3, decoupled_weight_decay=True)\n",
    "ft_scheduler = optim.lr_scheduler.CosineAnnealingLR(ft_optimizer, T_max = num_epochs_ft, eta_min = 1e-6, last_epoch = -1)\n",
    "\n",
    "# Training loop\n",
    "decoder.train()\n",
    "autoencoder.train()\n",
    "aggregator.train()\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ffe77dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_COLS = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82cf1777-49e7-462d-ae51-549ea6c8305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 50it [00:05,  9.65it/s, mean loss: 0.458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 0.4584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 1: 50it [00:03, 14.74it/s, mean loss: 0.220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40, Loss: 0.2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 2: 15it [00:01, 14.04it/s, mean loss: 0.127]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     loop_object\u001b[38;5;241m.\u001b[39mset_postfix_str(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean loss: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(loss_per_step[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:]))\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#loss.backward(retain_graph=True)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     ft_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# On epoch end\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_per_epoch = []\n",
    "for epoch in range(num_epochs_ft):\n",
    "    # print(epoch)\n",
    "    loss_per_step = []\n",
    "    loop_object  = tqdm(enumerate(train_dataloader), desc=f\"Epochs {epoch}\")\n",
    "    for i, batch in loop_object :        \n",
    "        ft_optimizer.zero_grad() \n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "\n",
    "        input_year, input_seq = inputs\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings = autoencoder.get_encoding(input_year, input_seq).view(bs,ss, -1)\n",
    "        survey_emb = aggregator(encodings)\n",
    "        mask = ~((input_seq == 101).sum(-1) == NUM_COLS).view(bs,ss).detach()\n",
    "\n",
    "        # Forward pass\n",
    "        xx = decoder(survey_emb, mask)\n",
    "        outputs = torch.nn.functional.sigmoid(xx)\n",
    "\n",
    "        loss = ft_loss(torch.flatten(outputs), labels)  \n",
    "        loss_per_step.append(loss.detach().cpu().numpy())\n",
    "        loop_object.set_postfix_str(\"mean loss: %.3f\"%np.mean(loss_per_step[-100:]))\n",
    "\n",
    "        #loss.backward(retain_graph=True)\n",
    "        loss.backward()\n",
    "        ft_optimizer.step()\n",
    "    # On epoch end\n",
    "    loss_per_epoch.append(np.mean(loss_per_step))\n",
    "    ft_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_ft}, Loss: {loss_per_epoch[-1]:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29d5ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, z = decoder.gru(survey_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5701b4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 14, 64])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35208203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "649bc948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, average_precision_score\n",
    "\n",
    "val_loss = []\n",
    "preds = []\n",
    "targets = []\n",
    "\n",
    "## Set both models into the eval mode.=\n",
    "rnn_model.eval()\n",
    "autoencoder.eval()\n",
    "for batch in test_dataloader:\n",
    "    inputs, labels = batch\n",
    "    labels = labels.to(torch.float).to(device)\n",
    "\n",
    "    input_year, input_seq = inputs\n",
    "    bs, ss = labels.size(0), 14\n",
    "    input_year = input_year.reshape(-1).to(device)\n",
    "    input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "    encodings = autoencoder.get_encoding(input_year, input_seq).view(bs,ss, -1)\n",
    "    survey_emb = aggregator(encodings)\n",
    "\n",
    "\n",
    "    # Forward pass\n",
    "    xx = rnn_model(survey_emb)\n",
    "    outputs = torch.nn.functional.sigmoid(xx).flatten()\n",
    "    loss = ft_loss(outputs, labels)  \n",
    "    val_loss.append(loss.detach().cpu().numpy())\n",
    "    preds.extend(outputs.detach().cpu().numpy().tolist())\n",
    "    targets.extend(labels.cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fbdb333f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64])\n"
     ]
    }
   ],
   "source": [
    "x = survey_emb\n",
    "denom = torch.sum(mask, -1, keepdim=True)\n",
    "x = torch.div(torch.sum(x * mask.unsqueeze(-1), dim=1), denom)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2687279a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3091, -0.2374,  0.1403,  ...,  0.0838, -0.6693, -0.2271],\n",
       "         [-0.3071, -0.2357,  0.1400,  ...,  0.0838, -0.6653, -0.2254],\n",
       "         [-0.3030, -0.2313,  0.1479,  ...,  0.0913, -0.6647, -0.2209],\n",
       "         ...,\n",
       "         [-0.3321, -0.2307,  0.3069,  ...,  0.2265, -0.8448, -0.2158],\n",
       "         [-0.3961, -0.2505,  0.5156,  ...,  0.4015, -1.1266, -0.2297],\n",
       "         [-0.3248, -0.2470,  0.1643,  ...,  0.1028, -0.7170, -0.2360]],\n",
       "\n",
       "        [[-0.3091, -0.2374,  0.1403,  ...,  0.0838, -0.6693, -0.2271],\n",
       "         [-0.3070, -0.2358,  0.1400,  ...,  0.0838, -0.6653, -0.2254],\n",
       "         [-0.3030, -0.2313,  0.1480,  ...,  0.0913, -0.6647, -0.2209],\n",
       "         ...,\n",
       "         [-0.3321, -0.2306,  0.3069,  ...,  0.2265, -0.8447, -0.2157],\n",
       "         [-0.3961, -0.2505,  0.5156,  ...,  0.4016, -1.1266, -0.2297],\n",
       "         [-0.3248, -0.2470,  0.1643,  ...,  0.1028, -0.7170, -0.2360]],\n",
       "\n",
       "        [[-0.3091, -0.2374,  0.1403,  ...,  0.0838, -0.6693, -0.2271],\n",
       "         [-0.3070, -0.2357,  0.1400,  ...,  0.0838, -0.6653, -0.2255],\n",
       "         [-0.3030, -0.2312,  0.1479,  ...,  0.0912, -0.6646, -0.2209],\n",
       "         ...,\n",
       "         [-0.3321, -0.2308,  0.3070,  ...,  0.2265, -0.8448, -0.2157],\n",
       "         [-0.3961, -0.2506,  0.5156,  ...,  0.4015, -1.1266, -0.2297],\n",
       "         [-0.3248, -0.2470,  0.1643,  ...,  0.1028, -0.7170, -0.2360]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.3091, -0.2374,  0.1403,  ...,  0.0838, -0.6693, -0.2271],\n",
       "         [-0.3070, -0.2358,  0.1400,  ...,  0.0838, -0.6653, -0.2254],\n",
       "         [-0.3030, -0.2313,  0.1480,  ...,  0.0913, -0.6647, -0.2209],\n",
       "         ...,\n",
       "         [-0.3321, -0.2306,  0.3069,  ...,  0.2266, -0.8448, -0.2158],\n",
       "         [-0.3960, -0.2505,  0.5156,  ...,  0.4017, -1.1267, -0.2296],\n",
       "         [-0.3248, -0.2470,  0.1643,  ...,  0.1028, -0.7170, -0.2360]],\n",
       "\n",
       "        [[-0.3091, -0.2373,  0.1403,  ...,  0.0837, -0.6691, -0.2270],\n",
       "         [-0.3071, -0.2357,  0.1401,  ...,  0.0838, -0.6652, -0.2254],\n",
       "         [-0.3030, -0.2312,  0.1479,  ...,  0.0912, -0.6646, -0.2208],\n",
       "         ...,\n",
       "         [-0.3320, -0.2308,  0.3069,  ...,  0.2265, -0.8448, -0.2157],\n",
       "         [-0.3961, -0.2504,  0.5156,  ...,  0.4016, -1.1266, -0.2297],\n",
       "         [-0.3248, -0.2470,  0.1643,  ...,  0.1028, -0.7170, -0.2360]],\n",
       "\n",
       "        [[-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000],\n",
       "         ...,\n",
       "         [-0.3321, -0.2307,  0.3069,  ...,  0.2265, -0.8448, -0.2157],\n",
       "         [-0.3961, -0.2505,  0.5155,  ...,  0.4017, -1.1267, -0.2296],\n",
       "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0000]]],\n",
       "       device='mps:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_emb * mask.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7edf0584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "        101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "        101, 101], device='mps:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e33b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "denom = torch.sum(mask, -1, keepdim=True)\n",
    "x = torch.sum(x, dim=1) / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0d2fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.1364\n",
      "Recall: 0.0600\n",
      "F1 Score: 0.0833\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all the batches\n",
    "predictions = (torch.tensor(preds) > 0.5).float()\n",
    "actuals = torch.tensor(targets).flatten()\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(actuals.cpu().numpy(), predictions.cpu().numpy(), average='binary')\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c675d413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  0]],\n",
       "         device='mps:0'),\n",
       "  tensor([[[101, 101, 101,  ..., 101, 101, 101],\n",
       "           [101, 101, 101,  ..., 101, 101, 101],\n",
       "           [101, 101, 101,  ..., 101, 101, 101],\n",
       "           ...,\n",
       "           [101, 101, 101,  ..., 101, 101, 101],\n",
       "           [  0,  27,  34,  ..., 171,  76,  73],\n",
       "           [101, 101, 101,  ..., 101, 101, 101]],\n",
       "  \n",
       "          [[101, 101, 101,  ..., 101, 101, 101],\n",
       "           [101, 101, 101,  ..., 101, 101, 101],\n",
       "           [101, 101, 101,  ..., 101, 101, 101],\n",
       "           ...,\n",
       "           [  0,  72, 101,  ..., 177,  30,  32],\n",
       "           [  0, 101, 101,  ..., 177,  91,  97],\n",
       "           [101, 101, 101,  ..., 101, 101, 101]],\n",
       "  \n",
       "          [[101, 101, 101,  ..., 101, 101, 101],\n",
       "           [101, 101, 101,  ..., 101, 101, 101],\n",
       "           [101, 101, 101,  ..., 101, 101, 101],\n",
       "           ...,\n",
       "           [101, 101, 101,  ..., 177,  24, 101],\n",
       "           [  0, 101, 101,  ..., 177,   0,   0],\n",
       "           [101, 101, 101,  ..., 101, 101, 101]],\n",
       "  \n",
       "          [[101, 101, 101,  ..., 101, 101, 101],\n",
       "           [101, 101, 101,  ..., 101, 101, 101],\n",
       "           [101, 101, 101,  ..., 101, 101, 101],\n",
       "           ...,\n",
       "           [  0, 101,   3,  ..., 177,  86,  90],\n",
       "           [  0, 101,   3,  ..., 177,  83,  88],\n",
       "           [101, 101, 101,  ..., 101, 101, 101]],\n",
       "  \n",
       "          [[101, 101, 101,  ..., 101, 101, 101],\n",
       "           [101, 101, 101,  ..., 101, 101, 101],\n",
       "           [101, 101, 101,  ..., 101, 101, 101],\n",
       "           ...,\n",
       "           [101, 101, 101,  ..., 170,  60,  59],\n",
       "           [  0, 101, 101,  ..., 170,  56,  53],\n",
       "           [101, 101, 101,  ..., 101, 101, 101]],\n",
       "  \n",
       "          [[101, 101, 101,  ..., 181,   0,   0],\n",
       "           [101, 101, 101,  ..., 181,   0,   0],\n",
       "           [101, 101, 101,  ..., 181,   0,   0],\n",
       "           ...,\n",
       "           [  0, 101, 101,  ..., 170,  48,  55],\n",
       "           [  0, 101, 101,  ..., 170,  50,  53],\n",
       "           [101, 101, 101,  ..., 181, 101, 101]]], device='mps:0')],\n",
       " tensor([0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207f1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
