{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9914262-7a3e-4e3a-9e07-043d40efd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data packages\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, average_precision_score\n",
    "from model.rnn import GRUDecoder\n",
    "from model.autoencoder import SimpleAutoEncoder\n",
    "from data_processing.pipeline import encoding_pipeline, get_generic_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18aee1dd-7884-40e7-be05-db070f1a452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders) device\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    # Check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        # If CUDA is available, select the first CUDA device\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(\"Using CUDA device:\", torch.cuda.get_device_name(0))\n",
    "    # Check for MPS availability on supported macOS devices (requires PyTorch 1.12 or newer)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        # If MPS is available, use MPS device\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS (Metal Performance Shaders) device\")\n",
    "    else:\n",
    "        # Fallback to CPU if neither CUDA nor MPS is available\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b651aa-752a-4c71-990a-332ff4099791",
   "metadata": {},
   "source": [
    "# Read the data\n",
    "\n",
    "Right now the notebook is set to work with fake data. This can be changed once the pipeline works.\n",
    "\n",
    "The data is stored as a Dict[person_id, Sequences] where Sequences is a Dict[year, survery_wave_response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c3f871-21e8-418c-9aa7-31ad09283a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y6/j9fbqcvx6lb5l99614n30y4c0000gn/T/ipykernel_33218/1529815470.py:2: DtypeWarning: Columns (2583,2584,2585,2586,2587,2588,2589,4358,4359,4360,4361,4362,4363,4364,4365,4366,4367,4368,4369,4370,4371,4372,4373,4374,4375,4379,4380,4381,4382,4383,4384,4385,4386,4387,4388,4389,4390,4391,4392,4393,4394,4395,4396,4397,4398,4399,4400,4401,4405,4406,4407,4408,4409,5215,5216,5219,5220,5613,5614,5615,5616,5617,5618,5619,5620,5621,5622,5624,5625,5626,5627,5628,5629,5630,5631,5632,5633,5634,5635,5636,5638,5639,5640,5787,5788,5789,5790,5791,5792,5793,5794,5795,5796,6393,6394,6395,6396,6397,6398,6399,6400,6401,6402,6403,6619,6620,6621,6622,6623,6624,6625,6626,6627,6628,6629,6630,6631,6632,6633,6634,6635,6638,6640,6641,6642,6643,6644,6645,6646,6647,6648,6649,6650,6651,6652,6653,6654,6655,6656,6657,6658,6659,6660,6661,6664,6666,6667,6668,6669,6670,6965,6966,6967,6968,6969,6970,6971,6972,6973,6974,6975,7064,7065,7066,7067,7068,7069,7070,7071,7072,7073,7074,7163,7164,7165,7166,7167,7168,7169,7170,7171,7172,7408,7409,7410,7411,7412,7413,7414,7415,7416,7417,7418,7419,7420,8818,8819,8820,8821,8822,8823,8824,8825,8826,8827,8828,9989,9990,9991,9992,9993,9994,9995,9996,9997,9998,9999,10065,10066,10067,10068,10069,10070,10071,10072,10073,10074,10075,10076,10077,10078,10079,10080,10081,10082,10083,10085,10086,10087,10088,10089,10090,10091,10092,10093,10094,10095,10096,10097,10098,10099,10100,10101,10102,10103,10104,10105,10106,10107,10108,10109,10111,10112,10113,10114,10115,10116,10340,10341,10342,10343,10344,10736,10737,10738,10739,10740,10741,10742,10743,10744,10745,10746,10747,10748,11896,11897,11898,11899,11900,11901,11902,11903,11904,11905,11906,11907,12168,12169,12170,12171,12172,12173,12174,12175,12176,12177,12178,12179,12180,12181,12182,12183,12184,12185,12186,12187,12188,12189,12190,12191,12192,12193,13339,13340,13341,13342,13343,13344,13345,13346,13347,13348,13349,13488,13489,13490,13491,13492,13493,13494,13495,13496,13497,13498,13499,13500,13501,13502,13506,13507,13510,13511,13512,13513,13514,13515,13516,13517,13518,13519,13520,13521,13522,13523,13524,13525,13526,13530,13531,13534,13535,15787,15788,15789,15790,15791,15792,15793,15794,15795,15796,15797,15798,15799,15800,15801,15802,15805,15808,15809,15810,15811,15812,15813,15814,15815,15816,15817,15818,15819,15820,15821,15822,15823,15824,15825,15826,15829,15832,15833,15834,15951,15952,16600,16601,16602,16606,16607,16608,16612,16613,16754,16755,16756,16757,16758,16759,16760,16761,16762,16763,16764,16765,16848,17490,17491,17492,17493,17494,17495,17496,17497,17498,17499,17500,17501,17505,17506,17507,17508,17509,17510,17511,17512,17513,17514,17515,17516,17517,17521,17548,17549,17550,17551,17552,17553,17554,17555,17556,17559,17560,17563,17564,17605,17608,17628,17630,17644,17645,17646,17647,17648,17649,17650,17654,17655,17656,17657,17658,17659,17660,17797,17798,17799,17800,17801,17802,17807,17808,17809,17810,17811,17812,17813,17820,17821,17824,17914,17915,17916,17917,17980,17981,17982,17983,17984,17985,17986,17987,17988,17989,17990,17991,17992,17993,17994,17995,17996,17997,17998,17999,18000,18001,18002,18003,18004,18005,18006,18007,18008,18009,18010,18011,18012,18013,18014,18015,18055,18056,18057,18058,18059,18060,18061,18062,18063,18064,18065,18066,18067,18068,18069,18070,18071,18072,18073,18074,18075,18076,18077,18078,18079,18080,18081,18082,18083,18084,18085,18086,18087,18088,18089,18090,18091,18092,18093,18094,18095,18096,18097,18098,18099,18100,18101,18102,18103,18104,18105,18106,18107,18108,18109,18110,18111,18112,18197,18198,18199,18200,18201,18202,18204,18205,18206,18207,18208,18209,18210,18211,18212,18213,18215,18216,18217,18218,18219,18220,18221,18222,18223,18224,18225,18226,18227,18228,18229,18239,18241,18242,18243,18244,18245,18246,18247,18248,18249,18250,18251,18311,18317,18318,18319,18320,18322,18324,18329,18330,18331,18332,18333,18334,18335,18336,18337,18338,18339,18340,18341,18345,18347,18349,18351,18352,18353,18354,18355,18356,18357,18358,18359,18360,18361,18407,18409,18414,18416,18428,18429,18430,18431,18432,18433,18434,18435,18436,18437,18438,18441,18450,18451,18452,18453,18454,18455,18456,18457,18458,18459,18460,18744,18745,18746,18747,18748,18749,18750,18751,18752,18753,18754,18755,18756,18783,18784,18785,18786,18787,18788,18789,18790,18791,18792,18793,18794,18795,19062,19063,19064,19065,19066,19067,19068,19069,19070,19071,19072,19073,19074,19075,19076,19077,19078,19082,19083,19084,19085,19086,19087,19088,19089,19090,19091,19092,19093,19094,19095,19096,19097,19098,19099,19100,19101,19102,19103,19104,19108,19109,19110,19111,19112,19113,19141,19142,19143,19144,19145,19146,19147,19148,19149,19150,19160,19161,19162,19163,19164,19165,19166,19167,19168,19169,19170,19189,19190,19227,19228,20075,20076,20077,20078,20079,20080,20081,20082,20083,20084,20085,20086,20087,20165,20166,20167,20168,20169,20170,20171,20172,20173,20174,20175,20176,20177,20241,20242,20243,20244,20245,20246,20247,20248,20249,20250,20251,20252,20757,20758,20759,20760,20761,20762,20763,20764,20765,20766,20767,20768,20769,23130,23131,23132,23133,23134,23135,23136,23137,23138,23139,23140,23141,23142,23272,23273,23274,23275,23276,23277,23278,23279,23280,23281,23282,23283,23284,23414,23415,23416,23417,23418,23419,23420,23421,23422,23423,23424,23425,23426,23556,23557,23558,23559,23560,23561,23562,23563,23564,23565,23566,23567,23568,23698,23699,23700,23701,23702,23703,23704,23705,23706,23707,23708,23709,23710,23814,23815,23816,23817,23818,23819,23820,23821,23822,23823,23824,23825,23826,23827,23828,23829,23830,23835,23836,23837,23838,23839,23840,23841,23842,23843,23844,23845,23846,23847,23848,23849,23850,23851,23852,23853,23854,23855,23856,23861,23862,23863,23864,23865,24683,24684,24685,24686,24687,24688,24746,24747,24748,24749,24750,24751,24752,24974,24975,24976,24977,24978,24979,24980,24981,24982,24983,24984,24985,24986,24995,25003,25153,25154,25155,25156,25157,25158,25159,25160,25161,25162,25163,25190,25191,25192,25193,25194,25195,25196,25197,25198,25199,25200,25434,25435,25436,25437,25438,25439,25440,25441,25442,25443,25444,25445,25446,25530,25531,25532,25533,25534,25535,25536,25537,25538,25539,25540,25575,25576,25577,25578,25579,25580,25581,25582,25583,25584,25585,25658,25659,25660,25661,25662,25663,25664,25665,25666,25667,25668,25693,25694,25695,25696,25697,25698,25699,25700,25701,25702,25703,25728,25729,25730,25731,25732,25733,25734,25735,25736,25737,25738,25772,25773,25774,25775,25776,25777,25778,25779,25780,25781,25782,25849,25850,25851,25852,25853,25854,25855,25856,25857,25858,25859,25882,25883,25884,25886,25887,25888,25889,25890,25891,25892,25915,25916,25917,25918,25919,25920,25921,25922,25923,25924,25925,25959,25960,25961,25962,25963,25964,25965,25966,25967,25968,25969,26036,26037,26038,26039,26040,26041,26042,26044,26045,26046,26069,26070,26071,26072,26074,26075,26076,26077,26078,26079,26413,26414,26415,26416,26417,26418,26419,26420,26421,26422,26423,26424,26425,26865,26866,26867,26868,26869,26870,26871,26872,26873,26874,26875,26876,26877,26914,26915,26916,26917,26918,26919,26920,26921,26922,26923,26924,26925,26926,26963,26964,26965,26966,26967,26968,26969,26970,26971,26972,26973,26974,26975,27012,27015,27017,27018,27019,27020,27022,27023,27024,27061,27066,27068,27069,27070,27071,27073,27115,27118,27119,27120,27122,27164,27167,27168,27169,27171,27213,27216,27217,27218,27265,27266,27314,27810,27811,27812,27813,27814,27815,27816,27817,27818,27819,27820,27821,27822,27823,27824,27825,27826,27827,27828,27829,27830,27831,27832,27833,27834,27835,27839,27842,27844,27845,27846,27847,27848,27859,27861,27872,30979,30980,30981,30982,30983,30984,30985,30986,30987,30988,30989,30990,30991,30992,30993,30994,30995,30996,30999,31000,31001,31002,31003,31004,31005,31006,31007,31008,31009,31010,31011,31012,31013,31014,31015,31016,31017,31018,31019,31020,31021,31022,31025,31026,31027,31028,31029,31030) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"data/training_data/PreFer_train_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "# read in data and prepare transformations\n",
    "data = pd.read_csv(\"data/training_data/PreFer_train_data.csv\", low_memory=False)\n",
    "targets = pd.read_csv('data/training_data/PreFer_train_outcome.csv')\n",
    "codebook = pd.read_csv('data/codebooks/PreFer_codebook.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0077c81",
   "metadata": {},
   "source": [
    "### Select the top 10 most important questions (there's overlap, so there's only gonna be 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d39d171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.read_csv('features_importance_1000.csv')\n",
    "custom_pairs = importance.iloc[:200].feature.map(lambda x: get_generic_name(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d165d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lmmi/fertility-prediction-challenge/data_processing/pipeline.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  codebook[\"pairs\"] = codebook['var_name'].apply(get_generic_name)\n"
     ]
    }
   ],
   "source": [
    "# check if sequences have been preprocessed (saves time)\n",
    "if False:# os.path.exists('data/processed_data/sequences.pt'):\n",
    "    sequences = torch.load('data/processed_data/sequences.pt')\n",
    "else:\n",
    "    sequences = encoding_pipeline(data, codebook, custom_pairs=custom_pairs)\n",
    "    #torch.save(sequences, 'data/processed_data/sequences.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddb66e-cba5-4bb9-854d-811d49599b93",
   "metadata": {},
   "source": [
    "# Train the SIMPLE Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d9275bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dataset import PretrainingDataset\n",
    "pretrain_dataset = PretrainingDataset(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8544475-54af-4874-9e83-7f39193eb651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/eyra-rank/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### Initialization of the Autoencoder \n",
    "HIDDEN_DIM = 128\n",
    "#ENCODING_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "num_epochs_autoencoder = 3\n",
    "learning_rate_autoencoder = 10e-2\n",
    "\n",
    "SEQ_LEN = pretrain_dataset.get_seq_len()\n",
    "vocab_size = pretrain_dataset.get_vocab_size()\n",
    "\n",
    "train_dataloader = DataLoader(pretrain_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "autoencoder = SimpleAutoEncoder(vocab_size=vocab_size, embedding_size=HIDDEN_DIM, sequence_len=SEQ_LEN).to(device)\n",
    "\n",
    "#loss_f1 = nn.HuberLoss(delta=1.0)\n",
    "loss_cls = nn.CrossEntropyLoss()\n",
    "#loss_cos = nn.CosineEmbeddingLoss()\n",
    "optimizer = optim.RAdam( autoencoder.parameters(), lr = learning_rate_autoencoder, weight_decay=1e-2, decoupled_weight_decay=True)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = num_epochs_autoencoder, eta_min = 1e-5, last_epoch = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9094f791",
   "metadata": {},
   "source": [
    "### (or) Train the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f3d49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slows down the training but allows use to detect nan\n",
    "DETECT_ANOMALY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8be77146-e5b0-4a06-9d9e-d651c88d32a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 122it [00:28,  4.26it/s, mean loss: 0.21123]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (year, seq) \u001b[38;5;129;01min\u001b[39;00m loop_object :\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m     year \u001b[38;5;241m=\u001b[39m \u001b[43myear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     seq \u001b[38;5;241m=\u001b[39m seq\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m     nan_index \u001b[38;5;241m=\u001b[39m seq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m101\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" INPUT MODEL NAME BELOW (FOR CHECKPOINTS)\"\"\"\n",
    "model_name = \"foo\"\n",
    "\n",
    "autoencoder.train()\n",
    "autoencoder.to(device)\n",
    "loss_metric = []\n",
    "all_loss = []   # for plotting\n",
    "######## ANNOMALY DETECTION\n",
    "torch.autograd.set_detect_anomaly(DETECT_ANOMALY)\n",
    "\n",
    "for epoch in range(num_epochs_autoencoder):\n",
    "    loss_epoch_metric = []\n",
    "    loop_object  = tqdm(enumerate(train_dataloader), desc=f\"Epochs {epoch}\")\n",
    "    for i, (year, seq) in loop_object :\n",
    "        optimizer.zero_grad()\n",
    "        year = year.to(device)\n",
    "        seq = seq.to(device)\n",
    "        \n",
    "        nan_index = seq == 101\n",
    "\n",
    "        x = autoencoder(year, seq)\n",
    "        loss = loss_cls(x.permute(0,2,1), seq.long()) #+ #+ loss_cos(x1.reshape(x1.size(1) * x1.size(0), -1 ), \n",
    "                                                       #         autoencoder.embedding(year, seq).view(x1.size(1) * x1.size(0), -1), \n",
    "                                                        #        torch.ones(seq.size(0) * seq.size(1)).to(device))\n",
    "         #+ 0.7 * loss_f1(x1, autoencoder.embedding(year, seq)) +  \n",
    "        loss_epoch_metric.append(loss.detach().cpu().numpy())\n",
    "        all_loss.append(loss_epoch_metric[-1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loop_object.set_postfix_str(\"mean loss: %.5f\"%np.mean(loss_epoch_metric[-100:]))\n",
    "    ## After epoch end\n",
    "    scheduler.step()\n",
    "    loss_metric.append(np.mean(loss_epoch_metric))\n",
    "    print(f'epoch {epoch} \\t Loss: {loss_metric[-1]:.4g} and LR: {scheduler.get_last_lr()[0]:.5g}')\n",
    "    torch.save(autoencoder.state_dict(), f'weights/{model_name}_{epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f78caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_loss)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3968c76",
   "metadata": {},
   "source": [
    "### Explroe the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0778109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Embedding has 0 NaNs\n",
      "Year Embedding has 0 NaNs\n"
     ]
    }
   ],
   "source": [
    "### FAST CHECK\n",
    "print(\"Answer Embedding has %s NaNs\" %torch.isnan(autoencoder.embedding.answer_embedding.weight).sum().cpu().numpy())\n",
    "print(\"Year Embedding has %s NaNs\" %torch.isnan(autoencoder.embedding.yearly_embedding.weight).sum().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8118b33",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#import umap\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mumap_\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m w \u001b[38;5;241m=\u001b[39m autoencoder\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39manswer_embedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eyra-rank/lib/python3.11/site-packages/umap/umap_.py:41\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     36\u001b[0m     submatrix,\n\u001b[1;32m     37\u001b[0m     ts,\n\u001b[1;32m     38\u001b[0m     csr_unique,\n\u001b[1;32m     39\u001b[0m     fast_knn_indices,\n\u001b[1;32m     40\u001b[0m )\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspectral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spectral_layout, tswspectral_layout\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayouts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     43\u001b[0m     optimize_layout_euclidean,\n\u001b[1;32m     44\u001b[0m     optimize_layout_generic,\n\u001b[1;32m     45\u001b[0m     optimize_layout_inverse,\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpynndescent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NNDescent\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eyra-rank/lib/python3.11/site-packages/umap/spectral.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsgraph\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TruncatedSVD\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpectralEmbedding\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pairwise_distances\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _VALID_METRICS \u001b[38;5;28;01mas\u001b[39;00m SKLEARN_PAIRWISE_VALID_METRICS\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eyra-rank/lib/python3.11/site-packages/sklearn/manifold/__init__.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_mds\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MDS, smacof\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_spectral_embedding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpectralEmbedding, spectral_embedding\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_t_sne\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE, trustworthiness\n\u001b[1;32m     11\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocally_linear_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocallyLinearEmbedding\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrustworthiness\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eyra-rank/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:35\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _num_samples, check_non_negative\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# mypy error: Module 'sklearn.manifold' has no attribute '_utils'\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# mypy error: Module 'sklearn.manifold' has no attribute '_barnes_hut_tsne'\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _barnes_hut_tsne, _utils  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     37\u001b[0m MACHINE_EPSILON \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(np\u001b[38;5;241m.\u001b[39mdouble)\u001b[38;5;241m.\u001b[39meps\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_joint_probabilities\u001b[39m(distances, desired_perplexity, verbose):\n",
      "File \u001b[0;32msklearn/manifold/_barnes_hut_tsne.pyx:1\u001b[0m, in \u001b[0;36minit sklearn.manifold._barnes_hut_tsne\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msklearn/neighbors/_quad_tree.pyx:1\u001b[0m, in \u001b[0;36minit sklearn.neighbors._quad_tree\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eyra-rank/lib/python3.11/site-packages/sklearn/tree/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`sklearn.tree` module includes decision tree-based models for\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mclassification and regression.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     BaseDecisionTree,\n\u001b[1;32m      8\u001b[0m     DecisionTreeClassifier,\n\u001b[1;32m      9\u001b[0m     DecisionTreeRegressor,\n\u001b[1;32m     10\u001b[0m     ExtraTreeClassifier,\n\u001b[1;32m     11\u001b[0m     ExtraTreeRegressor,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m export_graphviz, export_text, plot_tree\n\u001b[1;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseDecisionTree\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecisionTreeClassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport_text\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/eyra-rank/lib/python3.11/site-packages/sklearn/tree/_classes.py:44\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     39\u001b[0m     _assert_all_finite_element_wise,\n\u001b[1;32m     40\u001b[0m     _check_sample_weight,\n\u001b[1;32m     41\u001b[0m     assert_all_finite,\n\u001b[1;32m     42\u001b[0m     check_is_fitted,\n\u001b[1;32m     43\u001b[0m )\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _criterion, _splitter, _tree\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_criterion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Criterion\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Splitter\n",
      "File \u001b[0;32msklearn/tree/_criterion.pyx:1\u001b[0m, in \u001b[0;36minit sklearn.tree._criterion\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msklearn/tree/_splitter.pyx:1\u001b[0m, in \u001b[0;36minit sklearn.tree._splitter\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:405\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import umap\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "w = autoencoder.embedding.answer_embedding.weight.detach().cpu().numpy()\n",
    "projector = umap.UMAP(n_components=2)\n",
    "wp = projector.fit_transform(w)\n",
    "plt.scatter(wp[:,0], wp[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada8018b-7ebc-4db4-aadf-191b214b1470",
   "metadata": {},
   "source": [
    "# Train the RNN\n",
    "\n",
    "First we need to create Dataset class that can hold both the target (stored in a pd.DataFrame) and the sequences.\n",
    "\n",
    "The sequences will be of dimension 14 x encoding_dimension, because we have 14 years of surveys.\n",
    "\n",
    "I have created some code for getting the data into the right format, but it might not be useful.\n",
    "\n",
    "## Regarding masks\n",
    "Right now the masking is done already in the encoding. I haven't found exactly where Mikkel implemented this.\n",
    "So for now, assume that nothing is padded, and then we'll figure it out with Mikkel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af1786da-d2b5-4e1f-81f6-1db7bca5515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# its not everyone we have a target for, so we do restrict the data to \n",
    "# the ones with known outcomes\n",
    "targets = targets[targets.new_child.notna()]\n",
    "train_person_ids, test_person_ids = train_test_split(targets['nomem_encr'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "754d624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_data = {person_id: (\n",
    "        torch.tensor([year-2007 for year, _ in wave_responses.items()]).to(device),\n",
    "        torch.tensor([ wave_response for _, wave_response in wave_responses.items()]).to(device)\n",
    "        )\n",
    "        for person_id, wave_responses in sequences.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3eaadc01-1e81-4727-b7f4-f14823463fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data based on the splits made for the target\n",
    "train_data = {person_id: rnn_data[person_id] for person_id in train_person_ids}\n",
    "test_data = {person_id: rnn_data[person_id] for person_id in test_person_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d85edc9-8457-439f-8fe9-fbd94eb9b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dataset import FinetuningDataset\n",
    "train_dataset = FinetuningDataset(train_data, targets = targets)\n",
    "test_dataset = FinetuningDataset(test_data, targets = targets)\n",
    "\n",
    "rnn_batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=rnn_batch_size, shuffle=True)\n",
    "test_dataloader  = DataLoader(test_dataset,  batch_size=rnn_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caf49964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "# ft - fine-tuning\n",
    "ENCODING_SIZE = 64\n",
    "HIDDEN_SIZE = 64\n",
    "\n",
    "num_epochs_ft = 10\n",
    "learning_rate_ft = 5e-3\n",
    "\n",
    "aggregator = nn.Sequential(\n",
    "    nn.LazyInstanceNorm1d(),\n",
    "    nn.LazyLinear(ENCODING_SIZE),\n",
    "    nn.LazyBatchNorm1d()).to(device)\n",
    "\n",
    "######## WHAT we used initialy\n",
    "#decoder = GRUDecoder(\n",
    "#    input_size=ENCODING_SIZE,\n",
    "#    hidden_size=HIDDEN_SIZE,\n",
    "#    max_seq_len=14\n",
    "#).to(device)\n",
    "\n",
    "#### Simple RNN\n",
    "decoder = GRUDecoder(\n",
    "    input_size=ENCODING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    max_seq_len=14,\n",
    "    dropout=0.15,\n",
    "    bidirectional= False, \n",
    "    num_layers=2,\n",
    "    with_attention= True\n",
    ").to(device)\n",
    "\n",
    "\n",
    "##### SIMPLE DECODER with AVERGING (no reccurences or attention)\n",
    "#decoder = SimpleDecoder(input_size=ENCODING_SIZE, output_size=1).to(device)\n",
    "\n",
    "# Define loss function and optimizer for RNN\n",
    "ft_loss = torch.nn.BCELoss()\n",
    "ft_optimizer = torch.optim.RAdam(list(decoder.parameters()) + list(autoencoder.parameters()) + list(aggregator.parameters()) , \n",
    "                                                                   lr=learning_rate_ft, \n",
    "                                                                   weight_decay=1e-3, \n",
    "                                                                   decoupled_weight_decay=True)\n",
    "ft_scheduler = optim.lr_scheduler.CosineAnnealingLR(ft_optimizer, T_max = num_epochs_ft, eta_min = 1e-6, last_epoch = -1)\n",
    "\n",
    "# Training loop\n",
    "decoder.train()\n",
    "autoencoder.train()\n",
    "aggregator.train()\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ffe77dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_COLS = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82cf1777-49e7-462d-ae51-549ea6c8305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[224, -1]' is invalid for input of size 1664",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m bs, ss \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m14\u001b[39m\n\u001b[1;32m     13\u001b[0m input_year \u001b[38;5;241m=\u001b[39m input_year\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 14\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m \u001b[43minput_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m encodings \u001b[38;5;241m=\u001b[39m autoencoder\u001b[38;5;241m.\u001b[39mget_encoding(input_year, input_seq)\u001b[38;5;241m.\u001b[39mview(bs,ss, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m encodings \u001b[38;5;241m=\u001b[39m aggregator(encodings) \u001b[38;5;66;03m# make sure that the number of dimensions is aligned after the Simple Decoder\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[224, -1]' is invalid for input of size 1664"
     ]
    }
   ],
   "source": [
    "loss_per_epoch = []\n",
    "for epoch in range(num_epochs_ft):\n",
    "    # print(epoch)\n",
    "    loss_per_step = []\n",
    "    loop_object  = tqdm(enumerate(train_dataloader), desc=f\"Epochs {epoch}\")\n",
    "    for i, batch in loop_object :        \n",
    "        ft_optimizer.zero_grad() \n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "\n",
    "        input_year, input_seq = inputs\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings = autoencoder.get_encoding(input_year, input_seq).view(bs,ss, -1)\n",
    "        encodings = aggregator(encodings) # make sure that the number of dimensions is aligned after the Simple Decoder\n",
    "        mask = ~((input_seq == 101).sum(-1) == NUM_COLS).view(bs,ss).detach()\n",
    "\n",
    "        # Forward pass\n",
    "        xx = decoder(encodings, mask)\n",
    "        outputs = F.sigmoid(xx)\n",
    "\n",
    "        loss = ft_loss(torch.flatten(outputs), labels)  \n",
    "        loss_per_step.append(loss.detach().cpu().numpy())\n",
    "        loop_object.set_postfix_str(\"mean loss: %.3f\"%np.mean(loss_per_step[-100:]))\n",
    "\n",
    "        #loss.backward(retain_graph=True)\n",
    "        loss.backward()\n",
    "        ft_optimizer.step()\n",
    "    # On epoch end\n",
    "    loss_per_epoch.append(np.mean(loss_per_step))\n",
    "    ft_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_ft}, Loss: {loss_per_epoch[-1]:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37c653ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss * bs\n",
    "#input_seq = input_seq.reshape(bs * ss, -1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649bc948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_loss = []\n",
    "preds = []\n",
    "targets = []\n",
    "\n",
    "## Set both models into the eval mode.=\n",
    "decoder.eval()\n",
    "autoencoder.eval()\n",
    "aggregator.eval()\n",
    "for batch in test_dataloader:\n",
    "    inputs, labels = batch\n",
    "    labels = labels.to(torch.float).to(device)\n",
    "\n",
    "    input_year, input_seq = inputs\n",
    "    bs, ss = labels.size(0), 14\n",
    "    input_year = input_year.reshape(-1).to(device)\n",
    "    input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "    encodings = autoencoder.get_encoding(input_year, input_seq).view(bs,ss, -1)\n",
    "    encodings = aggregator(encodings)\n",
    "    mask = ~((input_seq == 101).sum(-1) == NUM_COLS).view(bs,ss).detach()\n",
    "\n",
    "\n",
    "    # Forward pass\n",
    "    xx = decoder(encodings, mask)\n",
    "    outputs = torch.nn.functional.sigmoid(xx).flatten()\n",
    "    loss = ft_loss(outputs, labels)  \n",
    "    val_loss.append(loss.detach().cpu().numpy())\n",
    "    preds.extend(outputs.detach().cpu().numpy().tolist())\n",
    "    targets.extend(labels.cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9143\n",
      "Recall: 0.6400\n",
      "F1 Score: 0.7529\n",
      "-- mAP Score: 0.6761 --\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all the batches\n",
    "predictions = (torch.tensor(preds) > 0.5).float()\n",
    "probs = F.sigmoid(predictions)\n",
    "actuals = torch.tensor(targets).flatten()\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(actuals.cpu().numpy(), predictions.cpu().numpy(), average='binary')\n",
    "map_roc = average_precision_score(actuals.numpy(), probs.numpy())\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"-- mAP Score: {map_roc:.4f} --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83cad7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "816a65b0",
   "metadata": {},
   "source": [
    "# What is the effect of increasing the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acf98115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dataset import PretrainingDataset\n",
    "from model.dataset import FinetuningDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df7ef1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain(pt_epochs, train_dataloader, autoencoder, optimizer_pt, loss_pt, scheduler_pt):\n",
    "    for epoch in range(pt_epochs):\n",
    "\n",
    "        loop_object  = tqdm(enumerate(train_dataloader), desc=f\"Epochs {epoch}\")\n",
    "        for i, (year, seq) in loop_object :\n",
    "            optimizer_pt.zero_grad()\n",
    "            year = year.to(device)\n",
    "            seq = seq.to(device)\n",
    "            \n",
    "            x = autoencoder(year, seq)\n",
    "            loss = loss_pt(x.permute(0,2,1), seq.long())\n",
    "                                                            \n",
    "            loss.backward()\n",
    "            optimizer_pt.step()\n",
    "        ## After epoch end\n",
    "        scheduler_pt.step()\n",
    "        \n",
    "\n",
    "def finetune(num_epochs_ft, train_dataloader, autoencoder, decoder, ft_loss, ft_optimizer):\n",
    "    for epoch in range(num_epochs_ft):\n",
    "        # print(epoch)\n",
    "        loop_object  = tqdm(enumerate(train_dataloader), desc=f\"Epochs {epoch}\")\n",
    "        for i, batch in loop_object :        \n",
    "            ft_optimizer.zero_grad() \n",
    "            inputs, labels = batch\n",
    "            labels = labels.to(torch.float).to(device)\n",
    "\n",
    "            input_year, input_seq = inputs\n",
    "            bs, ss = labels.size(0), 14\n",
    "            input_year = input_year.reshape(-1).to(device)\n",
    "            input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "            encodings = autoencoder.get_encoding(input_year, input_seq).view(bs,ss, -1)\n",
    "            encodings = aggregator(encodings) # make sure that the number of dimensions is aligned after the Simple Decoder\n",
    "            mask = ~((input_seq == 101).sum(-1) == NUM_COLS).view(bs,ss).detach()\n",
    "\n",
    "            # Forward pass\n",
    "            xx = decoder(encodings, mask)\n",
    "            outputs = F.sigmoid(xx)\n",
    "\n",
    "            loss = ft_loss(torch.flatten(outputs), labels)  \n",
    "\n",
    "            #loss.backward(retain_graph=True)\n",
    "            loss.backward()\n",
    "            ft_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9011e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_dataloader, encoder, decoder):\n",
    "    val_loss = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    ## Set both models into the eval mode.=\n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "    for batch in test_dataloader:\n",
    "        inputs, labels = batch\n",
    "        labels = labels.to(torch.float).to(device)\n",
    "\n",
    "        input_year, input_seq = inputs\n",
    "        bs, ss = labels.size(0), 14\n",
    "        input_year = input_year.reshape(-1).to(device)\n",
    "        input_seq = input_seq.reshape(bs * ss, -1).to(device)\n",
    "\n",
    "        encodings = encoder(input_year, input_seq).view(bs,ss, -1)\n",
    "        mask = ~((input_seq == 101).sum(-1) == NUM_COLS).view(bs,ss).detach()\n",
    "\n",
    "        # Forward pass\n",
    "        xx = decoder(encodings, mask)\n",
    "        outputs = torch.nn.functional.sigmoid(xx).flatten()\n",
    "        loss = ft_loss(outputs, labels)  \n",
    "        val_loss.append(loss.detach().cpu().numpy())\n",
    "        preds.extend(outputs.detach().cpu().numpy().tolist())\n",
    "        targets.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "\n",
    "    # Concatenate all the batches\n",
    "    predictions = (torch.tensor(preds) > 0.5).float()\n",
    "    probs = F.sigmoid(predictions)\n",
    "    actuals = torch.tensor(targets).flatten()\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(actuals.cpu().numpy(), predictions.cpu().numpy(), average='binary')\n",
    "    map_roc = average_precision_score(actuals.numpy(), probs.numpy())\n",
    "    \n",
    "    return precision, recall, f1, map_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08697fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_HIDDEN_DIM=128\n",
    "PT_BATCH_SIZE=128\n",
    "PT_NUM_EPOCHS=1\n",
    "PT_LEARNING_RATE=10e-2\n",
    "\n",
    "\n",
    "FT_BATCH_SIZE = 16\n",
    "FT_ENCODING_SIZE = 64\n",
    "FT_HIDDEN_SIZE = 64\n",
    "\n",
    "FT_NUM_EPOCHS= 1\n",
    "FT_LEARNING_RATE= 5e-3\n",
    "n_questions = [10]\n",
    "\n",
    "importance = pd.read_csv('features_importance_1000.csv')\n",
    "targets = targets[targets.new_child.notna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a6fd488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lmmi/fertility-prediction-challenge/data_processing/pipeline.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  codebook[\"pairs\"] = codebook['var_name'].apply(get_generic_name)\n",
      "Epochs 0: 652it [00:23, 28.17it/s]\n",
      "/opt/anaconda3/envs/eyra-rank/lib/python3.11/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "Epochs 0: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[224, -1]' is invalid for input of size 1664",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 87\u001b[0m\n\u001b[1;32m     83\u001b[0m aggregator\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     86\u001b[0m loss_per_epoch \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 87\u001b[0m \u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs_ft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFT_NUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mft_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mft_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mft_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mft_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m precision, recall, f1, map_roc \u001b[38;5;241m=\u001b[39m evaluate(test_dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader, encoder\u001b[38;5;241m=\u001b[39mautoencoder, decoder\u001b[38;5;241m=\u001b[39mdecoder)\n\u001b[1;32m     97\u001b[0m precision_train, recall_train, f1_train, map_roc_train \u001b[38;5;241m=\u001b[39m evaluate(train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader, encoder\u001b[38;5;241m=\u001b[39mautoencoder, decoder\u001b[38;5;241m=\u001b[39mdecoder)\n",
      "Cell \u001b[0;32mIn[11], line 31\u001b[0m, in \u001b[0;36mfinetune\u001b[0;34m(num_epochs_ft, train_dataloader, autoencoder, decoder, ft_loss, ft_optimizer)\u001b[0m\n\u001b[1;32m     29\u001b[0m bs, ss \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m14\u001b[39m\n\u001b[1;32m     30\u001b[0m input_year \u001b[38;5;241m=\u001b[39m input_year\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 31\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m \u001b[43minput_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m encodings \u001b[38;5;241m=\u001b[39m autoencoder\u001b[38;5;241m.\u001b[39mget_encoding(input_year, input_seq)\u001b[38;5;241m.\u001b[39mview(bs,ss, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m encodings \u001b[38;5;241m=\u001b[39m aggregator(encodings) \u001b[38;5;66;03m# make sure that the number of dimensions is aligned after the Simple Decoder\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[224, -1]' is invalid for input of size 1664"
     ]
    }
   ],
   "source": [
    "\n",
    "prec_list = []\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "train_prec_list = []\n",
    "train_rec_list = []\n",
    "train_f1_list = []\n",
    "\n",
    "#n_question_list = []\n",
    "\n",
    "train_person_ids, test_person_ids = train_test_split(targets['nomem_encr'], test_size=0.2, random_state=42)\n",
    "\n",
    "for n_question in n_questions:\n",
    "\n",
    "    custom_pairs = importance.iloc[:n_question].feature.map(lambda x: get_generic_name(x))\n",
    "    sequences = encoding_pipeline(data, codebook, custom_pairs=custom_pairs)\n",
    "\n",
    "    # initialize autoencoder and pretrain    \n",
    "    pretrain_dataset = PretrainingDataset(sequences)\n",
    "\n",
    "    SEQ_LEN = pretrain_dataset.get_seq_len()\n",
    "    vocab_size = pretrain_dataset.get_vocab_size()\n",
    "\n",
    "    pt_dataloader = DataLoader(pretrain_dataset, batch_size=PT_BATCH_SIZE, shuffle=True)\n",
    "    autoencoder = SimpleAutoEncoder(vocab_size=vocab_size, embedding_size=PT_HIDDEN_DIM, sequence_len=SEQ_LEN).to(device)\n",
    "    autoencoder.train()\n",
    "    autoencoder.to(device)\n",
    "\n",
    "    pt_loss_cls = nn.CrossEntropyLoss()\n",
    "    pt_optimizer = optim.RAdam(autoencoder.parameters(), lr = PT_LEARNING_RATE, weight_decay=1e-2, decoupled_weight_decay=True)\n",
    "    pt_scheduler = optim.lr_scheduler.CosineAnnealingLR(pt_optimizer, T_max = PT_NUM_EPOCHS, eta_min = 1e-5, last_epoch = -1)\n",
    "\n",
    "    autoencoder.train()\n",
    "    pretrain(pt_epochs=PT_NUM_EPOCHS,\n",
    "        train_dataloader=pt_dataloader,\n",
    "        autoencoder=autoencoder,\n",
    "        loss_pt=pt_loss_cls,\n",
    "        optimizer_pt=pt_optimizer,\n",
    "        scheduler_pt=pt_scheduler,\n",
    "        )\n",
    "\n",
    "    # initialize GRU and perform fine-tuning    \n",
    "\n",
    "    rnn_data = {\n",
    "    person_id: (\n",
    "            torch.tensor([year-2007 for year, _ in wave_responses.items()]).to(device),\n",
    "            torch.tensor([ wave_response for _, wave_response in wave_responses.items()]).to(device)\n",
    "            )\n",
    "            for person_id, wave_responses in sequences.items()\n",
    "    }\n",
    "\n",
    "    train_data = {person_id: rnn_data[person_id] for person_id in train_person_ids}\n",
    "    test_data = {person_id: rnn_data[person_id] for person_id in test_person_ids}\n",
    "    train_dataset = FinetuningDataset(train_data, targets = targets)\n",
    "    test_dataset = FinetuningDataset(test_data, targets = targets)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=FT_BATCH_SIZE, shuffle=True)\n",
    "    test_dataloader  = DataLoader(test_dataset,  batch_size=FT_BATCH_SIZE)\n",
    "\n",
    "    aggregator = nn.Sequential(\n",
    "        nn.LazyInstanceNorm1d(),\n",
    "        nn.LazyLinear(FT_ENCODING_SIZE),\n",
    "        nn.LazyBatchNorm1d()).to(device)\n",
    "\n",
    "    decoder = GRUDecoder(\n",
    "        input_size=FT_ENCODING_SIZE,\n",
    "        hidden_size=FT_HIDDEN_SIZE,\n",
    "        max_seq_len=14,\n",
    "        dropout=0.15,\n",
    "        bidirectional= False, \n",
    "        num_layers=2,\n",
    "        with_attention= True\n",
    "    ).to(device)\n",
    "\n",
    "    # Define loss function and optimizer for RNN\n",
    "    ft_loss = torch.nn.BCELoss()\n",
    "    ft_optimizer = torch.optim.RAdam(list(decoder.parameters()) + list(autoencoder.parameters()) + list(aggregator.parameters()) , \n",
    "                                                                       lr=FT_LEARNING_RATE, \n",
    "                                                                       weight_decay=1e-3, \n",
    "                                                                       decoupled_weight_decay=True)\n",
    "    ft_scheduler = optim.lr_scheduler.CosineAnnealingLR(ft_optimizer, T_max = FT_NUM_EPOCHS, eta_min = 1e-6, last_epoch = -1)\n",
    "\n",
    "    # Training loop\n",
    "    decoder.train()\n",
    "    aggregator.train()\n",
    "\n",
    "    \n",
    "    loss_per_epoch = []\n",
    "    finetune(\n",
    "        num_epochs_ft=FT_NUM_EPOCHS,\n",
    "        train_dataloader=train_dataloader,\n",
    "        autoencoder=autoencoder,\n",
    "        ft_loss=ft_loss,\n",
    "        ft_optimizer=ft_optimizer,\n",
    "        decoder=decoder,\n",
    "    )\n",
    "    \n",
    "    precision, recall, f1, map_roc = evaluate(test_dataloader=test_dataloader, encoder=autoencoder, decoder=decoder)\n",
    "    precision_train, recall_train, f1_train, map_roc_train = evaluate(train_dataloader=train_dataloader, encoder=autoencoder, decoder=decoder)\n",
    "    \n",
    "    prec_list.append(precision)\n",
    "    rec_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "    train_prec_list.append(precision_train)\n",
    "    train_rec_list.append(recall_train)\n",
    "    train_f1_list.append(f1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81284a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PreFer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
