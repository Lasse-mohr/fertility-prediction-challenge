{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b651aa-752a-4c71-990a-332ff4099791",
   "metadata": {},
   "source": [
    "# Read the data\n",
    "\n",
    "Right now the notebook is set to work with fake data. This can be changed once the pipeline works.\n",
    "\n",
    "The data is stored as a Dict[person_id, Sequences] where Sequences is a Dict[year, survery_wave_response]\n",
    "\n",
    "Tokenizing takes a bit of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c9914262-7a3e-4e3a-9e07-043d40efd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model.rnn import GRUDecoder\n",
    "from model.autoencoder import AutoEncoder\n",
    "\n",
    "from data_processing.encoding.categorical import CategoricalTransformer\n",
    "from data_processing.encoding.numeric_and_date import ToQuantileTransformer\n",
    "from data_processing.encoding.text2vec import TextTransform\n",
    "from data_processing.sequences.sequencing import to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c3f871-21e8-418c-9aa7-31ad09283a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lmmi/fertility-prediction-challenge/data_processing/encoding/categorical.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  core_cat_df['values_cat'] = core_cat_df['values_cat'].str.split(\"; \").apply(lambda x: [e.strip() for e in x])\n",
      "/Users/lmmi/fertility-prediction-challenge/data_processing/encoding/categorical.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  core_cat_df['labels_cat'] = core_cat_df['labels_cat'].str.split(\"; \").apply(lambda x: [e.strip() for e in x])\n"
     ]
    }
   ],
   "source": [
    "# read in data and prepare transformations\n",
    "data = pd.read_csv('data/other_data/PreFer_fake_data.csv')\n",
    "targets = pd.read_csv('data/other_data/PreFer_fake_outcome.csv')\n",
    "codebook = pd.read_csv('data/codebooks/PreFer_codebook.csv')\n",
    "summary = pd.read_csv('data/codebooks/PreFer_codebook_summary.csv')\n",
    "\n",
    "categorical_columns = codebook[(codebook.var_name.str.startswith('c')) & (codebook.type_var == 'categorical')].var_name.tolist()\n",
    "quantile_columns = codebook[(codebook.var_name.str.startswith('c')) & ((codebook.type_var == 'numeric') | (codebook.type_var == 'date or time'))].var_name.tolist()\n",
    "\n",
    "cat_transform = CategoricalTransformer()\n",
    "cat_transform.fit(codebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a10fe311-0b4d-4f88-851e-50327041814d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y6/j9fbqcvx6lb5l99614n30y4c0000gn/T/ipykernel_7270/2027597144.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data2['nomem_encr'] = data['nomem_encr']\n",
      "/var/folders/y6/j9fbqcvx6lb5l99614n30y4c0000gn/T/ipykernel_7270/2027597144.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data2['nomem_encr'] = data['nomem_encr']\n"
     ]
    }
   ],
   "source": [
    "# this cell takes a bit of time\n",
    "for col in categorical_columns:\n",
    "    data[col] = cat_transform.transform(data[col])\n",
    "    \n",
    "quantile_transform = ToQuantileTransformer(quantile_columns)\n",
    "quantile_transform.fit(data)\n",
    "data = quantile_transform.transform(data)\n",
    "\n",
    "data.fillna(101, inplace=True)\n",
    "data[quantile_columns] = data[quantile_columns].astype(int)\n",
    "\n",
    "data2 = data[data.columns[data.columns.str.startswith('c')]]\n",
    "data2['nomem_encr'] = data['nomem_encr']\n",
    "\n",
    "sequences = to_sequences(data2, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18aee1dd-7884-40e7-be05-db070f1a452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    # Check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        # If CUDA is available, select the first CUDA device\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(\"Using CUDA device:\", torch.cuda.get_device_name(0))\n",
    "    # Check for MPS availability on supported macOS devices (requires PyTorch 1.12 or newer)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        # If MPS is available, use MPS device\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS (Metal Performance Shaders) device\")\n",
    "    else:\n",
    "        # Fallback to CPU if neither CUDA nor MPS is available\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1704f8ed-5b81-49db-b19c-6970db7d04b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some strings that have still not been properly been filtered out\n",
    "# this cell gets rid of them. \n",
    "# The real solution is to change the categorical encoding class\n",
    "for _, wave_responses in sequences.items():\n",
    "    for year, wave_response in wave_responses.items():\n",
    "        \n",
    "        not_int = np.array([not isinstance(x, int) for x in wave_response], dtype = bool)\n",
    "        wave_responses[year] = [\n",
    "            item if not _bool else 101 for (item, _bool) in zip(wave_response, not_int)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddb66e-cba5-4bb9-854d-811d49599b93",
   "metadata": {},
   "source": [
    "# Train the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5d585bb-2414-4cec-954c-511d78015560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We dont need targets or year information for the autoencoder\n",
    "# so we merge everthing together in one tensor\n",
    "autoencoder_data = torch.tensor([\n",
    "                                wave_response\n",
    "                                for _, wave_responses in sequences.items()\n",
    "                                for _, wave_response in wave_responses.items()\n",
    "                        ]).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "249dc8a5-a82d-4fab-aaab-2c5775263d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is my attempt at getting the autoencoder to work\n",
    "\n",
    "# the existing autoencoder does not collapse peoples survery\n",
    "# responses to one-dimensional representations.\n",
    "# However, I can't get it to work.\n",
    "\n",
    "\n",
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, num_embeddings, n_questions, embedding_dim=512, encoding_dim=16) -> None:\n",
    "        super().__init__()\n",
    "        self.encoding_dim = encoding_dim\n",
    "\n",
    "        self.embed = torch.nn.Embedding(\n",
    "            num_embeddings=num_embeddings,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, encoding_dim),\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(encoding_dim, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.embed(x) # shape n_years x n_questions x embedding_dim\n",
    "\n",
    "        # reduce to 2-dimensional tensor\n",
    "        x_flat = x_embed.view(-1, x_embed.size(-1)) # shape n_years x (n_questions x embedding_dim)\n",
    "        \n",
    "        x_encoded = self.encoder(x_flat) \n",
    "\n",
    "        x_decoded = self.decoder(x_encoded) \n",
    "\n",
    "        # Reshape the decoded tensor back to its original 3-dimensional shape\n",
    "        x_reconstructed = x_decoded.view(x.size(0), x.size(1), -1)\n",
    "\n",
    "        return x_reconstructed\n",
    "\n",
    "    def get_loss(self, x):\n",
    "        x_flat = x.view(-1, x.size(1) * x.size(2))\n",
    "        x_hat = self.forward(x)\n",
    "        return torch.nn.functional.mse_loss(x_hat, x_flat)\n",
    "\n",
    "    def embed_and_encode(self, x):\n",
    "        x_flat = x.view(-1, x.size(1) * x.size(2))\n",
    "        x_emb = self.embed(x_flat)\n",
    "        return self.encoder(x_emb)\n",
    "\n",
    "    def get_encoding_dim(self):\n",
    "        return self.encoding_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "aa220af4-5eb6-47f1-b4e1-5814be3228c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt at training the autoencoder.\n",
    "# However, I get the error that I'm trying to look up too many\n",
    "# embeddings. I haven't solved why.\n",
    "batch_size = 128 \n",
    "\n",
    "train_dataloader = DataLoader(autoencoder_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "num_epochs_autoencoder = 10\n",
    "learning_rate_autoencoder = 0.001\n",
    "embedding_dim = 2\n",
    "vocab_size = 15000#len(set([ elem for  _, sequence in sequences.items() for _, item in sequence.items() for elem in item]))\n",
    "\n",
    "n_questions = autoencoder_data.shape[1]\n",
    "\n",
    "error = nn.MSELoss()\n",
    "\n",
    "autoencoder = AutoEncoder(num_embeddings=vocab_size, n_questions=n_questions).to(device)\n",
    "\n",
    "optimizer = optim.Adam( autoencoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8be77146-e5b0-4a06-9d9e-d651c88d32a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m      4\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[127], line 51\u001b[0m, in \u001b[0;36mAutoEncoder.get_loss\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 51\u001b[0m     x_flat \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     52\u001b[0m     x_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(x_hat, x_flat)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "autoencoder.train()\n",
    "for epoch in range(num_epochs_autoencoder):\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = autoencoder.get_loss(batch[0].to(device))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if epoch % int(0.1*num_epochs_autoencoder) == 0:\n",
    "            print(f'epoch {epoch} \\t Loss: {loss.item():.4g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4209ad65-61d5-4644-9110-d75fd32ca5e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ada8018b-7ebc-4db4-aadf-191b214b1470",
   "metadata": {},
   "source": [
    "# Train the RNN\n",
    "\n",
    "First we need to create Dataset class that can hold both the target (stored in a pd.DataFrame) and the sequences.\n",
    "\n",
    "The sequences will be of dimension 14 x encoding_dimension, because we have 14 years of surveys.\n",
    "\n",
    "I have created some code for getting the data into the right format, but it might not be useful.\n",
    "\n",
    "## Regarding masks\n",
    "Right now the masking is done already in the encoding. I haven't found exactly where Mikkel implemented this.\n",
    "So for now, assume that nothing is padded, and then we'll figure it out with Mikkel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af9fd88-99ca-49cc-a078-7706dafd4a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorSequencesWithTarget(Dataset):\n",
    "    def __init__(self, sequences:dict, target: pd.DataFrame):\n",
    "        self.sequences = sequences \n",
    "        self.target = targets.set_index(keys = 'nomem_encr').squeeze().to_dict()\n",
    "        self.keys = list(sequences.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        person_id = self.keys[index]\n",
    "        \n",
    "        target = self.target[person_id]\n",
    "        sequence = self.sequences[person_id]\n",
    "        \n",
    "\n",
    "        return target, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1786da-d2b5-4e1f-81f6-1db7bca5515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# its not everyone we have a target for, so we do restrict the data to \n",
    "# the ones with known outcomes\n",
    "\n",
    "train_person_ids, test_person_ids = train_test_split(targets['nomem_encr'], test_size=0.2, random_state=42)\n",
    "train_person_ids, val_person_ids = train_test_split(train_person_ids, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317ed83d-0d05-4404-92cb-16889c744519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure the data as a Dict[person_id, survey_embedding_sequence] \n",
    "# where survey_embedding_sequence is a tensor of size 14 x embedding_dimension\n",
    "rnn_data = {person_id:\n",
    "                autoencoder.embed_and_encode(\n",
    "                    torch.tensor(\n",
    "                        [ wave_response for _, wave_response in wave_responses.items()]\n",
    "                    )\n",
    "                ).to(torch.float)\n",
    "            for person_id, wave_responses in sequences.items()\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaadc01-1e81-4727-b7f4-f14823463fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data based on the splits made for the target\n",
    "train_data = {person_id: rnn_data[person_id] for person_id in train_person_ids}\n",
    "val_data = {person_id: rnn_data[person_id] for person_id in val_person_ids}\n",
    "test_data = {person_id: rnn_data[person_id] for person_id in test_person_ids}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d85edc9-8457-439f-8fe9-fbd94eb9b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SequencesWithTarget(train_data, target = targets)\n",
    "val_dataset = SequencesWithTarget(val_data, target = targets)\n",
    "test_dataset = SequencesWithTarget(test_data, target = targets)\n",
    "\n",
    "rnn_batch_size = 50\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=rnn_batch_size, shuffle=True)\n",
    "val_dataloader   = DataLoader(val_dataset,   batch_size=rnn_batch_size)\n",
    "test_dataloader  = DataLoader(test_dataset,  batch_size=rnn_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73f88fd-bac9-4e45-96f6-6aa7037cfd1e",
   "metadata": {},
   "source": [
    "### My attempt at training the model, but probably not correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e220b8-17b1-4a26-9c02-b4ac3de853af",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 10\n",
    "num_epochs_rnn = 10\n",
    "learning_rate_rnn = 0.001\n",
    "\n",
    "rnn_model = GRUDecoder(\n",
    "    input_size=autoencoder.get_encoding_dim(),\n",
    "    hidden_size=hidden_size,\n",
    "    max_seq_len=14\n",
    ").to(device)\n",
    "\n",
    "# assume that all 14 years are observed for everyone\n",
    "single_mask = torch.BoolTensor([True]*14).to(device) \n",
    "\n",
    "# Define loss function and optimizer for RNN\n",
    "rnn_criterion = torch.nn.CrossEntropyLoss()\n",
    "rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr=learning_rate_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf1777-49e7-462d-ae51-549ea6c8305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "rnn_model.train()\n",
    "\n",
    "for epoch in range(num_epochs_rnn):\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        labels, inputs = batch\n",
    "        print(labels)\n",
    "        \n",
    "        rnn_optimizer.zero_grad() \n",
    "\n",
    "        # Forward pass\n",
    "        mask = torch.stack([single_mask]*len(labels) ) #  not correct masking\n",
    "                           \n",
    "        xx = rnn_model(inputs, mask)\n",
    "        outputs = torch.nn.functional.sigmoid(xx)\n",
    "        loss = rnn_criterion(outputs.squeeze(), labels)  \n",
    "        \n",
    "        loss.backward()\n",
    "        rnn_optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PreFer data expl)",
   "language": "python",
   "name": "prefer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
