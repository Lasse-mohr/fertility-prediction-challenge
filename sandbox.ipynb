{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.rnn import GRUDecoder\n",
    "from model.autoencoder import AutoEncoder\n",
    "import torch        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    # Check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        # If CUDA is available, select the first CUDA device\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(\"Using CUDA device:\", torch.cuda.get_device_name(0))\n",
    "    # Check for MPS availability on supported macOS devices (requires PyTorch 1.12 or newer)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        # If MPS is available, use MPS device\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS (Metal Performance Shaders) device\")\n",
    "    else:\n",
    "        # Fallback to CPU if neither CUDA nor MPS is available\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialization of the Autoencoder \n",
    "SEQ_LEN = 3000\n",
    "HIDDEN_DIM = 512\n",
    "ENCODING_SIZE = 64\n",
    "model = AutoEncoder(vocab_size=100, embedding_size=HIDDEN_DIM, encoding_size=ENCODING_SIZE, sequence_len=SEQ_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's assume we have a batch of 2 people\n",
    "x = torch.randint(1,99, size=(2,SEQ_LEN))\n",
    "y = model(x) \n",
    "## returns the original shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### only to use the encoder part \n",
    "y = model.encode(x) # here y contains embedding of a survey per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.5367e-08, -3.7087e-08,  3.1789e-08, -5.4306e-08, -6.8876e-08,\n",
       "         -5.2982e-09, -5.6956e-08, -5.8280e-08, -4.2386e-08, -4.3710e-08,\n",
       "          2.6491e-09, -6.8876e-08,  2.6491e-08,  7.6824e-08,  1.3245e-08,\n",
       "         -2.7816e-08,  3.1789e-08, -5.2982e-08,  5.2982e-08, -2.7816e-08,\n",
       "          6.6227e-09, -3.1789e-08, -1.8544e-08, -1.0596e-08,  0.0000e+00,\n",
       "         -1.0596e-08,  2.2517e-08, -2.1193e-08,  1.0596e-08, -1.7219e-08,\n",
       "         -4.5035e-08,  2.1193e-08, -2.1193e-08,  4.2386e-08, -2.7816e-08,\n",
       "          3.1789e-08,  4.7684e-08, -3.7087e-08,  9.2718e-09,  1.0596e-08,\n",
       "          1.0596e-08, -3.1789e-08,  5.0333e-08, -3.4438e-08, -5.2982e-08,\n",
       "          4.2386e-08, -3.7087e-08,  0.0000e+00,  1.0596e-08, -1.3245e-08,\n",
       "          9.5367e-08, -1.3245e-08, -3.8412e-08,  1.1126e-07,  5.0333e-08,\n",
       "         -7.1526e-08,  1.8544e-08,  1.5895e-08,  3.3114e-08, -4.5035e-08,\n",
       "         -3.1789e-08,  1.0596e-08,  8.4771e-08,  7.4175e-08],\n",
       "        [-2.6491e-09,  3.1789e-08, -1.5895e-08, -1.1126e-07, -1.5895e-08,\n",
       "          6.8876e-08,  3.0465e-08, -7.8148e-08, -5.2982e-09, -2.6491e-09,\n",
       "          0.0000e+00, -1.3245e-09,  2.3842e-08,  6.8876e-08,  4.2386e-08,\n",
       "         -2.1193e-08, -1.1921e-08,  0.0000e+00, -5.2982e-09,  7.9473e-09,\n",
       "          1.5895e-08, -1.0596e-08, -1.0596e-08,  0.0000e+00,  7.4175e-08,\n",
       "         -1.5895e-08, -2.6491e-09, -1.0596e-08, -6.3578e-08,  2.6491e-09,\n",
       "          5.9605e-08,  3.1789e-08,  6.4903e-08, -5.2982e-09,  1.5895e-08,\n",
       "          6.8876e-08, -2.6491e-08,  3.5763e-08, -3.1789e-08, -5.2982e-09,\n",
       "         -2.6491e-09, -2.1193e-08,  4.2386e-08, -7.4175e-08,  0.0000e+00,\n",
       "          1.3245e-08,  1.5895e-08,  1.3245e-09, -2.1193e-08,  1.4570e-08,\n",
       "          1.3245e-08, -5.8280e-08, -1.0596e-08, -8.6096e-08, -3.3114e-08,\n",
       "         -3.4438e-08, -1.0994e-07,  1.0596e-08,  1.3245e-08,  0.0000e+00,\n",
       "          0.0000e+00, -1.4570e-08,  0.0000e+00, -4.1061e-08]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders) device\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "# input_size -> the size of the embedding of the autoencoder model\n",
    "# hidden_size -> the size of the RNN to use in the decoder (the input_size and hidden_size can be different)\n",
    "model = GRUDecoder(input_size=6, hidden_size=10, max_seq_len=4).to(get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders) device\n",
      "Using MPS (Metal Performance Shaders) device\n"
     ]
    }
   ],
   "source": [
    "# This is just an example\n",
    "\n",
    "MAX_SEQ_LEN = 4 # max number of surveyas a person (in our dataset can have)\n",
    "INPUT_SIZE = 6 # hidden dimmensions of autoencodder.\n",
    "\n",
    "# let's say we have a person who only have 2 surveys\n",
    "x0 = torch.rand(INPUT_SIZE) # embedding for the 1st survey \n",
    "x1 = torch.rand(INPUT_SIZE) # embedding for the 2nd survey\n",
    "\n",
    "# the tensor for the person should be on the shape [MAX_SEQ_LEN, INPUT_SIZE]\n",
    "\n",
    "e = torch.zeros(MAX_SEQ_LEN, INPUT_SIZE)\n",
    "e[0] = x0\n",
    "e[1] = x1\n",
    "e = e.to(get_device()) # so this is a tensor for the person\n",
    "#we also need to specify that the sequence has 'empty' embeddings\n",
    "mask = torch.BoolTensor([True, True, False, False]).to(get_device()) # the last two dimensions are empty\n",
    "## it is important that you append existing survey embeddings right next to each other (even if the year is missign between them, they should be still appended one after another)\n",
    "\n",
    "## let assume we have a batch of people, I am reusing the same person, but in the pipeline is should be different people\n",
    "# the batch size is 3 here \n",
    "\n",
    "x = torch.stack([e,e,e])\n",
    "mask = torch.stack([mask, mask, mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = model(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4641],\n",
       "        [0.3718],\n",
       "        [0.4972]], device='mps:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.sigmoid(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
