{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.rnn import GRUDecoder\n",
    "from model.autoencoder import AutoEncoder\n",
    "import torch        \n",
    "from data_processing.sequences.sequencing import get_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ch014'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pairs(\"ch07014\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    # Check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        # If CUDA is available, select the first CUDA device\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(\"Using CUDA device:\", torch.cuda.get_device_name(0))\n",
    "    # Check for MPS availability on supported macOS devices (requires PyTorch 1.12 or newer)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        # If MPS is available, use MPS device\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS (Metal Performance Shaders) device\")\n",
    "    else:\n",
    "        # Fallback to CPU if neither CUDA nor MPS is available\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomarx/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "### Initialization of the Autoencoder \n",
    "SEQ_LEN = 3000\n",
    "HIDDEN_DIM = 512\n",
    "ENCODING_SIZE = 64\n",
    "model = AutoEncoder(vocab_size=100, embedding_size=HIDDEN_DIM, encoding_size=ENCODING_SIZE, sequence_len=SEQ_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's assume we have a batch of 2 people\n",
    "x = torch.randint(1,99, size=(2,SEQ_LEN))\n",
    "y = model(x) \n",
    "## returns the original shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### only to use the encoder part \n",
    "y = model.encode(x) # here y contains embedding of a survey per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0832, 0.9053, 1.0793, 1.0069, 1.0820, 1.0080, 1.0972, 1.1351, 1.1825,\n",
       "         1.1825, 1.1798, 1.2497, 1.0592, 1.1617, 1.0967, 0.9440, 0.9736, 1.0652,\n",
       "         1.1814, 1.1295, 1.1983, 0.9670, 1.0642, 0.9383, 1.0616, 1.2068, 1.0834,\n",
       "         1.1106, 1.0930, 1.1473, 1.0343, 1.2510, 1.1105, 1.1286, 1.0923, 1.0668,\n",
       "         1.0937, 1.0260, 1.0193, 1.0188, 1.0924, 1.0847, 1.0894, 1.1251, 1.1681,\n",
       "         1.0378, 1.0206, 1.2475, 1.1310, 0.9895, 1.1751, 0.9599, 1.1929, 1.1732,\n",
       "         1.1567, 0.9172, 0.8553, 1.0445, 1.0713, 1.0777, 1.0420, 1.1589, 0.9551,\n",
       "         1.1668],\n",
       "        [1.1318, 1.0795, 1.1133, 1.0535, 1.2317, 1.0746, 1.0192, 1.1769, 1.0959,\n",
       "         1.1960, 1.0426, 0.9671, 0.9651, 1.0878, 1.1340, 1.1135, 1.1042, 1.0427,\n",
       "         1.1906, 1.0300, 0.9913, 1.0032, 1.0898, 1.0759, 1.0755, 1.0717, 1.1187,\n",
       "         1.1150, 1.0206, 1.0369, 1.0803, 1.0738, 0.9569, 1.0141, 1.1088, 1.0496,\n",
       "         1.0675, 1.1581, 1.1693, 1.1407, 1.0574, 1.0215, 0.9724, 1.0124, 0.9426,\n",
       "         1.1183, 1.0408, 1.2030, 0.9046, 1.2108, 1.1305, 1.0039, 1.0511, 1.0254,\n",
       "         0.9691, 0.9591, 1.1534, 0.8263, 1.1786, 0.9885, 0.9253, 1.0275, 1.0149,\n",
       "         1.0960]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3000, 512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_attn = torch.nn.MultiheadAttention(HIDDEN_DIM, 8, vdim=16, kdim=16)\n",
    "_x = model.embedding(x)\n",
    "_k, _v = torch.rand((2,SEQ_LEN,16)), torch.rand((2,SEQ_LEN,16))\n",
    "_x , _ = multihead_attn(_x, _k,_v, need_weights=False)\n",
    "_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is going to set all input MASK to None\n",
      "Using MPS (Metal Performance Shaders) device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomarx/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/carlomarx/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "# input_size -> the size of the embedding of the autoencoder model\n",
    "# hidden_size -> the size of the RNN to use in the decoder (the input_size and hidden_size can be different)\n",
    "model = GRUDecoder(input_size=6, hidden_size=10, max_seq_len=4).to(get_device())\n",
    "loss_f = nn.BCEWithLogitsLoss()\n",
    "solver = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders) device\n",
      "Using MPS (Metal Performance Shaders) device\n"
     ]
    }
   ],
   "source": [
    "# This is just an example\n",
    "\n",
    "MAX_SEQ_LEN = 4 # max number of surveyas a person (in our dataset can have)\n",
    "INPUT_SIZE = 6 # hidden dimmensions of autoencodder.\n",
    "\n",
    "# let's say we have a person who only have 3 surveys\n",
    "x0 = torch.rand(INPUT_SIZE) # embedding for the 1st survey \n",
    "x1 = torch.rand(INPUT_SIZE) # embedding for the 2nd survey\n",
    "x2 = torch.rand(INPUT_SIZE)\n",
    "x3 = torch.rand(INPUT_SIZE)\n",
    "# the tensor for the person should be on the shape [MAX_SEQ_LEN, INPUT_SIZE]\n",
    "\n",
    "e = torch.zeros(MAX_SEQ_LEN, INPUT_SIZE)\n",
    "e[0] = x0\n",
    "e[1] = x1\n",
    "e[2] = x2\n",
    "e = e.to(get_device()) # so this is a tensor for the person\n",
    "#we also need to specify that the sequence has 'empty' embeddings\n",
    "#mask = torch.BoolTensor([True, True, False, False]).to(get_device()) # the last two dimensions are empty\n",
    "## it is important that you append existing survey embeddings right next to each other (even if the year is missign between them, they should be still appended one after another)\n",
    "\n",
    "## let assume we have a batch of people, I am reusing the same person, but in the pipeline is should be different people\n",
    "# the batch size is 3 here \n",
    "\n",
    "x = torch.stack([e,e,e])\n",
    "y = torch.tensor([1.,1,1.]).to(get_device())\n",
    "#mask = torch.stack([mask, mask, mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    solver.zero_grad()\n",
    "    #xx = torch.nn.functional.sigmoid(model(x, mask))\n",
    "    xx = model(x, None)\n",
    "\n",
    "    loss = loss_f(xx.view(-1), y.view(-1))\n",
    "    loss.backward()\n",
    "    solver.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0794, device='mps:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
